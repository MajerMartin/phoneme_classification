\documentclass[12pt]{article}
\usepackage[czech]{babel}
\usepackage[cp1250]{inputenc}
\usepackage[a4paper,left=30mm,right=20mm,top=25mm,bottom=25mm]{geometry}

% pseudocodes
\usepackage[tworuled]{algorithm2e}

% tables and cline
\usepackage{multirow}
\usepackage{regexpatch}
\makeatletter
% Change the `-` delimiter to an active character
\xpatchparametertext\@@@cmidrule{-}{\cA-}{}{}
\xpatchparametertext\@cline{-}{\cA-}{}{}
\makeatother

% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{lmodern}
\usepackage{interval}

% equation numbering
\numberwithin{equation}{section}

% line-spacing 1.5
\usepackage{setspace}
\renewcommand{\baselinestretch}{1.5}

% indentation
\usepackage{indentfirst}

% citations
\usepackage{cite}
\usepackage[nottoc]{tocbibind}
\usepackage{url}

% images
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{float}
\graphicspath{{imgs/}}

% enumerate
\usepackage{enumitem}

\begin{document}

% TITULNÍ STRÁNKA
\begin{titlepage}
\thispagestyle{empty}
\begin{center}
\vspace*{0.5cm}
\Large Západoèeská univerzita v Plzni

\Large Fakulta aplikovanıch vìd

\Large Katedra kybernetiky

\vspace{8cm}
\LARGE DIPLOMOVÁ PRÁCE
\end{center}
\vfill
\large Plzeò, 2018
\hfill \large Martin Majer
\vspace*{1cm}
\end{titlepage}

% PROHLÁŠENÍ
\section*{Prohlášení}
\noindent Pøedkládám tímto k posouzení a obhajobì diplomovou práci zpracovanou na závìr studia na Fakultì aplikovanıch vìd Západoèeské univerzity v Plzni. \\[12pt]
\noindent Prohlašuji, e jsem diplomovou práci vypracoval samostatnì a vıhradnì s pouitím odborné literatury a pramenù, jejich úplnı seznam je její souèástí. \\[24pt]
\noindent V Plzni dne 20. dubna 2018
\hfill
$ \dots \dots \dots \dots \dots \dots \dots $
\thispagestyle{empty}
\newpage

% PODÌKOVÁNÍ
\section*{Podìkování}
Tímto bych rád podìkoval vedoucímu diplomové práce, Ing. Luboši Šmídlovi, Ph.D., za cenné rady a pøipomínky.
\thispagestyle{empty}
\newpage

\section*{Anotace}
\noindent Tato práce se zabıvá klasifikací izolovanıch slov pomocí neuronovıch sítí, klasifikátoru SVM a klasifikátoru zaloeném na algoritmu Dynamic Time Warping s ohledem na nízkou vıpoèetní nároènost. V první èásti jsou pøedstaveny pøíznaky v èasové a frekvenèní oblasti a odvozeny vyuité klasifikaèní algoritmy. Ve druhé èásti jsou uvedeny zvolené parametrizace testovanıch pøíznakù a struktura navrenıch klasifikaèních algoritmù. V závìru práce je pak vyhodnocena pøesnost klasifikace jednotlivıch metod pro zvolené parametrizace pøíznakù. \\[12pt]
\noindent \textbf{Klíèová slova:} zpracování akustického signálu, extrakce pøíznakù, detekce klíèovıch frází, dynamic time warping, support vector machine, neuronová sí

\section*{Abstract}
\noindent This thesis focuses on low computational cost isolated word recognition using neural networks, SVM classifier and Dynamic Time Warping based classifier. First part of the thesis introduces features in time and frequency domain and used classification techniques are derived. Parameterizations of tested features and structure of proposed classification algorithms are described in the second part of the thesis. Classification accuracy results of proposed methods for feature parameterizations are presented at the end of the thesis. \\[12pt]
\noindent \textbf{Keywords:} acoustic signal processing, feature extraction, keyword spotting, dynamic time warping, support vector machine, neural network
\thispagestyle{empty}
\newpage

% OBSAH
\tableofcontents
\thispagestyle{empty}
\newpage

% TEORETICKÁ ÈÁST
\pagenumbering{arabic}
\section{Úvod}
Neuronové sítì byly vyvinuty ji v polovinì minulého století, ale kvùli nedostateèné vıpoèetní kapacitì nemohly bıt plnì vyuity pro øešení reálnıch problémù. A v posledních letech, kdy došlo k vıznamnému vıvoji v oblasti hardwaru jak pro poèítaèe, tak i pro mobilní a vloená zaøízení, zaèali bıt plnì vyuívány a to zejména pro komplexní úlohy v oblasti zpracování obrazu a hlasu, kde stabilnì pøekonávají ostatní algoritmy strojového uèení. Vıvoj vıkonnıch grafickıch karet a optimalizovanıch knihoven pak umonil rychlé trénování tìchto modelù na velkém mnoství dat a díky vıkonnım èipùm lze provádìt predikci v reálném èase i na mobilních zaøízeních.

Tato práce se vìnuje vyuití neuronovıch sítí pro zpracování hlasu a to zejména úloze klasifikace fonémù s vyuitím základních metod zpracování akustického signálu pøedstavenıch v \cite{bp}. Ve zpracování hlasu jsou bìnì vyuívány jak sítì dopøedné, tak i rekurentní, které byly vytvoøeny pro klasifikaci èasovıch øad èi sekvencí a jsou schopny vyuívat kontextu. Pro trénování neuronovıch sítí je potøeba velké mnoství dat, aby byla zajištìna robustnost a schopnost generalizace s tím, e v úloze klasifikace fonémù jsou tato data ve formì zvukovıch nahrávek a odpovídajících pøepisù neboli transkripcí. 

Cílem práce je porovnat rùzné architektury neuronovıch sítí, které by mohly bıt vyuity pro pøepis mluvené øeèi do textové podoby (speech-to-text) v reálném èase. Za tímto úèelem byly voleny pøedevším jednodušší sítì s nízkım poètem parametrù. 

V první èásti práce je uvedena obecná teorie optimalizaèních algoritmù a neuronovıch sítí. Druhá èást se pak vìnuje metodì trénování rekurentních neuronovıch sítí, která nevyaduje pøedsegmentovaná trénovací data (více o segmentaci v \cite{bp}). Nakonec jsou porovnány rùzné architektury jak dopøednıch, tak i rekurentních neuronovıch sítí, na nìkolika datovıch sadách, které vznikly vyuitím rùznım typù pøíznakù.

\newpage
\section{Optimalizaèní algoritmy}
Vìtšina uèících se algoritmù vyuívá jistou formu optimalizace. Optimalizací rozumíme úlohu, kdy minimalizujeme èi maximalizujeme pøedem danou funkci $ f(\boldsymbol{x}) $ zmìnou parametru $ \boldsymbol{x} $. Hledáme tedy hodnotu $ x $ takovou, aby funkce funkce $ f(\boldsymbol{x}) $ nabıvala minimální èi maximální hodnoty. Vìtšina optimalizaèních metod uvauje minimalizace funkce $ f(\boldsymbol{x}) $ a její pøípadná maximalizace se provádí minimalizací funkce $ -f(\boldsymbol{x}) $.

Funkce, kterou optimalizujeme, nazıváme kritérium (v terminologii strojového uèení se také èasto objevují názvy cenová, ztrátová èi chybová funkce). Tato práce se zabıvá pøedevším optimalizací pro neuronové sítì, kde jsou nejèastìji vyuívány optimalizaèní metody zaloené na gradientu.

Základní metodou zaloenou na gradientu je tzv. gradientní sestup. Pøedpokládejme cenovou funkci $ J(\boldsymbol{\theta}) $ parametrizovanou souborem parametrù $ \boldsymbol{\theta} $. Gradientní sestup hledá optimální soubor parametrù $ \boldsymbol{\theta}^{*} = \text{argmin } J(\boldsymbol{\theta}) $ pomocí iterativního pravidla pro zmìnu parametrù
\begin{equation}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}),
\end{equation}
kde $ \epsilon $ je tzv. konstanta uèení, která udává velikost kroku v opaèném smìru nejvìtšího gradientu. Gradientní sestup pro konvexní cenové funkce vdy nalezne globální minimum a pro nekonvexní cenové funkce lokální minimum \cite{dl,karpathy,ruder}.

\subsection{Stochastickı gradientní sestup}
Aèkoliv je gradientní sestup efektivní optimalizaèní metoda, pøi optimalizaci nad velkım objemem dat mùe bıt velice pomalá a vıpoèetnì nároèná, jeliko pro jednu zmìnu parametrù je tøeba spoèítat gradient nad celou datovou sadou. Jednou z nejpouívanìjších modifikací gradientního sestupu, která tyto problémy øeší, je stochastickı gradientní sestup.

Pøedpokládejme cenovou funkci ve tvaru záporného logaritmu podmínìné pravdìpodobnosti
\begin{equation}
J(\boldsymbol{\theta}) =  \mathbb{E}_{\boldsymbol{x},  \boldsymbol{y} \sim p_{data}} L(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{\theta}) = \dfrac{1}{m} \sum_{i=1}^{m} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}),
\end{equation}
kde $ p_{data} $ je mnoina trénovacích dat o velikost $ m $ a $ L(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{\theta}) = -\log p(\boldsymbol{y} \mid \boldsymbol{x}; \boldsymbol{\theta}) $ je cena pro jedno pozorování v závislosti na souboru parametrù $ \boldsymbol{\theta} $. Pro takto definovanou cenovou funkci by gradientní sestup musel spoèítat gradient
\begin{equation}
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \dfrac{1}{m} \sum_{i=1}^{m} \nabla_{\boldsymbol{\theta}} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}),
\end{equation}
jeho komplexita je $ O(m) $. Stochastickı gradientní sestup nahlíí na gradient jako na støední hodnotu a tudí se pøedpokládá, e mùe bıt aproximován menšími soubory pozorování náhodnì vybíranımi z datové sady, tzv. dávky. V optimalizaèním kroku je tedy náhodnì vybrána dávka pozorování $ \boldsymbol{b} = \{ x^{1}, \ldots, x^{m'} \} $, kde $ m' $ se volí jako malé èíslo v závislosti na vıpoèetní kapacitì a velikosti $ m $ úplné datové sady. Pøi trénování modelu na grafické kartì (GPU) je vhodné volit velikost dávky jako mocninu dvou a to v rozmezí 8 a 256, aby mohly bıt plnì vyuity vektorizované operace GPU. Menší dávky mohou mít zároveò regularizaèní efekt díky šumu, kterı vnášejí do uèícího se procesu. Odhad gradientu je pak vypoèten s vyuitím dávky $ \boldsymbol{b} $
\begin{equation}
g = \dfrac{1}{m'} \nabla_{\boldsymbol{\theta}} \sum_{i=1}^{m'} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}).
\end{equation}
Zmìna parametrù je pak dána dle pravidla
\begin{equation}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon g.
\end{equation}
Pro pevnì danou velikost modelu tedy vıpoèetní cena nezávisí na velikosti datové sady $ m $  \cite{dl,ruder}.

\subsection{Další modifikace gradientního sestupu}
Jedním z nejvìtších problémù pøi vyuití gradientního sestupu a gradientního stochastického sestupu je vıbìr konstanty uèení $ \epsilon $. Vysoká hodnota $ \epsilon $ mùe zpùsobit fluktuaci okolo lokálního minima nebo dokonce divergenci optimalizaèního procesu a nízká hodnota $ \epsilon $ mùe mít za následek vırazné zpomalení uèení. Konstanta uèení se tedy v praxi dynamicky mìní v závislosti na poètu epoch $ k $, tedy $ \epsilon_{k} $. Dalším bìnım problémem je uvíznutí v mìlkém lokálním minimu èi sedlovém bodì, co znemoní další uèení. Bylo tedy vytvoøeno nìkolik modifikací základních gradientních algoritmù, které tyto problémy do jisté míry øeší \cite{dl, ruder}.

\subsubsection{Moment}
Moment byl navren za úèelem zrychlení trénování a to pøedevším pøi optimalizace ztrátovıch funkcí, které
mají mnoho mìlkıch lokálních minim nebo v pøípadech, kdy jsou gradienty znaènì zašumìné. Algoritmus momentu vyuívá plovoucí prùmìr minulıch gradientù s exponenciálním zapomínáním a pokraèuje v jejich smìru.

Tento algoritmus zavádí parametr $ \alpha $, kterı udává, jakou rychlostí mají bıt minulé gradienty zapomínány. Pravidlo pro zmìnu parametrù pak vypadá následovnì
\begin{align}
\boldsymbol{v} &\leftarrow \alpha \boldsymbol{v} - \epsilon \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) \right), \\
\boldsymbol{\theta} &\leftarrow \boldsymbol{\theta} + \boldsymbol{v}.
\end{align}
Promìnná $ \boldsymbol{v} $ akumuluje prvky gradientu $ \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) \right) $ s tím, e èím vìtší je parametr $ \alpha $ vùèi konstantì uèení $ \epsilon $, tím více minulé gradienty ovlivòují aktuální smìr kroku. Stejnì jako u konstanty uèení, i parametr $ \alpha $ mùe bıt mìnen v závislosti na èase. Vìtšinou je jako poèáteèní hodnota zvolena 0.5 a je navyšována a na 0.99 \cite{dl, karpathy, ruder}.

\subsubsection{Nesterovùv moment}
Dalším variantou je Nesterovùv moment, kterı dále rozšiøuje algoritmus momentu. Pravidlo pro zmìnu parametrù se zmìní na
\begin{align}
\boldsymbol{v} &\leftarrow \alpha \boldsymbol{v} - \epsilon \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta} + \alpha \boldsymbol{v}), \boldsymbol{y}^{(i)}) \right), \\
\boldsymbol{\theta} &\leftarrow \boldsymbol{\theta} + \boldsymbol{v},
\end{align}
kde parametry $ \alpha $ a $ \epsilon $ odpovídají stejnım parametrùm jako u algoritmu momentu. Hlavním rozdílem oproti algoritmu momentu je, e gradient je vyhodnocen a po aplikaci minulıch gradientù. Nejprve je tedy proveden krok ve smìru akumulovanıch minulıch gradientù a následnì je provedena korekce \cite{dl, karpathy, ruder, nesterov}.

\subsection{ADAM}
Cenová funkce bıvá èasto citlivá na urèité smìry v prostoru parametrù a naopak necitlivá na smìry jiné. Moment tento problém do urèité míry øeší, ale za cenu zavedení nového parametru, kterı je tøeba správnì nastavit. ADAM je adaptivní metoda, která tento problém øeší tím, e zavede vlastní konstantu uèení pro kadı parametr a tyto konstanty pak automaticky v prùbìhu uèení adaptuje.

ADAM pouívá ke zmìnì parametrù plovoucí prùmìr (první centrální moment) gradientù s exponenciálním zapomínáním $ r $ a druhı necentrální moment $ v $. Poèáteèní hodnota obou momentù je nastavena na nulu a zejména v prvních krocích algoritmu je potøeba momenty opravit korekèním faktorem. Parametry $ \beta_{1} $ a $ \beta_{2} $ udávají rychlost zapomínání a $ \delta $ je malá konstanta  \cite{dl, karpathy, ruder, adam}.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
inicializace \\
$ \boldsymbol{r} = 0 $, $ \boldsymbol{v} = 0 $, $ \boldsymbol{t} = 0 $  \\
\While{zastavovací podmínka není splnìna}{
 $ t \leftarrow t + 1 $  \\
 vıpoèet gradientu \\
$ g = \dfrac{1}{m} \nabla_{\boldsymbol{\theta}} \sum_{i} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) $ \\ 
 zmìna odhadu prvního centrálního momentu \\
 $ \boldsymbol{r} \leftarrow \beta_{1}\boldsymbol{r} + (1 - \beta_{1})g $ \\
 zmìna odhadu druhého necentrálního momentu \\
 $ \boldsymbol{v} \leftarrow \beta_{2}\boldsymbol{v} + (1 - \beta_{2})g \odot g $ \\
 korekce odhadu prvního centrálního momentu \\
 $ \hat{\boldsymbol{r}} \leftarrow \dfrac{\boldsymbol{r}}{1- \beta_{1}^{t}} $ \\
 korekce odhadu druhého necentrálního momentu \\
 $ \hat{\boldsymbol{v}} \leftarrow \dfrac{\boldsymbol{v}}{1- \beta_{2}^{t}} $ \\
 zmìna parametrù \\
 $ \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + -\epsilon \dfrac{\hat{\boldsymbol{r}}}{\sqrt{\hat{\boldsymbol{v}} + \delta}} $
}
\end{algorithm}
\begin{center}
Algoritmus 1: ADAM.
\end{center}

\newpage
\section{Neuronové sítì}
Neuronová sí je algoritmus, kterı je schopen aproximovat i silnì nelineární funkce a zároveò je schopen dosáhnout vysoké míry statistické generalizace. Tento parametrickı model bıvá zpravidla sloen z nìkolika vrstev reprezentovanımi vektory, jejich dimenze udává šíøku modelu. Prvky tìchto vektorù, jednotky èi perceptrony, pracují paralelnì a reprezentují funkce zobrazující vstupní vektor na skalár. Cílem je natrénovat parametry tohoto modelu tak, aby dokázal splnit zadanou úlohu s minimální chybou. Jedná se tedy o optimalizaèní úlohu.

Základním rozdílem mezi bìnou optimalizací a optimalizací v neuronovıch sítí je, e optimalizace v neuronovıch sítích a ve strojovém uèení obecnì probíhá nepøímo. To znamená, e aèkoliv optimalizujeme zvolenou metriku $ P $, která v jistém smyslu kvantifikuje vıkon algoritmu na dané úloze, minimalizujeme jinou cenovou funkci $ J(\boldsymbol{\theta}) $ za úèelem minimalizace metriky $ P $. V bìném optimalizaèním problému bychom minimalizovali pøímo cenovou funkci $ J(\boldsymbol{\theta}) $ za úèelem její minimalizace \cite{dl, karpathy, bishop}.

\subsection{Cenová funkce}
Jedním ze zásadních aspektù pøi návrhu neuronovıch sítí je právì vıbìr cenové funkce. Vıbìr cenové funkce závisí na dané úloze a poadovaném vıstupu. Cenovou funkci definujeme stejnım zpùsobem jako u ostatních parametrickıch modelù, které generují hustotu pravdìpodobnosti $ p(\boldsymbol{y} \mid \boldsymbol{x} ; \boldsymbol{\theta}) $ a pøi trénování vyuívají principu maximální vìrohodnosti. Vìtšinou tedy cenovou funkci definujeme jako vzájemnou entropii mezi trénovacími daty a predikcemi modelu
\begin{equation}
J(\boldsymbol{\theta}) = -\mathbb{E}_{\boldsymbol{x}, \boldsymbol{y} \sim \hat{p}_{data}} \log p_{model} (\boldsymbol{y} \mid \boldsymbol{x}).
\end{equation}
Pøedpis cenové funkce se liší model od modelu v závislosti na tvaru $ \log p_{model} $ a kromì definice ceny také mùe obsahovat regularizaèní prvky. Vıhodou vyuití principu maximální vìrohodnosti je, e specifikací modelu $ p(\boldsymbol{y} \mid \boldsymbol{x}) $ zároveò definujeme cenovou funkci $ \log p(\boldsymbol{y} \mid \boldsymbol{x}) $ \cite{dl, bishop}.

\subsection{Dopøedné neuronové sítì}
Dopøedné neuronové sítì, obèas také nazıvané vícevrstvé perceptrony, jsou základním typem neuronovıch sítí. Jejich cílem je aproximovat urèitou funkci $ f^{*} $, napø. klasifikátor $ \boldsymbol{y} = f^{*}(\boldsymbol{x}) $ zobrazuje vstup $ \boldsymbol{y} $ na vıstupní tøídu $ \boldsymbol{x} $. Dopøedná neuronová sí tedy definuje zobrazení $ \boldsymbol{y} = f^{*}(\boldsymbol{x} ; \boldsymbol{\theta}) $ a uèí se optimální hodnoty parametrù $ \boldsymbol{\theta} $ pøi kterıch by mìla bıt dosaena nejlepší aproximace funkce.

Jak název napovídá, tok informací smìøuje od vstupu $ \boldsymbol{x} $ pøes nìkolik po sobì jdoucích vıpoèetních vrstev definujících $ f $ a k vıstupu $ \boldsymbol{y} $. Jedná se tedy o sloení nìkolika rùznıch funkcí do øetìzové struktury. Takto definovanı model lze také popsat jako orientovanı acyklickı graf, kterı urèuje závislosti mezi funkcemi. Mìjme napøíklad sí sloenou ze tøí funkcí $ f^{(1)} $, $ f^{(2)} $ a $ f^{(3)} $ spojenıch do øady $ f( \boldsymbol{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\boldsymbol{x}))) $. V tomto pøípadì nazıváme  $ f^{(1)} $ první skrytou vrstvou, $ f^{(2)} $ druhou skrytou vrstvou a $ f^{(3)} $ vıstupní vrstvou. Celková délka tohoto øetìzce pak udává hloubku modelu \cite{dl}.

Cílem trénování sítì je, aby se nauèila odpovídající zobrazení mezi $ f(\boldsymbol{x}) $ a $ f^{*}(\boldsymbol{x}) $. Uèení probíhá na základì trénovacích dat, která poskytují zašumìná aproximovaná pozorování $ f^{*}(\boldsymbol{x}) $ vyhodnocena v rùznıch bodech. Ke kadému pozorování $ \boldsymbol{x} $ je k dispozici také skuteèná hodnota $ \boldsymbol{y} \approx f^{*}(\boldsymbol{x}) $. Vıstupní vrstva sítì se tedy pro kadou trénovací hodnotu $ \boldsymbol{x} $ snaí vyprodukovat hodnotu blízkou $ \boldsymbol{y} $. Chování skrytıch vrstev není pøímo dáno trénovacími daty a sí se je musí nauèit vyuívat k získání poadovaného vısledku, tedy k aproximaci $ f^{*}(\boldsymbol{x}) $ \cite{dl, bishop}.

\subsubsection{Architektura}
Klíèovım prvkem pøi návrhu neuronovıch sítí je jejich architektura. Architekturou sítì èi modlu rozumíme celkovou strukturu sítì - kolik má mít jednotek a jak mají bıt tyto jednotky mezi sebou propojeny.

Jak ji bylo uvedeno, neuronové sítì jsou tvoøeny skupinami jednotek, které jsou uspoøádány do jednotlivıch vrstev. Vìtšina architektur tyto vrstvy uspoøádává do øetìzové struktury, kde kadá vrstva je funkcí vrstvy, která ji pøedchází. V této struktuøe je pak první vrstva definována jako
\begin{equation}
\boldsymbol{h}^{(1)} = g^{(1)}\left( \boldsymbol{W}^{(1)\top} \boldsymbol{x} + \boldsymbol{b}^{(1)} \right),
\end{equation}
druhá vrstva jako 
\begin{equation}
\boldsymbol{h}^{(2)} = g^{(2)}\left( \boldsymbol{W}^{(2)\top} \boldsymbol{x} + \boldsymbol{b}^{(2)} \right),
\end{equation}
a tak dále, kde $ g^{(i)} $ je aktivaèní funkce vrstvy $ i $, $ \boldsymbol{W}^{(i)\top} $ je váhová matice vrstvy $ i $ a $ \boldsymbol{b}^{(i)} $ je prahovı vektor vrstvy $ i $. 

Pro takto definovanou øetìzovou architekturu je pak hlavním problémem urèení hloubky sítì a šíøky kadé vrstvy. Vhodnou architekturu pro danou úlohu je tøeba nalézt pomocí experimentù zaloenıch na sledování chyby na validaèní datové sadì a apriorní znalosti o úloze a datech \cite{dl, karpathy, bishop, hastie}.

\subsubsection{Vıstupní jednotky}
Reprezentace vıstupu je úzce spojena s danou úlohou a tím i s vıbìrem cenové funkce. Pøedpokládejme, e dopøedná neuronová sí poskytuje vıstupní vrstvì soubor skrytıch pøíznakù  $ \boldsymbol{h} = f(\boldsymbol{x} ; \boldsymbol{\theta}) $, tj. vıstup poslední skryté vrstvy. Cílem vıstupní vrstvy je pak provést urèitou transformaci tìchto pøíznakù, aby sí plnila úlohu, pro kterou byla navrena. Tato transformace je provedena pouitím aktivaèní funkce $ g(\boldsymbol{h}) $. Aèkoliv existuje mnoho aktivaèních funkcí, které lze ve vıstupní vrstvì vyuít, zde se zamìøíme pouze na aktivaèní funkci softmax, která je vyuívána v experimentech provedenıch v rámci této práce.

Aktivaèní funkce softmax je vyuívána pro úlohy, kde je tøeba reprezentovat vıstup jako hustotu pravdìpodobnosti diskrétní promìnné s $ n $ monımi hodnotami (vıstupními tøídami). K odvození aktivaèní funkce softmax vyuijeme znalosti o úloze binární klasifikaci, pøi které predikujeme hodnotu
\begin{equation}
\hat{\boldsymbol{y}} = P(\boldsymbol{y} = 1 \mid \boldsymbol{x}),
\end{equation}
kde $ \hat{\boldsymbol{y}} \in \interval{0}{1} $. Aby byla zajištìna numerická stabilita pøi optimalizaci, budeme radìji hodnotu
\begin{equation}
\boldsymbol{z} = \log \tilde{P} (\boldsymbol{y} = 1 \mid \boldsymbol{x}).
\end{equation}
Aplikací exponenciální funkce a následnou normalizací bychom pak dostali Bernoulliho rozloení pravdìpodobnosti øízeného sigmoidální funkcí.

Pro generalizaci tohoto postupu pro diskrétní promìnnou s $ n $ hodnotami je tedy potøeba získat vektor $ \hat{\boldsymbol{y}} $, kde $ \hat{\boldsymbol{y}}_{i} = P(\boldsymbol{y} = i \mid \boldsymbol{x}) $. Pro kadı prvek $ \hat{\boldsymbol{y}}_{i}  $ musí platit $ \hat{\boldsymbol{y}}_{i} \in \interval{0}{1} $ a zároveò $ \sum_{i} \hat{\boldsymbol{y}}_{i}  = 1 $, aby bylo moné tento vektor interpretovat za hustotu pravdìpodobnosti. Nejprve je potøeba provést predikci vrstvou s lineární aktivaèní funkcí, která predikuje nenormalizované logaritmované pravdìpodobnosti
\begin{equation}
\boldsymbol{z} = \boldsymbol{W}^{\top} \boldsymbol{h} + \boldsymbol{b},
\end{equation}
kde $ \boldsymbol{z}_{i} = \log \tilde{P}(\boldsymbol{y} = i \mid \boldsymbol{x}) $. Softmax funkce pak aplikuje exponenciální funkci a normalizuje $ \boldsymbol{z} $, èím získáme poadované $ \hat{\boldsymbol{y}} $.
\begin{equation}
softmax(\boldsymbol{z}_{i}) = \dfrac{\exp(\boldsymbol{z}_{i})}{\sum_{j} \exp(\boldsymbol{z}_{j})}
\end{equation}
Natrénováním parametrù modelu pak bude vıstupní vrstva s aktivaèní funkcí softmax predikovat podíly poètù všech pozorovanıch vısledkù v trénovací datové sadì \cite{dl}.
\begin{equation}
softmax(\boldsymbol{z}(\boldsymbol{x} ; \boldsymbol{\theta}))_{i} \approx \dfrac{\sum_{j=1}^{m} 1_{\boldsymbol{y}^{(j)} = i, \boldsymbol{x}^{(j)} = \boldsymbol{x}}}{\sum_{j=1}^{m} 1_{\boldsymbol{x}^{(j)} = \boldsymbol{x}}}
\end{equation}

\subsubsection{Skryté jednotky}
Jak název napovídá, skryté jednotky jsou jednotky skryté vrstvy, jejich vstupem je vektor $ \boldsymbol{x} $, kterı je transformován na $ \boldsymbol{z} = \boldsymbol{W}^{\top} \boldsymbol{x} + \boldsymbol{b} $. Na takto transformovanı vstup je pak po prvcích aplikována nelineární aktivaèní funkce  $ g(\boldsymbol{z}) $. Volba aktivaèních funkcí skrytıch vrstev vyaduje mnoho experimentù a vyhodnocení pøesnosti modelu na validaèní datové sadì. V dnešní dobì se pro dopøedné neuronové sítì vìtšinou volí jednotky s aktivaèní funkcí ReLU (z anglického "rectified linear unit") èi její modifikace, pro sítì rekurentní jsou pak voleny funkce hyperbolickı tangens a hard sigmoid.
\begin{itemize}
\item \textbf{ReLU} - tyto jednotky vyuívají aktivaèní funkci $ g(\boldsymbol{z}) = \max\{0, \boldsymbol{z}\} $. Jedná se tedy o lineární jednotky s prahem v bodì nula - levá polovina jejich definièního je rovna nule. To zaruèuje rychlı vıpoèet a vysokou hodnotu gradientu, kdykoliv je jednotka aktivní. Zásadním nedostatkem je, e tyto jednotky se pomocí gradientních metod nemohou uèit z pozorování, které mají aktivaèní hodnotu rovnou nule. Existuje proto nìkolik modifikací, které zajistí, e jednotky budou mít gradient všude (napø. Leaky ReLU, PReLU, Maxout).
\item \textbf{sigmoid} a \textbf{tanh} - tyto jednotky vyuívají logistickou funkci (sigmoid) $ g(\boldsymbol{z}) = \sigma (\boldsymbol{z}) $, resp. hyperbolickı tangens $ \tanh (\boldsymbol{z}) = 2\sigma (2\boldsymbol{z}) - 1 $. Hlavním nedostatkem sigmoidální funkce je její citlivost a náchylnost k saturaci, kdy mùe dojít k tzv. explozi èi vymizení gradientu a sí nebude schopná se uèit.
\item \textbf{hard sigmoid} - tyto jednotky vyuívají aktivaèní funkci $ g(\boldsymbol{z}) = \max(0, \min(1, \frac{\boldsymbol{z} + 1}{2}) $. Jedná se o lineární aproximaci funkce sigmoid a díky své vıpoèetní nenároènosti jsou vyuívány v sítích typu LSTM \cite{dl, karpathy}.
\end{itemize}

\subsubsection{Algoritmus zpìtného šíøení}
Dopøedná neuronová sí pøijme na vstupu poèáteèní informaci o pozorování $ \boldsymbol{x} $, kterou poté šíøí skrz skryté jednotky ve všech skrytıch vrstvách a k vıstupní vrstvì, která vygeneruje odhad $ \hat{\boldsymbol{y}} $. Informaèní tok tedy proudí skrz sí smìrem dopøedu a tomuto procesu se øíká dopøedné šíøení. V trénovací fázi probíhá dopøedné šíøení tak dlouho, dokud není vygenerována skalární cena $ J(\boldsymbol{\theta}) $. Algoritmus zpìtného šíøení (anglicky backpropagation) pak umoní zpìtnı tok informace od ceny skrz sí za úèelem vıpoètu gradientu.

Analytickı vıpoèet gradientu je pomìrnì pøímoèarı, nicménì numericky mùe bıt velice nároènı. Algoritmus zpìtného šíøení provádí vıpoèet gradientu $ \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) $ pomocí jednoduché a vıpoèetnì nenároèné procedury. Takto vypoètenı gradient je pak vyuit pøi uèení pomocí gradientních metod \cite{dl, karpathy}.

Algoritmus zpìtného šíøení vyuívá k vıpoètu gradientu øetìzové pravidlo. Øetìzové pravidlo se bìnì vyuívá k vıpoètu derivací sloenıch funkcí, které jsou sloeny z funkcí, jejich derivace jsou známé. Mìjme reálné èíslo $ x $ a funkce $ f : \mathbb{R} \mapsto \mathbb{R} $ a $ g: \mathbb{R} \mapsto \mathbb{R} $. Dále pøedpokládejme, e $ y = g(x) $ a $ z = f(g(x)) = f(y) $. Øetìzové pravidlo je pak dáno jako
\begin{equation}
\dfrac{dz}{dx} = \dfrac{dz}{dy}\dfrac{dy}{dx}.
\end{equation}
Toto pravidlo lze snadno rozšíøit ze skalárního pøípadu. Pøedpokládejme, e $ \boldsymbol{x} \in \mathbb{R}^{n} $, $ \boldsymbol{y} \in \mathbb{R}^{n}  $, $ g: \mathbb{R}^{m} \mapsto \mathbb{R}^{n} $ a $ f : \mathbb{R}^{n} \mapsto \mathbb{R} $. Pokud  $ \boldsymbol{y} = g(\boldsymbol{x}) $ a $ \boldsymbol{z} =  f(\boldsymbol{y}) $, pak
\begin{equation}\label{eq:chain}
\dfrac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}_{i}} = \sum_{j} \dfrac{\partial \boldsymbol{z}}{\partial \boldsymbol{y}_{j}} \dfrac{\partial \boldsymbol{y}_{j}}{\partial \boldsymbol{x}_{i}}
\end{equation}
Zapíšeme-li rovnici \eqref{eq:chain} vektorovì, získáme tvar
\begin{equation}
\nabla_{\boldsymbol{x}} \boldsymbol{z} = \left( \dfrac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \right)^{\top} \nabla_{\boldsymbol{y}} \boldsymbol{z},
\end{equation}
kde $ \dfrac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \in \mathbb{R}^{n \times m} $ je Jakobián $ \boldsymbol{x} $.
Toto pravidlo lze dále snadno rozšíøit i na tensory, jen jsou v neuronovıch sítích èasto vyuívány (podrobnìji v \cite{dl, chain}).

Pøedpokládejme sí o hloubce $ l $, kde kadé vrstvì náleí váhová matice $ \boldsymbol{W}^{(i)} $, $ i \in \{ 1, \ldots, l \} $ a prahovı vektor $ \boldsymbol{b}^{(i)} $. Ztrátová funkce $ L(\boldsymbol{y}, \hat{\boldsymbol{y}}) $ závisí na skuteèné hodnotì $ \boldsymbol{y} $ a na vıstupu sítì $ \boldsymbol{\hat{y}} $, kterı sí vygeneruje pro vstup $ \boldsymbol{x} $. Celková cena $ J $ pro zjednodušení odpovídá pøímo ztrátové funkci $ L $ a neobsahuje ádnou regularizaèní sloku.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
$ \boldsymbol{h}^{(0)} = \boldsymbol{x} $ \\
\For{$ k = 1,\ldots,l $}{
	$ \boldsymbol{a}^{(k)} = \boldsymbol{W}^{(k)} \boldsymbol{h}^{(k-1)} + \boldsymbol{b}^{(k)}  $ \\
	$ \boldsymbol{h}^{(k)} = f(\boldsymbol{a}^{(k)}) $
}
$ \hat{\boldsymbol{y}} = \boldsymbol{h}^{(l)} $ \\
$ J = L(\boldsymbol{y}, \hat{\boldsymbol{y}}) $
\end{algorithm}
\begin{center}
Algoritmus 2: Dopøedné šíøení.
\end{center}

Pøi zpìtném prùchodu jsou poèítány gradienty aktivací $ \boldsymbol{a}^{(k)} $ pro kadou vrstvu $ k $ poèínaje vıstupní vrstvou a k první skryté vrstvì. Tyto gradienty indikují, jak by se mìly zmìnit vıstupy jednotlivıch vrstev, aby došlo ke sníení chyby. Spoètené gradienty vah a prahù mohou bıt rovnou vyuity pro zmìnu parametrù pomocí optimalizaèního algoritmu \cite{dl}.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
vıpoèet gradientu vıstupní vrstvy (po dopøedném prùchodu sítì) \\
$ g \leftarrow \nabla_{\hat{\boldsymbol{y}}} J = \nabla_{\hat{\boldsymbol{y}}} L (\boldsymbol{y}, \hat{\boldsymbol{y}}) $ \\
\For{$ k = l, l-1, \ldots, 1 $}{
	pøevedení gradientu do tvaru pøed aplikací nelineární aktivaèní funkce \\
	$ g \leftarrow \nabla_{\boldsymbol{a}^{(k)}} J = g \odot f'(\boldsymbol{a}^{(k)}) $ \\
	vıpoèet gradientù prahovıch vektorù a váhovıch matic \\
	$ \nabla_{\boldsymbol{b}^{(k)}} J = g $ \\
	$ \nabla_{\boldsymbol{W}^{(k)}} J = g \boldsymbol{h}^{(k-1)\top} $ \\
	šíøení gradientu do niší vrstvy \\
	$ g \leftarrow \nabla_{\boldsymbol{h}^{(k-1)}} J = \boldsymbol{W}^{(k)\top} g $
}
\end{algorithm}
\begin{center}
Algoritmus 3: Zpìtné šíøení.
\end{center}

\subsection{Rekurentní neuronové sítì}
Rekurentní neuronové sítì (dále jen RNN z anglického "recurrent neural networks") jsou typem neuronovıch sítí, které umí zpracovávat sekvenèní data, tj. sekvenci hodnot $ \boldsymbol{x}^{(1)}, \ldots , \boldsymbol{x}^{(\tau)} $, a pøedávat si informace mezi jednotlivımi èasovımi kroky. Základním rozdílem oproti dopøednım neuronovım sítím je zavedení zpìtné smyèky a vyuití sdílení parametrù. RNN tedy netrénuje pro kadı èasovı krok samostatnı soubor parametrù, ale tyto parametry jsou sdíleny pøes všechny prvky sekvence. Díky tomu jsou RNN schopny generalizovat na rùzné délky sekvencí a na rùzné pozice dùleitıch informací v èase (napø. dvì témìø stejné vìty, kde jedna obsahuje datum na zaèátku vìty a druhá na konci).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{rnn_general.png}
    \caption{Schéma rekurentní neuronové sítì. Pøevzato z \cite{colahNN}.}
    \label{fig:rnn_general}
\end{figure}

Sdílení parametrù je zajištìno "rozvinutím" rekurentního vıpoètu do grafu s opakující se strukturou. Prvek sekvence v kadém èasovém kroku je tedy zpracováván stejnou sítí. Pro jednoduchost pøedpokládejme, e RNN pracuje se sekvencí obsahující vektory $ \boldsymbol{x}^{(t)} $, $ t = 1, \ldots, \tau $ a model je ve tvaru
\begin{equation}\label{eq:unfold}
\boldsymbol{s}^{(t)} = f(\boldsymbol{s}^{(t-1)}; \boldsymbol{\theta}),
\end{equation}
kde $ \boldsymbol{s} $ je stav systému. Tento rekurzivní vztah je moné pro koneènı poèet krokù $ \tau $ rozvinout $ \tau $-krát. Napøíklad pro model danı vztahem \eqref{eq:unfold} a $ \tau = 3 $ získáme
\begin{equation}
\boldsymbol{s}^{(3)} = f(\boldsymbol{s}^{(2)}; \boldsymbol{\theta}) = f(f(\boldsymbol{s}^{(1)}; \boldsymbol{\theta}); \boldsymbol{\theta}).
\end{equation}
Tímto rozvinutím zajistíme, e vztah neobsahuje ádnou rekurenci a je moné ho reprezentovat jako acyklickı orientovanı graf \cite{dl, colahLSTM}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{rnn_unrolled.png}
	\caption{Ilustrace rozvinutí rekurentní neuronové sítì. Pøevzato z \cite{colahLSTM}.}
    \label{fig:rnn_unrolled}
\end{figure}

Základní rovnici RNN získáme pøidáním závislosti na externím vstupu  $ \boldsymbol{x}^{(t)} $
\begin{equation}
\boldsymbol{h}^{(t)} = f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)}; \boldsymbol{\theta}),
\end{equation}
kde $ \boldsymbol{h} $ je stav skrytıch jednotek. Pøi trénování sítì na danou úlohu se pak sí uèí vyuívat $ \boldsymbol{h}^{(t)} $ jako zobrazení relevantních informací z minulıch èasovıch krokù od vstupu a po $ t $. Toto zobrazení je ztrátové, jeliko se jedná o zobrazení sekvence o promìnné délce $ (\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}) $ na vektor pevnì dané délky $ \boldsymbol{h}^{(t)} $.

Definujme si tedy rovnice pro dopøedné šíøení základní RNN s aktivaèní funkcí hyperbolickı tangens ve skryté vrstvì. Sí je urèena ke klasifikaci $ n $ tøíd, kde vıstup $ \boldsymbol{o} $ reprezentuje nenormalizované pravdìpodobnosti jednotlivıch tøíd vıstupní diskrétní promìnné a aplikací operace softmax je pak získán vıslednı vektor $ \hat{\boldsymbol{y}} $ normalizovanıch pravdìpodobností nad vıstupem. Dopøedné šíøení vyaduje specifikaci poèáteèního stavu $ \boldsymbol{h}^{(0)} $, poté jsou pro kadı èasovı krok $ t = 1, \ldots, \tau $ vypoèteny následující rovnice
\begin{align}
\boldsymbol{a}^{(t)} &= \boldsymbol{b} + \boldsymbol{W} \boldsymbol{h}^{(t-1)} + \boldsymbol{U} \boldsymbol{x}^{(t)} , \\
\boldsymbol{h}^{(t)} &= \tanh(\boldsymbol{a}^{(t)}) , \\
\boldsymbol{o}^{(t)} &= \boldsymbol{c} + \boldsymbol{V} \boldsymbol{h}^{(t)} , \\
\hat{\boldsymbol{y}}^{(t)} &= \text{softmax}(\boldsymbol{o}^{(t)}) ,
\end{align}
kde parametry $ \boldsymbol{b} $ a $ \boldsymbol{c} $ jsou prahové vektory a váhové matice $ \boldsymbol{U} $, $ \boldsymbol{V} $ a $ \boldsymbol{W} $ popoøadì odpovídají skrytım spojením mezi vstupem a skrytou vrstvou, mezi skrytou vrstvou a vıstupem a  mezi skrytou vrstvou a pøedcházející skrytou vrstvou \cite{dl}.

\subsubsection{Algoritmus zpìtného šíøení èasem}
Pro vıpoèet gradientu RNN se vyuívá algoritmus zpìtného šíøení èasem. Jedná se o generalizovanı algoritmus zpìtného šíøení nad rozvinutım vıpoèetním grafem a jeho èasová nároènost je $ O(\tau) $. Èasovou nároènost tohoto algoritmu není moné sníit paralelizací, jeliko dopøednıch prùchod je sekvenèní a hodnoty v aktuálním èasovém kroku jsou závislé na pøedchozích hodnotách.

Pøedpokládejme reprezentaci rozvinuté RNN ve tvaru acyklického orientovaného grafu, jeho uzly obsahují parametry $ \boldsymbol{U} $, $ \boldsymbol{V} $, $ \boldsymbol{W} $, $ \boldsymbol{b} $, $ \boldsymbol{c} $, a sekvenci uzlù indexovanıch èasem $ t $ pro $ \boldsymbol{x}^{(t)} $, $ \boldsymbol{h}^{(t)} $, $ \boldsymbol{o}^{(t)} $ a $ L^{(t)} $. Pro kadı uzel grafu $ N $ pak potøebujeme rekurzivnì spoèítat gradient $ \nabla_{N} L $ v závislosti na uzlech grafu, které tento uzel následují. Zaèneme tedy rekurzi v uzlu, kterı bezprostøednì pøedchází vıslednou ztrátu $ L $ 
\begin{align}
L(\{ \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(\tau)}  \}, \{ \boldsymbol{y}^{(1)}, \ldots, \boldsymbol{y}^{(\tau)} \}) &= - \sum_{t} \log_{p_{model}} \left( \boldsymbol{y}^{(t)} \mid \{ \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)} \}  \right) , \\
&\dfrac{\partial L}{\partial L^{(t)}} = 1.
\end{align}
Pro vıpoèet gradient $ \nabla_{\boldsymbol{o}^{(t)}} L $ nad všemi vıstupy v èase $ t $ pro všechna $ i, t $ je opìt vyuito øetìzové pravidlo
\begin{equation}
(\nabla_{\boldsymbol{o}^{(t)}} L)_{i} = \dfrac{\partial L}{\partial \boldsymbol{o}_{i}^{(t)}} = \dfrac{\partial L}{\partial L^{(t)}} \dfrac{\partial L^{(t)}}{\partial \boldsymbol{o}_{i}^{(t)}} = \hat{\boldsymbol{y}}_{i}^{(t)} - \boldsymbol{1}_{i, \boldsymbol{y}^{(t)}}.
\end{equation}
Vıpoèet gradientu dále pokraèuje pøes další uzly poèínaje koncem sekvence. V koneèném èasovém kroku $ \tau $ má $ \boldsymbol{h}^{(\tau)} $ za následníka pouze $ \boldsymbol{o}^{(\tau)} $, tedy
\begin{equation}
\nabla_{\boldsymbol{h}^{(\tau)}} L = \boldsymbol{V}^{\top} \nabla_{\boldsymbol{o}^{(\tau)}} L.
\end{equation}
Gradient se dál šíøí od $ t = \tau - 1 $ a k $ t = 1 $ s tím, e $ \boldsymbol{h}^{(t)} $ (pro $ t < \tau $) má jako své pøedchùdce $ \boldsymbol{o}^{(t)} $ a $ \boldsymbol{h}^{(t+1)} $. Gradient skryté vrstvy je pak
\begin{align}
\nabla_{\boldsymbol{h}^{(t)}} L &= \left( \dfrac{\partial \boldsymbol{h}^{(t+1)}}{\partial \boldsymbol{h}^{(t)}}  \right)^{\top} (\nabla_{\boldsymbol{h}^{(t+1)}} L) + \left( \dfrac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{h}^{(t)}}  \right)^{\top} ( \nabla_{\boldsymbol{o}^{(t)}} L ) \notag \\
&= \boldsymbol{W}^{\top} (\nabla_{\boldsymbol{h}^{(t+1)}} L) \text{ diag} \left(  1 - (\boldsymbol{h}^{(t+1)})^{2} \right) + \boldsymbol{V}^{\top} (\nabla_{\boldsymbol{o}}^{(t)} L),
\end{align}
kde $  \text{diag} \left(  1 - (\boldsymbol{h}^{(t+1)})^{2} \right) $ je Jakobián hyperbolického tangensu pro skryté jednotky $ i $ v èase $ t+1 $. Na základì gradientù skrytıch stavù lze dopoèítat gradienty parametrù. Vzhledem k tomu, e parametry jsou sdíleny mezi jednotlivımi èasovımi kroky, zavedem nové promìnné $ \boldsymbol{W}^{(t)} $, resp. $ \boldsymbol{U}^{(t)} $, které jsou kopiemi váhovıch matic $ \boldsymbol{W} $, resp. $ \boldsymbol{U} $ a znaèí, jakou mírou tyto váhy pøispívají ke gradienty v èase $ t $ \cite{dl}.
\begin{align}
\nabla_{\boldsymbol{c}} L &= \sum_{t} \left( \dfrac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{c}} \right)^{\top} \nabla_{\boldsymbol{o}^{(t)}} L = \sum_{t} \nabla_{\boldsymbol{o}^{(t)}} L  \\
\nabla_{\boldsymbol{b}} L &= \sum_{t} \left( \dfrac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{b}^{(t)}} \right)^{\top} \nabla_{\boldsymbol{h}^{(t)}} L = \sum_{t} \text{ diag} \left( 1 - (\boldsymbol{h}^{(t)})^{2} \right)  \nabla_{\boldsymbol{h}^{(t)}} L \\
\nabla_{\boldsymbol{V}} L &= \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial \boldsymbol{o}_{i}^{(t)}} \right) \nabla_{\boldsymbol{V}} \boldsymbol{o}_{i}^{(t)} = \sum_{t} (\nabla_{\boldsymbol{o}^{(t)}} L) \boldsymbol{h}^{(t)\top} \\
\nabla_{\boldsymbol{W}} L &= \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial \boldsymbol{h}_{i}^{(t)}} \right) \nabla_{\boldsymbol{W}^{(t)}} \boldsymbol{h}_{i}^{(t)} = \sum_{t} \text{ diag} \left( 1 - (\boldsymbol{h}^{(t)})^{2} \right) (\nabla_{\boldsymbol{h}}^{(t)} L) \boldsymbol{h}^{(t-1)\top} \\
\nabla_{\boldsymbol{U}} L &= \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial \boldsymbol{h}_{i}^{(t)}} \right) \nabla_{\boldsymbol{u}^{(t)}} \boldsymbol{h}_{i}^{(t)} = \sum_{t} \text{ diag} \left( 1 - (\boldsymbol{h}^{(t)})^{2} \right) (\nabla_{\boldsymbol{h}^{(t)}} L) \boldsymbol{x}^{(t)\top} 
\end{align}

\subsubsection{Obousmìrné rekurentní neuronové sítì}
Rekurentní neuronové sítì zachycují do stavu v èase $ t $ pouze informaci z minulosti, tj. informace ze vstupù $ \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t-1)} $ a aktuálního vstupu $ \boldsymbol{x}^{(t)} $. V mnoha úlohách ovšem mùe správná predikce $ \boldsymbol{y}^{(t)} $ záviset i na budoucích vstupech èi na celé sekvenci. Tento problém øeší obousmìrná rekurentní neuronová sí (dále BRNN z anglického "bidirectional recurrent neural network").

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{rnn_bidirectional.png}
    \caption{Schéma obousmìrné rekurentní neuronové sítì. Pøevzato z \cite{colahNN}.}    
    \label{fig:rnn_bidirectional}
\end{figure}

BRNN kombinuje dvì RNN, kde jedna sí se pohybuje v èase kupøedu od zaèátku sekvence, zatímco druhá se pohybuje v èase zpìt od konce sekvence. Tyto dvì sítì mezi sebou sdílí skryté stavy $ \boldsymbol{h}^{(t)} $ (sí dopøedná) a $ \boldsymbol{g}^{(t)} $ (sí zpìtná), co zajistí, e vıstupní jednotky $ \boldsymbol{o}^{(t)} $ jsou závislé jak na minulosti, tak na budoucnosti. Vıstupy skrytıch stavù  $ \boldsymbol{h}^{(t)} $ a  $ \boldsymbol{g}^{(t)} $ nejsou mezi sebou nijak propojeny. Pro vıpoèet gradientù lze opìt vyuít algoritmus zpìtného šíøení èasem dle postupu, kterı je shrnut v algoritmu è. 4 \cite{dl, brnn}.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
\For{$ t = 1,  \ldots, \tau $}{
	dopøedné šíøení dopøednou sítí od $ t = 1 $ do $ t = \tau $ \\
	dopøedné šíøení zpìtnou sítí od $ t = \tau $ do $ t = 1 $ \\
	dopøedné šíøení nad vıstupními jednotkami obou sítí \\
}
\For{$ t = 1,  \ldots, \tau $}{
	zpìtné šíøení nad vıstupními jednotkami obou sítí \\
	zpìtné šíøení dopøednou  sítí od $ t = \tau $ do $ t = 1 $ \\
	zpìtné šíøení zpìtnou sítí od $ t = 1 $ do $ t = \tau $
}
zmìna parametrù
\end{algorithm}
\begin{center}
Algoritmus 4: Postup trénování obousmìrné rekurentní neuronové sítì.
\end{center}

\subsubsection{LSTM}
Rekurentní sítì jsou velice náchylné na problém vymizení èi saturace gradientu. K tìmto problémùm dochází zejména pøi modelování dlouhodobıch závislostí, kdy jsou pøi vıpoètu váhy násobeny nìkolikrát samy sebou a v závislosti na jejich øádu mohou nabıvat velmi malıch èi velmi velkıch hodnot. To má pak za následek trvalou deaktivaci nìkterıch neuronù èi neschopnost sítì se uèit. Tento problém øeší sítì LSTM (z anglického "long short-term memory") a jejich modifikace GRU  (z anglického "gated reccurent unit").

LSTM sítì, na rozdíl od bìnıch RNN, nejsou tvoøeny jednotkami, ale buòkami, které kromì vnìjší rekurence obsahují i rekurenci vnitøní (smyèky). Kadá buòka má stejnı vstup a vıstup jako obyèejná rekurentní jednotka, ale má více parametrù díky systému jednotek opatøenıch bránou, které øídí tok informací stavem buòky. Jedná se o vstupní bránu, zapomínací bránu a vıstupní bránu, které urèují, jaké vstupní informace mají bıt vpuštìny do stavu buòky, jaké informace ze stavu buòky odstranit a jaké informace ze stavu buòky mají bıt vypuštìny na její vıstup.

Nejdùleitìjší komponentou LSTM je stavová jednotka $ \boldsymbol{s}_{i}^{(t)} $ (pro buòku $ i $ v èase $ t $), která je opatøena lineární smyèkou. Váha této smyèky je øízena jednotkou se zapomínací bránou $ \boldsymbol{f}_{i}^{(t)} $, která díky aktivaèní funkci sigmoid umoòuje nastavovat tuto váhu na hodnoty mezi 0 a 1
\begin{equation}
\boldsymbol{f}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{f} + \sum_{j} \boldsymbol{U}_{i,j}^{f} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{f} h_{j}^{(t-1)}  \right),
\end{equation}
kde $ \boldsymbol{x}^{(t)} $ je vstupní vektor v èase $ t $, $ \boldsymbol{h}^{(t)} $ vektor skryté vrstvy obsahující vıstupy všech LSTM buòek, $ \boldsymbol{b}^{f} $, $ \boldsymbol{U}^{f} $ a $ \boldsymbol{W}^{f} $ jsou popoøadì prahovı vektor, váhová matice vstupu a rekurentní váhová matice zapomínací brány.  Stav buòky se pak øídí pravidlem
\begin{equation}
\boldsymbol{s}_{i}^{(t)} = \boldsymbol{f}_{i}^{(t)} \boldsymbol{s}_{i}^{(t-1)} + \boldsymbol{g}_{i}^{(t)} \sigma \left( \boldsymbol{b}_{i} + \sum_{j} \boldsymbol{U}_{i,j} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j} \boldsymbol{h}_{j}^{(t-1)}  \right),
\end{equation}
kde $ \boldsymbol{b} $, $ \boldsymbol{U} $, $ \boldsymbol{W} $ jsou popoøadì prahovı vektor, váhová matice vstupu a rekurentní váhová matice buòky. Stav buòky je také závislı na jednotce s vstupní bránou $ \boldsymbol{g}_{i}^{(t)} $, která se chová podobnì jako jednotka se zapomínací branou
\begin{equation}
\boldsymbol{g}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{g} + \sum_{j} \boldsymbol{U}_{i,j}^{g} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{g} h_{j}^{(t-1)}  \right),
\end{equation}
kde $ \boldsymbol{b}^{g} $, $ \boldsymbol{U}^{g} $ a $ \boldsymbol{W}^{g} $ jsou popoøadì prahovı vektor, váhová matice vstupu a rekurentní váhová matice vstupní brány. Vıstup buòky  $ \boldsymbol{h}_{i}^{(t)} $ je pak øízen pomocí vıstupní brány $ \boldsymbol{q}_{i}^{(t)} $, která opìt øídí propustnost pomocí aktivaèní funkce sigmoid
\begin{align}
\boldsymbol{h}_{i}^{(t)} &= \tanh (\boldsymbol{s}_{i}^{(t)}) \boldsymbol{q}_{i}^{(t)}, \\
\boldsymbol{q}_{i}^{(t)} &= \sigma \left( \boldsymbol{b}_{i}^{o} + \sum_{j} \boldsymbol{U}_{i,j}^{o} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{o} \boldsymbol{h}_{j}^{(t-1)}  \right),
\end{align}
kde $ \boldsymbol{b}^{o} $, $ \boldsymbol{U}^{o} $ a $ \boldsymbol{W}^{o} $ jsou popoøadì prahovı vektor, váhová matice vstupu a rekurentní váhová matice vıstupní brány \cite{dl, colahLSTM, lstm}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{lstm.png}
    \caption{Schéma LSTM buòky. Pøevzato z \cite{colahLSTM}.}
    \label{fig:lstm}
\end{figure}

\subsubsection{GRU}
Sítì GRU dále zjednodušují LSTM architekturu ponecháním pouze nezbytnıch komponent. Vıstup buòky a stav buòky je slouèen do jednoho stavu, kterı je øízen jednotkou vzniklou slouèením jednotky s vstupní bránou a zapomínací bránou. Rovnice dopøedného šíøení má tedy tvar
\begin{equation}
\boldsymbol{h}_{i}^{(t)} = \boldsymbol{u}_{i}^{(t-1)} \boldsymbol{h}_{i}^{(t-1)} + (1 - \boldsymbol{u}_{i}^{(t-1)}) \sigma \left( \boldsymbol{b}_{i} + \sum_{j} \boldsymbol{U}_{i,j} \boldsymbol{x}_{j}^{(t-1)} + \sum_{j} \boldsymbol{W}_{i,j} r_{j}^{(t-1)} \boldsymbol{h}_{j}^{(t-1)}  \right) ,
\end{equation}
kde jednotka $ \boldsymbol{u} $ modifikuje stav buòky
\begin{equation}
\boldsymbol{u}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{u} + \sum_{j} \boldsymbol{U}_{i,j}^{u} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{u} \boldsymbol{h}_{j}^{(t)} \right)
\end{equation}
a jednotka $ \boldsymbol{r} $ øídí, které informace stavu budou vyuity pro vıpoèet pøíštího cílového stavu
\begin{equation}
\boldsymbol{r}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{r} + \sum_{j} \boldsymbol{U}_{i,j}^{r} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{r} \boldsymbol{h}_{j}^{(t)} \right ).
\end{equation}
Takto definované jednotky s branami umoòují vhodnì vybírat èásti stavového prostoru a uchovávat je ve stavu buòky. Zároveò slouèením vıstupu a stavu buòky dochází k vıznamnému sníení pamìové a vıpoèetní nároènosti \cite{dl, colahLSTM}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{gru.png}
    \caption{Schéma GRU buòky. Pøevzato z \cite{colahLSTM}.}
    \label{fig:gru}
\end{figure}


\newpage
\section{Kapacita modelu}
Základní vlastností kadého modelu by mìla bıt generalizace. To znamená, e model musí bıt schopnı správnì klasifikovat nejen pozorování, na kterıch byl natrénován, ale i vìtšinu novıch, døíve nevidìnıch pozorování. Pøi trénování modelu se tedy bìnì dìlí datová sada na tøi - trénovací, validaèní a testovací, kdy pøedpokládáme, e tyto sady podléhají stejnému rozloení a jednotlivá pozorování jsou nezávislá. Cílem trénování modelu je nalézt takové parametry, pro které bude mít model podobnou chybu na všech tøech datovıch sadách. Model by mìl tedy splòovat tyto vlasnosti:
\begin{enumerate}
\item musí minimalizovat chybu na trénovací sadì,
\item musí minimalizovat rozdíl mezi chybou nad trénovací a testovací (popø. validaèní) sadou.
\end{enumerate}
Bìhem optimalizace parametrù èasto dochází ke dvìma neádoucím jevùm:
\begin{itemize}
\item \textbf{underfitting} (podtrénování) - k underfittingu dochází, pokud model není schopnı dostateènì minimalizovat chybu na trénovací sadì, tj. není schopnı se nauèit informaèní souvislosti obsaené v trénovacích pozorováních;
\item  \textbf{overfitting} (pøetrénování) - k overfittingu dochází, pokud model není schopnı minimalizovat rozdíl mezi chybou nad trénovací a testovací sadou, tj. model si "zapamatuje" trénovací data místo toho, aby se nauèil souvislosti obsaené v trénovacích pozorováních a na novıch, dosud nevidìnıch pozorováních selhává \cite{dl, karpathy, hastie}.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{overfitting.eps}
    \caption{Pøíklad overfittingu a underfittingu.}
    \label{fig:overfitting}
\end{figure}


Oba tyto jevy lze ovlivnit pomocí tzv. kapacity modelu nebo pomocí regularizaèních metod. Modely s nízkou kapacitou jsou náchylné na underfitting a naopak modely s vysokou kapacitou jsou náchylné na overfitting. Kapacita modelu je vìtšinou dána pøímo zvolenou architekturou, kde sníením poètu parametrù sítì omezíme poèet volnıch stupòù volnosti a tím lze získat vyšší generalizaci \cite{dl, karpathy}.

\subsection{Regularizace}
Regularizace umoòuje navıšit generalizaci modelu bez toho, aby byla ovlivnìna kapacita modelu - nevyadují zmìnu architektury modelu. Jedná se o modifikaci uèícího se algoritmu, která sniuje generalizaèní chybu, ale zároveò nesniuje chybu trénovací \cite{dl}.

\subsubsection{Penalizace velikosti parametrù}
Populární metodou pro regularizaci jsou penalizaèní metody a to zejména L1 a L2 penalizace. Tyto penalizaèní metody nepøímo omezují kapacitu modelu pøidáním penalizace $ \Omega(\boldsymbol{\theta}) $, která odpovídá normì parametrù, k cenové funkci $ J $. Regularizovaná cenová funkce pak vypadá následovnì
\begin{equation}
\tilde{J}(\boldsymbol{\theta} ; \boldsymbol{x}, \boldsymbol{y}) = J(\boldsymbol{\theta} ; \boldsymbol{x}, \boldsymbol{y}) + \alpha \Omega(\boldsymbol{\theta}),
\end{equation}
kde síla regularizace je øízena parametrem $ \alpha $ (s vyšší hodnotou regularizace roste). Trénovací algoritmus pak minimalizuje jak pùvodní cenovou funkci $ J $, tak i zvolenou metriku velikosti parametrù $  \Omega(\boldsymbol{\theta}) $. V pøípadì neuronovıch sítí jsou regularizovány pouze váhy a prahy jsou ponechány bez regularizace  \cite{dl, karpathy}.

\subsubsection{Zanesení šumu}
Další metodou regularizace, která neovlivòuje model samotnı, je augmentace trénovacích dat ve formì aditivního šumu. Aèkoliv neuronové sítì nejsou obecnì robustní vùèi šumu, vìtšinu klasifikaèních i regresních úloh jsou schopny øešit i pokud jsou vstupní data zatíeny malım náhodnım šumem. Trénovací pozorování jsou tedy vdy pøed vstupem do modelu zatíena novım náhodnım šumem, kterı vede k vyšší generalizaci vısledného modelu \cite{dl}.

\subsubsection{Pøedèasné ukonèení}
Pøi trénování velkıch modelù s dostateènì velkou reprezentaèní kapacitou èasto dochází k jevu, kdy chyba na trénovací sadì pomalu bìhem èasu klesá, zatímco chyba na validaèní sadì pomalu roste. Vrátíme-li se k nastavení parametrù v èase s nejniší validaèní chybou, mùeme získat lepší model s niší chybou i na testovací sadì.

Algoritmus pøedèasného ukonèení (early stopping) je snadno implementovatelnı a vıpoèetnì nenároènı. Pokadé, kdy dojde k poklesu chyby na validaèní sadì, uloíme si kopii parametrù modelu. Jakmile trénování modelu skonèí, vrátíme se k nejlepšímu souboru parametrù místo ponechání souboru parametrù z poslední trénovací iterace. Trénování je pøedèasnì ukonèeno, pokud ádná zmìna parametrù nevede k niší chybì na validaèní sadì, ne které bylo dosaeno s aktuálnì nejlepším souborem parametrù, pro pøedem danı poèet trénovacích iterací.

Tato forma regularizace nevyaduje témìø ádné zmìny trénovacího procesu èi modifikaci cenové funkce. Tento algoritmus lze vyuít buï samostatnì nebo spolu s libovolnou regularizaèní metodou \cite{dl}.

\subsubsection{Dropout}
Dropout je vıpoèetnì nenároènou metodou regularizace, která nejlépe funguje pro modely, které vyuívají dávkové uèení s malımi kroky. Pokadé, kdy pozorování vstoupí do dávky, je vytvoøena binární maska, která je pak aplikována na všechny vstupy skrytıch jednotek v síti. Tato maska je pro kadou jednotku vytvoøena náhodnì nezávisle na ostatních. Jednotky, jejich odpovídající prvek v masce je roven nule, jsou pak pro dané pozorování vylouèeny z uèícího se procesu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{dropout.png}
    \caption{Vlevo - pøed aplikací dropoutu, vpravo - po aplikaci dropoutu. Pøevzato z \cite{dropout}.}
    \label{fig:overfitting}
\end{figure}

Pravdìpodobnost, e v masce na danou pozici bude vybrána hodnota 1, je dána pøedem zvolenım parametrem. Vìtšinou se volí pravdìpodobnost 0.8, e bude pøi trénování vyuita vstupní jednotka, a pravdìpodobnost 0.5, e bude vyuita skrytá jednotka.

Maskováním se do uèícího procesu vnáší jistá forma šumu, která má za následek adaptivní poškození informaèního obsahu na vstupu sítì a na vstupu skrytıch jednotek. Tím je zamezeno tomu, aby se urèitá jednotka zamìøila na rozpoznávání jednoho signifikantního pøíznaku, zatímco jiná jednotka by byla témìø nevyuita. Vylouèením náhodnıch jednotek z uèícího se procesu jsou tak jednotky donuceny "rozdìlit" si informace mezi sebou.

Dropout opìt mùe bıt vyuit spolu s ostatními metodami regularizace \cite{dl, karpathy, dropout}.

\clearpage
\section{CTC}
Tato práce se vìnuje úloze klasifikace fonémù, jejím cílem je pro danou zvukovou nahrávku získat odpovídající sekvenci fonému. Zvuková nahrávka je tedy rozdìlena na malé segmenty a tìmto segmentùm jsou pak pøiøazeny fonémy, které se v jednotlivıch segmentech vyskytují (více v \cite{bp}). K tomuto oznaèkování segmentù se vìtšinou vyuívají pøedtrénované modely, které ovšem vyadují jisté pøedpoklady, mohou bıt náchylné vùèi šumu a vısledné oznaèkování mùe bıt nepøesné, jeliko ne vdy lze pøesnì urèit hranici mezi jednotlivımi fonémy.

Bìnì vyuívané cenové funkce jsou ovšem definovány pro jednotlivé prvky trénovací sekvence (segmenty) a umoòují pouze klasifikaci nezávislıch fonému. To znamená, e takto definované cenové funkce je moné vyuít pouze pro nezávislou predikci fonémù v jednotlivıch segmentech, kdy vıstupní sekvence má stejnou délku jako vstupní sekvence. K získání vısledné sekvence fonémù je tøeba vıstupy sítì dále zpracovat pomocí dekodéru (napø. pro vstupní sekvenci o dvanácti segmentech: $ aahahhoohooj \rightarrow  ahoj $).

V této kapitole si uvedeme metodu CTC (z anglického "connectionist temporal classification"), díky které lze vyuít rekurentní neuronové sítì pøímo k predikci vısledné sekvence fonémù bez potøeby dalšího zpracování vıstupu sítì \cite{bp, distill, ctc}.


\subsection{Formální definice úlohy}
Mìjme trénovací mnoinu $ S $ z pevnì daného rozloení $ \mathcal{D}_{\mathcal{X} \times \mathcal{Z}} $. Prostor vstupních hodnot $ \mathcal{X} = (\mathcal{R})^{*} $ je mnoina všech sekvencí tvoøenıch $ m $ dimenzionálními reálnımi vektory. Prostor vıstupních hodnot $ \mathcal{Z} = L^{*} $ je mnoina všech sekvencí nad koneènou abecedou $ L $ znaèek (fonémù). Prvky $ L^{*} $ budeme nazıvat sekvencí znaèek (sekvence fonému) èi znaèkováním. Kadé pozorování v $ S $ je tvoøeno párem $ (\boldsymbol{x}, \boldsymbol{z}) $, kde cílová sekvence $ \boldsymbol{z} = (z_{1}, z_{2}, \ldots , z_{U}) $ je nejvıše tak dlouhá, jako vstupní sekvence $ \boldsymbol{x} = (x_{1}, x_{2}, \ldots , x_{T}) $, tedy $ U \leq T $. Vzhledem k tomu, e vstupní a cílové sekvence nejsou obecnì stejnì dlouhé, není moné je pøedem zarovnat, tj. pøiøadit prvky cílové sekvence k vstupním prvkùm sekvence.

Cílem je natrénovat klasifikátor $ h: \mathcal{X} \mapsto \mathcal{Z} $ za vyuití $ S $, kterı minimalizuje vhodnì zvolenou metriku pro danou úlohu (napø. minimalizace poètu transkripèních chyb) \cite{distill, ctc}.

\subsection{Reprezentace vıstupu}
Aby bylo moné vyuít RNN spolu s CTC, je tøeba vhodnì zvolit reprezentaci vıstupu sítì. Vyuijeme-li aktivaèní funkci softmax pro vıstupní jednotky, mùeme transformovat vıstupy sítì do tvaru podmínìné hustoty pravdìpodobnosti nad danou abecedou znaèek a vyuít sí jako klasifikátor, kdy vstupní sekvence klasifikujeme vıbìrem nejvíce pravdìpodobného znaèkování. Vıstupní vrstva zároveò musí obsahovat
o jednu jednotku více, ne je poèet znaèek v $ L $. Aktivace prvních $ |L| $ jednotek je interpretována jako pravdìpodobnost pozorování odpovídajících znaèek a aktivace $ |L| + 1 $ jednotky je interpretována jako pravdìpodobnost pozorování prázdné znaèky. Prázdná znaèka umoòuje zobrazit více prvkù vstupní sekvence na jednu znaèku èi ignorovat prvky sekvence, které odpovídají tichu, a z vısledné sekvence je odstranìn. Celkovou pravdìpodobnost libovolného znaèkování pak mùeme vyèíslit seètením pravdìpodobností jeho rùznıch zarovnání.

Mìjme tedy RNN s $ m $ vstupními jednotkami, $ n $ vıstupními jednotkami a váhovım vektorem $ \boldsymbol{w} $, která slouí jako zobrazení $ \mathcal{N}_{\boldsymbol{w}}: (\mathbb{R}^{m})^{T} \mapsto (\mathbb{R}^{n})^{T} $. Dále mìjme vıstupní sekvenci sítì $ \boldsymbol{y} = \mathcal{N}_{\boldsymbol{w}}(\boldsymbol{x}) $, její prvky $ \boldsymbol{y}_{k}^{(t)} $ odpovídají aktivaci vıstupní jednotky $ k $ v èase $ t $ a lze je interpretovat jako pravdìpodobnost pozorování znaèky $ k $ v èase $ t $. Hustota pravdìpodobnosti znaèkování nad mnoinou $ L^{'T} $ sekvencí délky $ T $ nad abecedou $ L^{'} = L \cup \{ \textit{prázdná znaèka} \} $,  je pak
\begin{equation}
p(\boldsymbol{\pi} \mid \boldsymbol{x}) = \prod _{t=1}^{T} \boldsymbol{y}_{\boldsymbol{\pi}_{t}}^{(t)}, \forall \boldsymbol{\pi} \in L^{'T},
\end{equation}
kde $ \boldsymbol{\pi} $ budeme nazıvat cestami a odpovídají prvkùm $ L^{'T} $.

Dále definujme zobrazení $ \mathcal{B}: L^{'T} \mapsto L^{\leq T} $, kde $ L^{\leq T} $ je mnoina všech monıch znaèkování, tj. mnoina vıstupních sekvencí o délce menší èí rovné $ T $ nad pùvodní abecedou znaèek $ L $. Toto zobrazení získáme odstranìním opakujících se znaèek a prázdnıch znaèek ze získanıch cest (napø. $ \mathcal{B}(aa-ab-) = \mathcal{B}(a-a-bb) = aab $) a vyuijeme ho k získání podmínìné pravdìpodobnosti daného znaèkování $ \boldsymbol{l} \in L^{\leq T} $ \cite{distill, ctc}
\begin{equation}
p(\boldsymbol{l} \mid \boldsymbol{x}) = \sum_{\boldsymbol{\pi} \in \mathcal{B}^{-1}(\boldsymbol{l})} p(\boldsymbol{\pi} \mid \boldsymbol{x}),
\end{equation}

\subsection{Dekódování}
Vıstupem klasifikátoru by mìlo bıt nejvíce pravdìpodobné znaèkování pro vstupní sekvenci
\begin{equation}
h(\boldsymbol{x}) = \underset{\boldsymbol{l} \in L^{\leq T}}{\mathrm{argmax}}\text{ } p(\boldsymbol{l} \mid \boldsymbol{x} ).
\end{equation}
Proces hledání znaèkování nazıváme dekódováním a pro metodu CTC lze vyuít dvì aproximativní metody.

První metoda, dekódování nejlepší cesty, je zaloena na pøedpokladu, e cesta s nejvyšší pravdìpodobností odpovídá nejvíce pravdìpodobnému znaèkování
\begin{align}
h(\boldsymbol{x}) &\approx \mathcal{B}(\boldsymbol{\pi}^{*}) , \\
\boldsymbol{\pi}^{*} &= \underset{\boldsymbol{\pi} \in N^{t}}{\mathrm{argmax}}\ \text{ } p(\boldsymbol{\pi} \mid \boldsymbol{x}).
\end{align}
Dekódování nejlepší cesty lze snadno najít zøetìzením vıstupu s nejvyšší aktivaèní hodnotou v kadém èasovém kroku. Není ovšem zaruèeno, e tato metoda nalezne nejlepší moné znaèkování.

Druhá metoda vyuívá sluèování cest, které procházejí stejnou sekvencí znaèek, a zaruèuje nalezení nejlepšího znaèkování. Je však vıpoèetnì nároèná a aby tato metoda byla upoèitatelná, je tøeba vyuít další heuristiky (napø. BEAM proøezávání, které umoní kompromis mezi rychlostí vıpoètu a pøesností).

Obì tyto metody mohou bıt zároveò obohaceny o jazykovı model
\begin{equation}
h(\boldsymbol{x}) = \underset{\boldsymbol{l} \in L^{\leq T}}{\mathrm{argmax}}\text{ } p(\boldsymbol{l} \mid \boldsymbol{x} ) \cdot p(\boldsymbol{l})^{\gamma} \cdot g(\boldsymbol{l})^{\zeta},
\end{equation}
kde $ p(\boldsymbol{l}) $ je pravdìpodobnost znaèkování daná jazykovım modelem, $ g(\boldsymbol{l}) $ je penalizace za vloení slova a parametry $ \gamma $ a $ \zeta $ jsou pøedem dané váhy \cite{distill, ctc}.

\subsection{Dopøednı a zpìtnı algoritmus}
Pro získání vısledného znaèkování je tøeba spoèítat podmínìnou pravdìpodobnost $ p(\boldsymbol{l} \mid \boldsymbol{x}) $ jednotlivıch znaèkování. To mùe bıt znaènì problematické, jeliko je potøeba spoèítat sumu pøes všechny cesty odpovídající danému znaèkování. Vyuijeme k tomu algoritmus dynamického programování, kterı vychází z pøedpokladu, e suma  všech cest odpovídajících znaèkování mùe bıt rozloena na iterativní sumu pøes cesty, které v daném èasovém kroku dosáhly stejné znaèky. Tyto iterace pak mohou bıt snadno spoèteny pomocí  rekurzivních dopøednıch a zpìtnıch promìnnıch.

Mìjme sekvenci $ \boldsymbol{q} $ délky $ r $, kde $ \boldsymbol{q}_{1:p} $ a $ \boldsymbol{q}_{r-p:r} $ znaèí prvních a posledních $ p $ znaèek. Potom pro znaèkování $ \boldsymbol{l} $ zavedeme dopøednou promìnnou $ \alpha_{t}(s) $, která odpovídá celkové pravdìpodobnosti $ \boldsymbol{l}_{1:s} $ v èase $ t $
\begin{equation}
\alpha_{t}(s) = \sum_{\substack{\boldsymbol{\pi} \in N^{T}: \\ \mathcal{B}(\boldsymbol{\pi}_{1:t}) = \boldsymbol{l}_{1:s}}} \prod_{t^{'}=1}^{t} \boldsymbol{y}_{\boldsymbol{\pi}_{t^{'}}}^{(t)^{'}} ,
\end{equation}
kde $ \alpha_{t}(s) $ mùe bıt rekurzivnì spoètena z $ \alpha_{t-1}(s) $ a $ \alpha_{t-1}(s-1) $. Aby bylo moné vyuít prázdné znaèky ve vıstupní cestì, je tøeba upravit znaèkování $ \boldsymbol{l} $ na $ \boldsymbol{l}^{'}  $ vloením prázdné znaèky na zaèátek a konec sekvence a mezi kadé dvì znaèky. Délka $ \boldsymbol{l}^{'}  $ je tedy $ 2 |\boldsymbol{l}| + 1  $. Pøi vıpoètu pravdìpodobnosti znaèkování $ \boldsymbol{l}^{'}  $ umoníme pøechody mezi prázdnımi a neprázdnımi znaèkami a také mezi kadım párem unikátních neprázdnıch znaèek. Zároveò musí všechny sekvence zaèínat buï prázdnou znaèkou $ b $ nebo prvním znaèkou v $ l $. Tìmito omezeními získáme inicializaèní hodnoty algoritmus
\begin{align}
\alpha_{1}(1) &= \boldsymbol{y}_{b}^{(1)} , \\
\alpha_{1}(2) &= \boldsymbol{y}_{\boldsymbol{l}_{1}}^{(1)} , \\
\alpha_{1}(s) &= 0, \forall s > 2
\end{align}
a rekurzi
\begin{equation}
\alpha_{t}(s) = 
\begin{cases}
\bar{\alpha}_{t}(s) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{pokud } \boldsymbol{l}_{s}^{'} = b \text{ nebo } \boldsymbol{l}_{s-2}^{'} = \boldsymbol{l}_{s}^{'} \\
(\bar{\alpha}_{t}(s) + \alpha_{t-1}(s-2)) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{jinak}
\end{cases} ,
\end{equation}
kde
\begin{equation}
\bar{\alpha}_{t}(s) = \alpha_{t-1}(s) + \alpha_{t-1}(s-1) .
\end{equation}
Pravdìpodobnost $ \boldsymbol{l} $ je pak dána sumou vıslednıch pravdìpodobností $ \boldsymbol{l}^{'} $ s prázdnou znaèkou a bez prázdné znaèky v èase $ T $
\begin{equation}
p(\boldsymbol{l} \mid \boldsymbol{x}) = \alpha_{T}(|\boldsymbol{l}^{'}|) + \alpha_{T}(|\boldsymbol{l}^{'}| - 1) . 
\end{equation}

Obdobnì zadefinujeme zpìtnou promìnnou $ \beta_{t}(s) $ jako celkovou pravdìpodobnost $ \boldsymbol{l}_{s:|\boldsymbol{l}|} $ v èase $ t $
\begin{align}
&\beta_{t}(s) = \sum_{\substack{ \boldsymbol{\pi} \in N^{T}: \\ \mathcal{B}(\boldsymbol{\pi}_{t:T})=\boldsymbol{l}_{s:|\boldsymbol{l}|}}} \prod_{t^{'}=t}^{T} \boldsymbol{y}_{\boldsymbol{\pi}_{t^{'}}}^{(t)^{'}} , \\
&\beta_{T}(|\boldsymbol{l}^{'}|) = \boldsymbol{y}_{b}^{(T)} , \\
&\beta_{T}(|\boldsymbol{l}^{'}| - 1) = \boldsymbol{y}_{\boldsymbol{l}_{\boldsymbol{l}}}^{(T)} , \\
&\beta_{T}(s) = 0, \forall s < |\boldsymbol{l}^{'}| - 1 , \\
&\beta_{t}(s) =
\begin{cases}
\bar{\beta}_{t}(s) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{pokud } \boldsymbol{l}_{s}^{'} = b \text{ nebo } \boldsymbol{l}_{s+2}^{'} = \boldsymbol{l}_{s}^{'} \\
(\bar{\beta}_{t}(s) + \beta_{t+1}(s+2)) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{jinak}
\end{cases} , \\
&\bar{\beta}_{t}(s) = \beta_{t+1}(s) + \beta_{t+1}(s+1) .
\end{align}

Aby byla pøi vıpoètech zajištìna numerická stabilita, je tøeba dopøedné a zpìtné promìnné pøeškálovat
\begin{align}
\hat{\alpha}_{t}(s) &= \dfrac{\alpha_{t}(s)}{\sum_{s} \alpha_{t}(s)} , \\
\hat{\beta}_{t}(s) &= \dfrac{\beta_{t}(s)}{\sum_{s} \beta_{t}(s)} .
\end{align}
Dosazením pøeškálovanıch promìnnıch do vzorcù a jejich úpravou (podrobnìji v \cite{ctc}) pak mùeme sí natrénovat pomocí metod zaloenıch na maximální vìrohodnosti. Zpìtné šíøení gradientu skrz vıstupní vrstvu s aktivaèní softmax funkcí je definováno následovnì
\begin{equation}
\dfrac{\partial O^{ML} (\{ \boldsymbol{x}, \boldsymbol{z} \},\mathcal{N}_{\boldsymbol{w}})}{\partial \boldsymbol{u}_{k}^{(t)}} = \boldsymbol{y}_{k}^{(t)} - \dfrac{1}{\boldsymbol{y}_{k}^{(t)} Z_{(t)}} \sum_{s \in lab(\boldsymbol{z}, k)} \hat{\alpha}_{t}(s) \hat{\beta}_{t}(s) ,
\end{equation}
kde $ \boldsymbol{u}_{k}^{(t)} $ jsou nenormalizované pravdìpodobnosti na vıstupu sítì, $ lab(\boldsymbol{z}, k) = lab(\boldsymbol{l}, k) = \{ s : \boldsymbol{l}_{s}^{'} = k \} $ je mnoina pozic, na kterıch se ve znaèkování $ \boldsymbol{l} $ vyskytuje znaèka $ k $, a kde \cite{distill, ctc}
\begin{equation}
Z_{t} = \sum_{s=1}^{|\boldsymbol{l}^{'}|} \dfrac{\hat{\alpha}_{t}(s) \hat{\beta}_{t}(s)}{\boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{t} } .
\end{equation}

% PRAKTICKÁ ÈÁST
\newpage
\section{Klasifikace fonémù}

% VYHODNOCENÍ
\newpage
\section{Vyhodnocení}

% ZÁVÌR
\newpage
\section{Závìr}















\clearpage
\clearpage
















































\section{Úvod}
V posledních letech došlo k vıznamnému vıvoji v oblasti mobilních zaøízení a s tím i k nárùstu popularity hlasového ovládání. Aplikace spoleènosti Google nabízejí monost vyhledávání hlasem, Apple a Microsoft vytvoøili inteligentní osobní asistentky Siri, resp. Cortana a stále èastìji se hlasové ovládání zaèíná objevovat i v automobilovém prùmyslu.

Jedním z øešenıch problémù hlasového ovládání je detekce klíèovıch frází  v proudu øeèi (KWS - Keyword spotting). Na pozadí zaøízení nepøetritì bìí KWS systém, kterı naslouchá mluvenému slovu a pøi zaznìní klíèové fráze provede urèitou akci. V dnešní dobì se nejèastìji pouívají dva typy KWS systémù - systémy s pøeddefinovanımi klíèovımi frázemi a systémy vyuívající obsáhlı slovník øeèi v textové podobì (LVCSR - Large vocabulary continuous speech recognition). LVCSR systémy jsou vıpoèetnì velmi nároèné, jeliko je potøeba nejprve pøevést mluvenou øeè do textové podoby a následnì vyhledat klíèovou frázi v databázi. Vzhledem k tomu, e tyto systémy neumoòují vytvoøení vlastní bezpeèné klíèové fráze, propojují se vìtšinou se systémem pro identifikaci øeèníka\cite{lvcsr}.

Bylo by tedy vhodné vytvoøit KWS systém, kterı by nebyl závislı na pøedem definovanıch frázích a umonil by uivateli volbu vlastních klíèovıch frází v libovolném jazyce. Také by mìl bıt vıpoèetnì nenároènı, aby byla zajištìna odezva v reálném èase na mobilních zaøízeních. Tyto poadavky splòuje metoda Query-by-Example, kdy si uivatel vytvoøí vzorovou klíèovou frázi a všechny budoucí promluvy jsou porovnávány vùèi ní\cite{chen,hazen}.

Práce se vìnuje zpracování akustického signálu a pøedevším problematice rozpoznání izolovanıch slov. Uvedeme si nejèastìji vyuívané algoritmy strojového uèení a porovnáme jejich rozpoznávací schopnosti a jejich vhodnost potenciálního vyuití v KWS systému.

\newpage
\section{Klasifikace}
Klasifikace neboli rozpoznávání je úloha, kde je cílem správnì zaøadit objekt do jedné z~$ k $ tøíd. Aby bylo moné objekt klasifikovat, je tøeba ho nejprve vhodnì popsat pomocí tzv. pøíznakového vektoru (obrazu). Pøíznakovı vektor se skládá z pøíznakù, které reprezentují jednotlivé mìøitelné èi pozorovatelné vlastnosti objektu. Pøíznaky je tøeba volit v závislosti na øešeném problému tak, aby dostateènì popisovaly pozorovanı objekt. Mùe se zdát, e vytvoøením pøíznakového vektoru ze všech mìøitelnıch velièin objektu získáme dokonalı popis objektu, kterı pøispìje k pøesnosti klasifikace. Opak je však pravdou, jeliko velké mnoství pøíznakù s nedostateènou informativní hodnotou mùe mít negativní vliv na pøesnost klasifikace a vést k pøetrénování modelu\cite{psutka1}.

Algoritmus, kterı provádí klasifikaci, se nazıvá klasifikátor. Tato práce se zabıvá pøedevším klasifikátory zaloenımi na uèení s uèitelem a klasifikátoru podle minimální vzdálenosti k vzorovım obrazùm.

Uèící se klasifikátor pracuje ve dvou fázích - trénovací fáze a klasifikaèní fáze:
\begin{enumerate}
	\item \textbf{trénovací fáze} - v trénovací fází jsou klasifikátoru pøedkládány dvojice $ \lbrace x_{i}, y_{i}\rbrace $, $ i = 1, 2, \dots, l $, kde $ x_{i} $ jsou pøíznakové vektory a $ y_{i} $ jsou cílové tøídy jednotlivıch obrazù z trénovací mnoiny. Klasifikátor vypoète svùj vıstup pro pøíznakovı vektor $ x_{i} $ (odhadovaná tøída) a porovná jej s cílovou tøídou. V pøípadì neshody odhadované a cílové tøídy upraví své parametry tak, aby minimalizoval danou chybovou funkci (jedná se tedy o optimalizaèní úlohu). Trénovací dvojice jsou klasifikátoru pøedkládány tak dlouho, dokud nedosáhne optimálního nastavení.
	\item \textbf{klasifikaèní fáze} - klasifikátoru pøedkládáme neznámé obrazy a na základì \newline natrénovanıch parametrù klasifikujeme do jedné z $ k $ tøíd\cite{alpaydin,duda}.
\end{enumerate}

Pøi trénování dochází velmi èasto k tzv. pøetrénování modelu (overfitting). Klasifikátor se nauèí dokonale rozpoznávat pozorování z trénovací mnoiny, ale ztrácí schopnost generalizace a nová, neznámá pozorování klasifikuje chybnì. Overfitting je zpùsoben volbou pøíliš komplexního modelu, nedostatkem trénovacích dat nebo vysokou dimenzí obrazù. Komplexitu modelu je tedy tøeba volit s ohledem na povahu klasifikovanıch dat, popø. lze vyuít jednu z metod, která overfitting omezí (regularizace, cross-validation, dropout vrstvy u neuronovıch sítí, atd.)\cite{abu-mostafa}. Opaènım pøípadem overfittingu je tzv. underfitting, kdy model na úkor generalizace ztrácí klasifikaèní schopnosti. Vliv volby modelu s rùznım poètem stupòù volnosti mùeme vidìt na obrázku \ref{fig:overfitting}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{overfitting.eps}
    \caption{Pøíklad overfittingu a underfittingu.}
    \label{fig:overfitting}
\end{figure}

\newpage
\section{Zpracování akustického signálu}
Základním krokem pro klasifikaci øeèi je samotné zpracování akustickıch signálù a jejich transformace na pøíznakové vektory. Pøi zpracování akustického signálu pøedpokládáme, e se jeho vlastnosti mìní pomalu a na krátkıch úsecích je témìø stacionární (v èasové i~frekvenèní oblasti). Signál je tedy rozdìlen na kratší úseky neboli mikrosegmenty, které jsou zpracovány samostatnì, jako by se jednalo o rùzné signály. Pro kadı mikrosegment pak vypoèteme èíslo (pøíznak), popøípadì vektor èísel na základì zvolené metriky. Délka mikrosegmentù se nejèastìji volí mezi 10 a 25ms a jednotlivé mikrosegmenty na sebe mohou navazovat nebo se i pøekrıvat. Díky tomu lze celı signál popsat posloupností èísel (pøíznakovı vektor), její délka je pøímo úmìrná délce slova a poètu mikrosegmentù\cite{psutka1}.

\subsection{Okénková funkce}
Jednotlivé mikrosegmenty jsou ze signálu vybírány pomocí okénka o urèité délce, které se posouvá o danı poèet vzorkù signálu. Okénko také slouí k pøidìlení vah zpracovávanım vzorkùm signálu. Mezi nejèastìji pouívané okénkové funkce pøi zpracování øeèi patøí pravoúhlé a Hammingovo okénko:
\begin{itemize}
	\item \textbf{pravoúhlé okénko} - pravoúhlé okénko pøidìlí všem vzorkùm mikrosegmentu stejnou váhu. Lze jej definovat vztahem
\begin{equation}
w(n) =
\begin{cases}
1 & \quad \text{pro } 0 \leq n \leq N-1 \\
0 & \quad \text{jinak,}
\end{cases}
\end{equation}
kde $ N $ je poèet vzorkù mikrosegmentu.
	\item \textbf{Hammingovo okénko} - Hammingovo okénko je vhodné pouít v pøípadì, kdy je tøeba potlaèit vzorky na krajích zpracovávaného mikrosegmentu\cite{psutka1, scipy}. Lze jej definovat vztahem
\begin{equation}
w(n) =
\begin{cases}
0.54 - 0.46\cos\left( \dfrac{2\pi n}{N-1} \right) & \quad \text{pro } 0 \leq n \leq N-1 \\
0 & \quad \text{jinak.}
\end{cases}
\end{equation}
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{window.eps}
    \caption{Aplikace Hammingova okénka na funkci $ f(x) = \sin (10\pi x) $.}
    \label{fig:window}
\end{figure}

\subsection{Pøíznaky v èasové a frekvenèní oblasti}
\subsubsection{Krátkodobá energie signálu}
Funkce krátkodobé energie signálu je definována vztahem
\begin{equation}
E_{n} = \sum_{k=-\infty}^{\infty} \left[ s(k)w(n-k)\right]^2,
\end{equation}
kde $ s(k) $ je vzorek signálu v èase $ k $ a $ w(n) $ je danı typ okénka. 
Hodnoty této funkce poskytují informaci o prùmìrné hodnotì energie v kadém mikrosegmentu signálu. Hlavním nedostatkem funkce krátkodobé energie signálu je její vysoká citlivost na velké vıkyvy v~amplitudì signálu.

\subsubsection{Krátkodobá intenzita signálu}
Funkce krátkodobé intenzity signálu je definována vztahem
\begin{equation}
I_{n} = \sum_{k=-\infty}^{\infty} |s(k)|w(n-k).
\end{equation}
Na rozdíl od funkce krátkodobé energie signálu není tato funkce citlivá na velké zmìny amplitudy signálu. 

\subsubsection{Krátkodobé prùchody nulou}
Funkce krátkodobıch prùchodù nulou je definována vztahem
\begin{equation}
Z_{n} = \dfrac{1}{2} \sum_{k=-\infty}^{\infty} |sign\left[ s(k) \right] - sign\left[ s(k-1) \right]|w(n-k).
\end{equation}
Funkce krátkodobıch prùchodù nulou nese informaci o frekvenci signálu - èím více prùchodù nulou, tím vyšší je v daném úseku frekvence signálu a naopak. Frekvenci prùchodù signálu nulovou úrovní mùeme tedy vyuít jako jednoduchou charakteristiku, která popisuje spektrální vlastnosti signálu. Hodnoty této funkce se nejèastìji vyuívají k detekci øeèi v~akustickém signálu (urèení zaèátku a konce promluvy)\cite{psutka1, psutka2}.

\subsubsection{Mel-frekvenèní kepstrální koeficienty}
Nejèastìji pouívanım typem pøíznakù v rozpoznávání øeèi jsou mel-frekvenèní kepstrální koeficienty (MFCC). Jedná se o velice robustní typ pøíznakù ve frekvenèní oblasti, kterı respektuje nelineární vlastnosti vnímání zvukù lidskım uchem. Lineární frekvence $ f $[Hz] je pøevedena na frekvenci $ f_{m} $[mel] v nelineární melovské frekvenèní škále
\begin{equation}
f_{m} = 2595 \log_{10} \left( \dfrac{f}{700} \right).
\end{equation}

Pøi vıpoètu MFCC se nejèastìji volí segmentace signálu na mikrosegmenty o délce 10 a 30ms, na které se aplikuje Hammingovo okénko s posunem o 10ms. Segmentovanı signál je následovnì zpracován rychlou Fourierovou transformací (FFT), díky které získáme amplitudové spektrum $ |S(f)| $ analyzovaného signálu. Vzhledem k pouití FFT je vhodné volit poèet vzorkù okénka pøi dané frekvenci roven mocninì 2.

Dalším krokem vıpoètu je melovská filtrace, pøi ní vyuijeme banku trojúhelníkovıch pásmovıch filtrù. Jednotlivé trojúhelníkové filtry jsou rozloeny pøes celé frekvenèní pásmo on nuly a do Nyquistovy frekvence a jejich støední frekvence jsou rovnomìrnì rozloeny podél frekvenèní osy v melovské škále. Støední hodnoty filtrù $ b_{m}$ lze vyjádøit vztahem
\begin{equation}
b_{m,i} = b_{m,i-1} + \Delta_{m},
\end{equation}
kde $ i = 1, 2,\dots,M^{*} $ je poèet trojúhelníkovıch filtrù v bance, $ b_{m,0} = 0 $ mel a $ \Delta_{m} = B_{m,w} / (M^{*} + 1)$.

Dále je tøeba vypoèítat odezvy všech filtrù, èeho dosáhneme jejich vyjádøením ve frekvenèní škále s mìøítkem v herzích za vyuití pùvodních koeficientù získanıch FFT. Pøepoèteme tedy všechny støední frekvence v melovské škále $ b_{m,i}, i=1,\dots ,M^{*}+1 $ pomocí inverzního vztahu $ f = 700[exp(0.887\cdot 10^{-3}f_{m})-1] $ na støední frekvence $ b_{i}, i=1,\dots ,M^{*}+1 $. Nyní mùeme trojúhelníkové filtry vyjádøit vztahem
\begin{equation}
u(f, i) =
\begin{cases}
\dfrac{1}{b_{i}-b_{i-1}}(f-b_{i-1}) & \quad \text{pro } b_{i-1} \leq f < b_{i} \\
\dfrac{1}{b_{i}-b_{i+1}}(f-b_{i+1}) & \quad \text{pro } b_{i} \leq f < b_{i+1} \\
0 & \quad \text{jinak}
\end{cases}
\end{equation}
a odezvy jednotlivıch filtrù $ y_{m}(i) $ vztahem
\begin{equation}
y_{m}(i) = \sum_{f=b_{i-1}}^{b_{i+1}} |S(f)|u(f,i),
\end{equation}
kde $ i = 1,2,\dots ,M^{*} $ a $ f $ jsou frekvence vyuité pøi vıpoètu FFT. Pøi prùchodu signálu filtrem je kadı koeficient FFT násoben odpovídajícím ziskem filtru a vısledky pro jednotlivé filtry jsou akumulovány a následnì zlogaritmovány. Tím dojde k dekorelaci energií filtrù a získané hodnoty lze pouít jako plnohodnotné pøíznaky.

Nakonec provedeme zpìtnou diskrétní kosinovou transformaci (DCT). Znaènou vıhodou DCT je vysoká nekorelovanost vzniklıch koeficientù. Koeficienty jsou dány vztahem
\begin{equation}
c_{m}(j) = \sum_{i=1}^{M^{*}}\log y_{m}(i) \cos \left( \dfrac{\pi j}{M^{*}} (i-0.5) \right) \quad \text{pro } j=0,1,\dots,M,
\end{equation}
kde $ M $ je poèet mel-frekvenèních kepstrálních koeficientù\cite{psutka2,mfcc}.

\subsection{Delta a delta-delta koeficienty}
Pøi vıpoètu pøíznakovıch vektorù pøedpokládáme, e signál je na krátkém úseku stacionární. Je tedy zøejmé, e tyto pøíznakové vektory budou popisovat pouze statické vlastnosti signálu a dynamické vlastnosti se ztrácí. Tento nedostatek mùeme vyøešit rozšíøením pøíznakovıch vektorù o dynamické koeficienty.

Vyuívají se delta (diferenèní) koeficienty a delta-delta (akceleraèní) koeficienty, které odpovídají první, respektive druhé derivaci pøíznakového vektoru. Delta koeficienty lze definovat vztahem
\begin{equation}
d_{t} = \dfrac{\sum_{n=1}^{N} n(c_{t+n} - c_{t-n})}{2\sum_{n=1}^{N}n^{2}},
\end{equation}
kde $ d_{t} $ je delta koeficient v èase $ t $ a $ N $ je volitelnı parametr. Tento parametr se vìtšinou volí $ N=1 $ nebo $ N=2 $ a urèuje, z kolika sousedních mikrosegmentù budou vypoèítány delta koeficienty (jeden, resp. dva leví a praví sousedé mikrosegmentu). Delta-delta koeficienty lze spoèítat vyuitím stejného vztahu s tím rozdílem, e se nepoèítají ze statickıch koeficientù, ale z delta koeficientù\cite{psutka2,mfcc}.

\subsection{Normalizace pøíznakù}
Jednotlivé pøíznaky se mohou pohybovat na rùznıch definièních oborech hodnot a~pøi klasifikaèních úlohách se mohou projevit s rùznou váhou. Proto je vhodné pøíznaky transformovat na stejnı definièní obor hodnot, popø. do rozdìlení o stejnıch parametrech, a tím zaruèit, e všechny pøíznaky budou mít stejnı vliv. Tuto transformaci nazıváme normalizací dat a v praxi se nejèastìji pouívají následující dvì metody:
\begin{itemize}
 \item \textbf{Z-score normalizace} - tato metoda se pouívá v pøípadì, kdy data odpovídají normálnímu rozloení. Z-score normalizací pøetransformujeme vstupní data $ x$, na data $ z $, které odpovídají normálnímu rozdìlení o støední hodnotì $ \mu = 0 $ s rozptylem $ \sigma = 1 $.
\begin{equation}
z = \dfrac{x-\mu}{\sigma}
\end{equation}
 \item \textbf{Min-Max normalizace} - Min-Max normalizace pøeškáluje data do definovaného intervalu (nejèastìji $ [0, 1] $ nebo $ [-1, 1] $, popø. $ [0, 255] $ pøi zpracování obrazu). Tím dosáhneme menšího rozptylu a eliminujeme vliv odlehlıch bodù (tzv. outliers).
\begin{equation}
x_{norm} = \dfrac{x - x_{min}}{x_{max} - x_{min}}
\end{equation}
\end{itemize}

Nìkteré klasifikaèní algoritmy pøímo vyadují, aby data byla normalizována. Pøíkladem mohou bıt algoritmy zaloené na gradientních metodách (logistická regrese, SVM, neuronové sítì, atd.), kdy jsou zmìny parametrù modelu pøi trénování pøímo závislé na vstupním pøíznakovém vektoru. Pøi velkıch rozdílech definièních oborù jednotlivıch pøíznakù se tedy nìkteré parametry mohou mìnit vıraznì rychleji ne ostatní. Normalizací dat zajistíme rychlejší a stabilnìjší konvergenci parametrù. Dalším pøíkladem mohou bıt shlukovací algoritmy pracující s Euklidovou vzdáleností, kde je vhodné, aby všechny pøíznaky mìly na shlukování stejnı vliv\cite{raschka}.


\newpage
\section{Support Vector Machine}
Pøedpokládejme, e máme trénovací mnoinu $ \lbrace x_{i}, y_{i}\rbrace $, $ i = 1, 2, \dots, l $, $ y_{i} \in \lbrace -1, 1 \rbrace $, která je sloena z pøíznakovıch vektorù $ x_{i} $ a odpovídajících cílovıch tøíd
$ y_{i} $ (binární klasifikace). Pro tuto mnoinu si odvodíme lineární klasifikátor zaloenı na podpùrnıch vektorech pro separabilní a neseparabilní pøípad a nelineární klasifikátor. Nakonec si uvedeme metody, jak lze klasifikovat do více tøíd\cite{burges,svec}.

\subsection{Nadrovina}
Pro odvození klasifikátoru SVM (a pozdìji i pro odvození neuronové sítì) je vhodné nejprve zavést pojem nadrovina. Pøedpokládejme tedy, e máme $ p $-dimenzionální prostor. Nadrovinou pak rozumíme libovolnı afinní podprostor o dimenzi $ p-1 $. Napøíklad ve dvoudimenzionálním prostoru je nadrovinou jednodimenzionální podprostor - pøímka, ve tøídimenzionálním prostoru je nadrovinou plocha. Nadrovinu o dimenzi $ p $ lze definovat následovnì
\begin{equation} \label{eq:hyperplane}
b + w_{1}x_{1} + w_{2}x_{2} + \dots w_{p}x_{p} = 0,
\end{equation}
kde $ x = (x_{1}, x_{2}, \dots, x_{p})^{T} $ je bod v $ p $-dimenzionálním prostoru vyhovující rovnici. Bod $ x $ tedy leí na nadrovinì.

V pøípadì, e bod $ x $ nevyhovuje rovnici \ref{eq:hyperplane}, tedy
\begin{equation}
b + w_{1}x_{1} + w_{2}x_{2} + \dots w_{p}x_{p} > 0,
\end{equation}
bod leí na jedné stranì poloroviny, resp. pokud
\begin{equation}
b + w_{1}x_{1} + w_{2}x_{2} + \dots w_{p}x_{p} < 0,
\end{equation}
bod leí na druhé stranì poloroviny. Nadrovina tedy dìlí $ p $-dimenzionální prostor na dvì poloviny, tzv. poloprostory. Náleitost bodu do tìchto poloprostorù pak mùeme zjistit jednoduchım vıpoètem znaménka levé strany rovnice \ref{eq:hyperplane}\cite{james}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{hyperplane.eps}
    \caption{Nadrovina oddìlující body ve dvoudimenzionálním prostoru\cite{sklearn}.}
    \label{fig:hyperplane}
\end{figure}

\subsection{Lineárnì separabilní pøípad}
Pøedpokládejme, e existuje nadrovina, která oddìlí body trénovací mnoiny jednotlivıch tøíd od sebe (oddìlující nadrovina). Body $ x $, které leí na této rovinì splòují rovnici $ w\cdot x + b = 0 $, kde $ w $ je normála nadroviny. Dále si definujme kolmou vzdálenost oddìlující nadroviny k poèátku jako $ \frac{|b|}{||w||} $, kde $ ||w|| $ je Euklidovská norma $ w $.

Dále si zaveïme nejkratší vzdálenost $ d_{+} $ ($ d_{-} $) mezi oddìlující rovinou a pozitivním (negativním) pøíkladem a tzv. margin (odstup) jako $ d_{+} + d_{-} $. Pro lineárnì separabilní pøípad hledá klasifikátor takovou nadrovinu, pro kterou nabıvá margin nejvyšší hodnoty. Všechny body trénovací mnoiny tedy splòují tyto podmínky
\begin{align}
x_{i} \cdot w + b &\geq +1 \quad \text{pro } y_{i}=+1, \label{eq:h1}\\
x_{i} \cdot w + b &\leq -1 \quad \text{pro } y_{i}=-1, \label{eq:h2}
\end{align}
které lze slouèit do jedné mnoiny nerovností
\begin{equation}
y_{i}(x_{i} \cdot w + b) - 1 \geq 0 \quad \forall i. \label{eq:h_united}
\end{equation}

Body, pro které v nerovnici \ref{eq:h1} platí rovnost, leí na nadrovinì $ H_{1}: x_{i} \cdot w + b = +1 $ a jejich kolmou vzdálenost vùèi poèátku lze vyjádøit jako $ \frac{|1-b|}{||w||} $. Obdobnì body, pro které v nerovnici \ref{eq:h2} platí rovnost, leí na nadrovinì $ H_{2}: x_{i} \cdot w + b = -1 $ v kolmé vzdálenosti od poèátku $ \frac{|-1-b|}{||w||} $. Z toho plyne, e $ d_{+} = d_{-} = \frac{1}{||w||} $ a margin je roven $ \frac{2}{||w||} $. Nadroviny $ H_{1} $ a $ H_{2} $ mají stejnou normálu $ w $ (jsou rovnobìné) a nenachází se mezi nimi ádnı bod z trénovací mnoiny. Nalezení takovıch nadrovin, které maximalizují margin, provedeme minimalizací $ ||w||^{2} $ za respektování omezujících podmínek.

Body leící na nadrovinách $ H_{1} $ a $ H_{2} $ se nazıvají podpùrné vektory (support vectors, zvıraznìné body na obrázku \ref{fig:svm_hyperplane}) a oddìlující rovina je na nich pøímo závislá. Pokud dojde k jejich pohybu nebo odstranìní, zmìní se i vısledné øešení.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{svm_hyperplane.eps}
    \caption{Vizualizace podpùrnıch vektorù\cite{sklearn}.}
    \label{fig:svm_hyperplane}
\end{figure}

Formulujme si nyní tento problém pomocí Lagrangeovıch multiplikátorù $ \alpha_{i}$, $ i=1, 2, \dots ,l $ - jeden pro kadou nerovnost \ref{eq:h_united}, neboli jeden pro kadı prvek trénovací mnoiny $  \lbrace x_{i}, y_{i}\rbrace $. Dostáváme Lagrangeovu funkci
\begin{equation}
L_{P} \equiv \dfrac{1}{2}||w||^{2} - \sum_{i=1}^{l}\alpha_{i}y_{i}(x_{i}\cdot w + b) + \sum_{i=1}^{l}\alpha_{i}. \label{eq:lagrangian1}
\end{equation}
Nyní staèí minimalizovat $ L_{P} $ podle $ w $ a $ b $ a zároveò poadujeme, aby $ \frac{\partial L_{P}}{\partial \alpha_{i}} = 0 $, $ \alpha_{i} \geq 0 $ (oznaème si tuto mnoinu omezujících podmínek $ \mathcal{C}_{1} $). Vzhledem k tomu, e se jedná o úlohu konvexního kvadratického programování, mùeme ekvivalentnì øešit duální problém: maximalizace $ L_{P} $ za omezujících podmínek $ \frac{\partial L_{P}}{\partial w} = 0 $ a $ \frac{\partial L_{P}}{\partial b} = 0 $ a zároveò $ \alpha_{i} \geq 0 $ (oznaème tuto mnoinu omezujících podmínek jako $ \mathcal{C}_{2} $). Tato duální formulace problému má tu vlastnost, e maximum $ L_{P} $ za podmínek $ \mathcal{C}_{2} $ nastává pøi stejnıch hodnotách  $ w $, $ b $ a $ \alpha $ jako minimum $ L_{P} $ za podmínek $ \mathcal{C}_{1} $.

Omezeními gradientu $ \frac{\partial L_{P}}{\partial w} = 0 $ a $ \frac{\partial L_{P}}{\partial b} = 0 $ získáme rovnice
\begin{align}
&w = \sum_{i} \alpha_{i} y_{i} x_{i}, \label{eq:sum1}\\
&\sum_{i} \alpha_{i} y_{i} = 0. \label{eq:sum2}
\end{align}

Dosazením rovnic \ref{eq:sum1} a \ref{eq:sum2} do \ref{eq:lagrangian1} dostaneme
\begin{equation}
L_{D} = \sum_{i} \alpha_{i} - \dfrac{1}{2} \sum_{i,j} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i} \cdot x_{j}.
\end{equation}

Po vyøešení optimalizaèní úlohy (trénovací fáze) mùeme klasifikovat libovolnı vektor $ x $ na základì toho, na které stranì oddìlující nadroviny leí. Tøída vektoru $ x $ tedy bude $ \text{sgn}(w\cdot x + b) $\cite{burges}.

\subsection{Lineárnì neseparabilní pøípad}
Pokud pouijeme vıše zmínìnı algoritmus na lineárnì neseparabilní data, nenalezneme ádné pøijatelné øešení, jeliko duální Langrangeova funkce $ L_{D} $ poroste nade všechny meze. Chceme-li algoritmus rozšíøit i pro neseparabilní data, musíme uvolnit podmínky \ref{eq:h1} a \ref{eq:h2}. Toho dosáhneme zavedením volnıch (slack) promìnnıch $ \varepsilon_{i} $, $ i=1,2,\dots,l $
\begin{align}
&x_{i} \cdot w + b \geq +1 - \varepsilon_{i} \quad \text{pro } y_{i}=+1 \\
&x_{i} \cdot w + b \leq -1 + \varepsilon_{i} \quad \text{pro } y_{i}=-1 \\
&\varepsilon_{i} \geq 0 \quad \forall i.
\end{align}
Chyba klasifikace tedy nastane v pøípadì, e $ \varepsilon > 1 $. Horní mez poètu chyb klasifikace prvkù trénovací mnoiny je tedy $ \sum_{i} \varepsilon_{i} $. Vhodnım zpùsobem, jak navıšit hodnotu kriteriální funkce za kadou chybu, je minimalizovat $ \frac{||w||^{2}}{2} + C(\sum_{i}\varepsilon_{i})^{k} $ místo pùvodního kritéria $ \frac{||w||^{2}}{2} $. Parametr $ C $ se volí a odpovídá penalizaci za chybnou klasifikaci - èím vyšší hodnota $ C $, tím vìtší penalizace. Pro $ k > 0 $, $ k \in \mathbb{Z} $ se jedná o úlohu konvexního programování, pro $ k = 2 $ a $ k = 1 $ se jedná pøímo o úlohu kvadratického programování. Volbou $ k = 1 $ navíc zajistíme, e $ \varepsilon_{i} $ ani jejich multiplikátory se neobjeví v definici duálního problému
\begin{equation}
L_{D} \equiv \sum_{i} \alpha_{i} - \dfrac{1}{2}\sum_{i,j}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}\cdot x_{j} \label{eq:lagrangian2}
\end{equation}
za podmínek
\begin{align}
0 \leq \alpha_{i} \leq C, \label{eq:cond1}\\
\sum_{i}\alpha_{i}y_{i} = 0. \label{eq:cond2}
\end{align}
Øešením je pak
\begin{equation}
w = \sum_{i=1}^{N_{S}}\alpha_{i}y_{i}x_{i},
\end{equation}
kde $ N_{S} $ je poèet podpùrnıch vektorù.

Primární úloha je dána Lagrangeovou funkcí
\begin{equation}
L_{P} = \dfrac{1}{2}||w||^{2} + C\sum_{i}\varepsilon_{i} - \sum_{i}\alpha_{i}\lbrace y_{i}(x_{i}\cdot w + b) - 1 + \varepsilon_{i} \rbrace - \sum_{i}\mu_{i}\epsilon_{i}, \label{eq:lagrangian3}
\end{equation}
kde $ \mu_{i} $ jsou Lagrangeovské multiplikátory zajištující kladnost $ \varepsilon_{i} $. Øešitelnost \ref{eq:lagrangian3} je dána Karush-Kuhn-Tucker podmínkami (více v \cite{burges}). Pro vıpoèet prahu $ b $ postaèí pouze dvì z~Karush-Kuhn-Tucker podmínek
\begin{align}
\alpha_{i}\lbrace y_{i}(x_{i}\cdot w + b) - 1 + \varepsilon_{i} \rbrace = 0,\\
\mu_{i}\epsilon_{i} = 0.
\end{align}

Aèkoliv k vıpoètu prahu $ b $ staèí znát pouze jeden prvek trénovací mnoiny splòující podmínku $ 0 < \alpha_{i} < C $ a zároveò $ \varepsilon_{i} = 0 $, z numerického hlediska je rozumnìjší urèit vıslednı práh jako prùmìr prahù pøes všechny prvky trénovací mnoiny\cite{burges}.

\subsection{Nelineárnì separabilní pøípad}
Uvaujme nyní pøípad, kdy oddìlující nadrovina není lineární funkcí trénovacích dat - potøebujeme tedy zobecnit rovnice \ref{eq:lagrangian2}, \ref{eq:cond1} a \ref{eq:cond2} pro nelineární oddìlující nadrovinu. Všimnìme si, e v tìchto rovnicích se data trénovací mnoiny vyskytují vdy jako skalární souèin $ x_{i} \cdot x_{j} $. Pøedpokládejme, e existuje zobrazení $ \Phi $ z $ n $-dimenzionálního prostoru do jiného Euklidovského prostoru $ \mathcal{H} $ o vyšší dimenzi (a nekoneènìdimenzionálního)
\begin{equation}
\Phi: \mathbb{R}^{n} \mapsto \mathcal{H}.
\end{equation}
Potom by trénovací algoritmus závisel pouze na skalárních souèinech $ \Phi (x_{i}) \cdot \Phi (x_{j}) $ v~prostoru $ \mathcal{H} $. Definujme si tedy jádrovou funkci (kernel function) $ K(x_{i},x_{j}) = \Phi (x_{i}) \cdot \Phi (x_{j}) $, kterou mùeme nahradit skalární souèin $ x_{i} \cdot x_{j} $ v trénovacím algoritmu \ref{eq:lagrangian2} a opìt dopoèítat normálu $ w $ oddìlující nadroviny. Nemusíme tedy explicitnì znát zobrazení $ \Phi $, pouze jádrovou funkci.

Klasifikaci libovolného bodu $ x $ provedeme vıpoètem znaménka funkce $ f(x) $
\begin{equation}
f(x) = \sum_{i=1}^{N_{S}}\alpha_{i}y_{i}\Phi (s_{i})\cdot\Phi (x) + b = \sum_{i=1}^{N_{S}}\alpha_{i}y_{i}K(s_{i},x) + b, \label{eq:svm}
\end{equation}
kde $ s_{i} $ jsou podpùrné vektory\cite{burges,friedman}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{svm_kernels.eps}
    \caption{SVM s lineární a polynomiální jádrovou funkcí\cite{sklearn}.}
    \label{fig:svm_kernels}
\end{figure}

\subsection{Klasifikace do více tøíd}
Zatím jsme se zabıvali pouze klasifikaci do dvou tøíd neboli binární klasifikací. Nyní uvaujme trénovací mnoinu  $ \lbrace x_{i}, y_{i}\rbrace $, $ i = 1, 2, \dots, l $, $ y_{i} \in \lbrace 1,2,\dots , k \rbrace $, kde $ k > 2 $ je poèet cílovıch tøíd. Uveïme si dva základní pøístupy, jak do tìchto tøíd klasifikovat:
\begin{itemize}
	\item \textbf{One-Versus-One} - natrénujeme $ \frac{k(k-1)}{2} $ binárních klasifikátorù, kde kadı z nich porovnává dvì rùzné tøídy navzájem. Klasifikace testovacího vektoru je pak zaloena na hlasování, kdy kadı klasifikátor hlasuje pro jednu tøídu a testovací vektor je zaøazen do té tøídy, která dostala nejvíce hlasù.
	\item \textbf{One-Versus-All} - natrénujeme $ k $ binárních klasifikátorù, kde kadı z nich porovnává jednu z $ k $ tøíd oproti zbylım $ k-1 $ tøídám. Testovací vektor je zaklasifikován do té tøídy, pro kterou nabıvá oddìlující nadrovina \ref{eq:svm} nejvyšší hodnoty (tj. bod leí nejdále od oddìlující nadroviny a byl zaklasifikován s nejvìtší "jistotou")\cite{friedman,svec}.
\end{itemize}

\newpage
\section{Neuronové sítì}
Neuronová sí je algoritmus inspirovanı funkcí neuronù a jejich propojením v lidském mozku. Skládá se z mnoha vıpoèetních jednotek (neurony) propojenıch pomocí numerickıch parametrù (synapse) a úpravou tìchto parametrù je schopna se uèit. Tato práce je omezena pouze na dopøedné neuronové sítì a jejich trénování pomocí uèení s uèitelem.

\subsection{Perceptron a aktivaèní funkce}
Základní vıpoèetní jednotkou neuronovıch sítí je tzv. perceptron. Vıstupní hodnota perceptronu je dána vztahem
\begin{equation}
y = f\left( \sum_{i=1}^{n} w_{i}x_{i} + w_{0} \right) = f(w^{T}x + w_{0}),
\end{equation}
kde $ w = [w_{1}, w_{2}, \dots ,w_{n}]^{T} $ je váhovı vektor, $ x = [x_{1}, x_{2}, \dots ,x_{n}]^{T} $ je vstupní vektor, $ w_{0} $ je práh a $ f(\cdot) $ je aktivaèní funkce. Práh reprezentuje váhu vedoucí od jednotkového vstupního bodu, kterı se zavádí z dùvodu generalizace sítì.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\textwidth]{perceptron.eps}
    \caption{Perceptron.}
    \label{fig:perceptron}
\end{figure}

Pro zjednodušení následujících odvození si rozšiøme váhovı vektor $ w $ o práh 
$ w_{0} $ a~vstupní vektor o jednotkovı vstupní bod
\begin{align}
w &= [w_{0}, w_{1}, \dots ,w_{n}]^{T}, \\
x &= [1, x_{1}, \dots ,x_{n}]^{T}.
\end{align}

Uveïme si také pøíklady nejznámìjších aktivaèních funkcí:
\begin{itemize}
 \item \textbf{Sigmoidální aktivaèní funkce} - sigmoidální aktivaèní funkce transformuje reálné èíslo do intervalu $ (0,1) $. Nevıhodou tohoto rozsahu je, e velmi malá èísla jsou transformována na hodnoty blízké nule, co má za následek i velice nízkou hodnotu lokálního gradientu a neuronem tak projde minimum signálu (více u algoritmu back\-propagation). Pøi inicializaci vah vysokımi hodnotami naopak mùe dojít k~saturaci signálu a sí nebude schopná se uèit. Vıstupy neuronu s touto aktivaèní funkcí zároveò nemají støední hodnotu v nule, co má za následek, e pro kladnı vstup budou mít všechny váhy vedoucí k neuronu stejné znaménko.
\begin{equation}
f(\xi) = \dfrac{1}{1 + e^{-\xi}}
\end{equation}
 \item \textbf{Tanh} - aktivaèní funkce tanh transformuje reálnì èíslo do intervalu $ (-1,1) $. Vıstupní interval má støední hodnotu v nule a øeší nedostatky sigmoidální aktivaèní funkce.
\begin{equation}
f(\xi) = \tanh (\xi) = \dfrac{2}{1 + e^{-2\xi}} - 1
\end{equation}
 \item \textbf{ReLU (Rectified Linear Unit)} - aktivaèní funkce ReLU je lineární aktivaèní funkce s prahem v hodnotì nula. Oproti vıše zmínìnım aktivaèním funkcím vıraznì urychluje konvergenci gradientu a není tak vıpoèetnì nároèná. Pøi nevhodnì zvolené konstantì uèení však mue dojít k "deaktivaci" neuronù, kdy jejich gradient poklesne na nulu a tyto neurony ji nikdy nebudou aktivovány.
\begin{equation}
f(\xi) = \max (0, \xi)
\end{equation}
 \item \textbf{Maxout} - Maxout je generalizací aktivaèní funkce ReLU. Na rozdíl od vıše zmínìnıch aktivaèních funkcí nemá stejnı funkcionální tvar $ f(w^{T}x+b) $, ale poèítá hodnotu funkce $ \max(w_{1}^{T}x+b_{1},w_{2}^{T}x+b_{2}) $. Maxout øeší nedostatky ReLU, ovšem za cenu zdvojnásobení poètu parametrù pro kadı neuron\cite{karpathy}.
\end{itemize}

Mùeme si všimnout, e argument aktivaèní funkce definuje nadrovinu v $ n $-dimenzionálním prostoru. Volbou aktivaèní funkce
\begin{equation}
f(\xi) = \text{sign}(\xi) = 
\begin{cases}
1 & \quad \text{pro } \xi \geq 0 \\
0 & \quad \text{jinak}\\
\end{cases}
\end{equation}
získáme jednoduché rozhodovací pravidlo, které pøiøazuje body do poloroviny na základì znaménka argumentu aktivaèní funkce. Jedná se tedy o jednoduchı klasifikátor, kterı dokáe klasifikovat do dvou tøíd (jedná se o analogii lineárnì separabilního pøípadu u~klasifikátoru SVM).

Paralelním spojením více perceptronù získáme nejjednodušší typ dopøedné neuronové sítì, tzv. jednovrstvou neuronovou sí\cite{alpaydin, mitchell}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{single_layer.eps}
    \caption{Jednovrstvá neuronová sí s $ n $ vstupy, aktivaèní funkcí $ f(\cdot) $ a $ m $ vıstupy.}
    \label{fig:slp}
\end{figure}

\subsection{Trénování jednovrstvé neuronové sítì}
Jedním ze zpùsobù trénování jednovrstvé neuronové sítì je tzv. perceptronové pravidlo. Nejprve síti pøidìlíme náhodné váhy (vìtšinou se volí malá èísla v okolí nuly) a poté pøivádíme na vstup sítì jednotlivé pøíznakové vektory z trénovací mnoiny. V pøípadì, e sí zaklasifikuje bod chybnì, dojde k úpravì hodnot vah. Tento proces probíhá tak dlouho, dokud nejsou všechny body zaklasifikovány správnì.

Jednotlivé váhy $ w_{i} $ vedoucí od $ i $-tého vstupu $ x_{i} $ jsou modifikovány pomocí perceptronového pravidla
\begin{equation}
w_{i}(k+1) = w_{i}(k) + \Delta w_{i}(k) = w_{i}(k) + \alpha(t-y)x_{i},
\end{equation}
kde $ k $ je iterace uèícího algoritmu, $ t $ je oèekávanı vıstup neuronu, $ y $ je skuteènı vıstup neuronu a $ \alpha \in \mathbb{R}^{+} $ je konstanta uèení. V pøípadì, e je vhodnì zvolena konstanta uèení $ \alpha $ a data jsou lineárnì separabilní, algoritmus uèení dokonverguje k optimálnímu nastavení vah.

Druhım zpùsobem uèení je tzv. delta pravidlo, které zajišuje konvergenci i pro lineárnì neseparabilní data. Pro zavedení delta pravidla si nejprve definujme trénovací chybu
\begin{equation}
E(w) = \dfrac{1}{2}\sum_{i=0}^{n} (t_{i}-y_{i})^{2},
\end{equation}
kde $ n $ je poèet prvkù trénovací mnoiny. 

Cílem uèení je tuto chybu minimalizovat, èeho dosáhneme pomocí gradientního sestupu (gradient descent). Jedná se o iterativní algoritmus, kterı opìt zaèíná s náhodnì inicializovanımi hodnotami vah a v kadém kroku je upraví ve smìru nejvìtšího sestupu gradientu. Tento proces probíhá tak dlouho, dokud není nalezeno globální minimum chybové funkce. Modifikace vah je tedy závislá na gradientu $ E(w) $ podle $ w $
\begin{equation}
\nabla E = \left[ \dfrac{\partial E}{\partial w_{0}}, \dfrac{\partial E}{\partial w_{1}}, \dots, \dfrac{\partial E}{\partial w_{n}} \right].
\end{equation}
Gradient $ \nabla E(w) $ udává smìr nejvìtšího rùstu chybové funkce - jeho zápornou hodnotou tedy získáme smìr nejvìtšího poklesu a trénovací pravidlo v maticovém tvaru bude
\begin{equation}
w(k+1) = w(k) + \Delta w(k) = w(k) - \alpha \nabla E(w) = w(k) - \alpha(t-y)x^{T}.
\end{equation}

Vzhledem k tomu, e daná chybová funkce má pouze jedno globální minimum, gradientní sestup pøi vhodnì zvolené konstantì uèení vdy dokonverguje k takovému váhovému vektoru, kterı zajišuje minimální chybu\cite{mitchell}.

\subsection{Vícevrstvá dopøedná neuronová sí}
Jak ji bylo zmínìno, jednovrstvé neuronové sítì umoòují vyjádøit pouze lineární rozhodovací hranici. Nelineární hranici mùeme vyjádøit pomocí vícevrstvé sítì, která se skládá z jedné vstupní vrstvy, jedné nebo více skrytıch vrstev a vıstupní vrstvy. Kadá skrytá vrstva se skládá z libovolného poètu neuronù a prahové jednotky a jednotlivé skryté vrstvy mohou mít rùzné aktivaèní funkce.

Pro zjednodušení následujících odvození pøedpokládejme sí se vstupní vrstvou s $ I $ vstupními jednotkami, skrytou vrstvu s $ J $ neurony a vıstupní vrstvu s $ K $ vıstupními jednotkami. Neurony ve skryté vrstvì mají aktivaèní funkci $ f(\cdot) $ a vıstupní vrstva má aktivaèní funkci $ g(\cdot) $ (znázornìno na obrázku \ref{fig:mlp}). Opìt rozšíøíme váhovou matici $ w $ o~práh $ w_{0} $ vedoucí k jednotkovému vstupnímu bodu $ x_{0} $, kterım rozšíøíme vstupní vektor $ x $. Vıstup neuronu $ j $ ve skryté vrstvì tedy mùeme zapsat jako
\begin{align}
net_{j}  &= \sum_{i=1}^{I}x_{i}w_{ji} + x_{0}w_{j0} = \sum_{i=0}^{I}x_{i}w_{ji} = w^{T}_{j}x, \\
z_{j} &= f(net_{j}),
\end{align}
kde $ w_{ji} $ znaèí váhu mezi $ j $-tım neuronem skryté vrstvy a $ i $-tou vstupní jednotkou. Obdobnì lze vypoèítat vıstup $ k $-té vıstupní jednotky
\begin{align}
net_{k} &= \sum_{j=1}^{J}z_{j}w_{kj} + z_{0}w_{k0} = \sum_{j=0}^{J}z_{j}w_{kj} = w^{T}_{k}z, \\
y_{k}  &= g\left(net_{k}\right).
\end{align}
Úpravou lze $ k $-tı vıstup zapsat jako
\begin{equation}
y_{k} = g\left(\sum_{j=1}^{J}w_{kj}f\left( \sum_{i=1}^{I}x_{i}w_{ji} + x_{0}w_{j0} \right)  + w_{k0} \right).
\end{equation}

Obdobnım zpùsobem lze vyjádøit vıstup pro neuronovou sí s libovolnım poètem skrytıch vrstev.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{mlp.eps}
    \caption{Dvouvrstvá neuronová sí.}
    \label{fig:mlp}
\end{figure}

\subsection{Trénování vícevrstvé neuronové sítì}
Pro trénování vícevrstvıch neuronovıch sítí se vyuívá algoritmus backpropagation neboli algoritmus zpìtného šíøení chyby, kterı je stejnì jako delta pravidlo zaloen na minimalizaci chybové funkce pomocí gradientu. Definujme si tedy trénovací chybu $ E $ jako kvadrát sumy rozdílù mezi skuteènım vıstupem $ k $-té vıstupní jednotky $ y_{k} $ a oèekávanım vıstupem $ t_{k} $
\begin{equation}
E(w) = \dfrac{1}{2}\sum_{k=1}^{K}(t_{k} - y_{t})^{2} = \dfrac{1}{2}(t - y)^{2}. \label{eq:error}
\end{equation}

Algoritmus backpropagation, stejnì jako delta pravidlo, vychází z algoritmu gradientního sestupu. Jednotlivé váhy jsou inicializovány náhodnımi malımi èísly a jsou modifikovány ve smìru nejvìtšího poklesu chybové funkce
\begin{equation}
\Delta w = -\alpha\dfrac{\partial E}{\partial w},
\end{equation}
èím opìt získáme pravidlo pro modifikaci vah
\begin{equation}
w(k+1) = w(k) + \Delta w(k).
\end{equation}

Nejprve si odvoïme pravidlo pro modifikaci vah mezi skrytou a vıstupní vrstvou. Vzhledem k tomu, e chybová funkce není pøímo závislá na $ w_{jk} $, musíme pouít øetìzové pravidlo.
\begin{equation}
\dfrac{\partial E}{\partial w_{jk}} = \dfrac{\partial E}{\partial net_{k}} \dfrac{\partial net_{k}}{\partial w_{jk}} = \delta_{k} \dfrac{\partial net_{k}}{\partial w_{jk}} \quad\Rightarrow\quad \delta_{k} = -\dfrac{\partial E}{\partial net_{k}}
\end{equation}
Hodnotu $ \delta_{k} $ nazıváme citlivost neuronu a popisuje, jak se zmìní celková chyba pøi jeho aktivaci. Derivací rovnice \ref{eq:error} získáme hledané pravidlo
\begin{equation}
\Delta w_{jk} = \alpha \delta_{k}z_{j} = \alpha (t_{k}-y_{k})f'(net_{k})z_{j}.
\end{equation}
Obdobnım zpùsobem vyjádøíme pravidlo pro modifikaci vah mezi vstupní a skrytou vrstvou
\begin{align}
\dfrac{\partial E}{\partial w_{ji}} &= \dfrac{\partial E}{\partial z_{j}}\dfrac{\partial z_{j}}{\partial net_{j}}\dfrac{\partial net_{j}}{\partial w_{ji}} \quad\Rightarrow\quad \delta_{j}=f'(net_{j})\sum_{k=1}^{K}w_{kj}\delta_{k}, \\
\Delta w_{ji} &= \alpha x_{i}\delta_{j} = \alpha x_{i} f'(net_{j} \sum_{k=1}^{K}w_{kj}\delta_{k}).
\end{align}
Podrobnìjší odvození algoritmu backpropagation lze nalézt v \cite{duda,mitchell}.

\subsubsection{Momentum}
Chybová funkce vícevrstvé sítì mùe mít více lokálních minim a na rozdíl od chybové funkce jednovrstvé sítì (parabolická funkce s jedním globálním minimem) gradientní sestup nezaruèuje nalezení globálního minima, ale pouze lokálního minima. Z tohoto dùvodu se zavádí tzv. momentum (setrvaènost), které zabraòuje uváznutí uèícího algoritmu v~mìlkém lokálním minimu a urychluje konvergenci na plochıch èástech povrchu chybové funkce. Pøidáním momentového èlenu získáme následující pravidlo pro úpravu vah
\begin{equation}
\Delta w_{ji}(k) = \alpha x_{i}\delta_{j} + \mu \Delta w_{ji}(k-1),
\end{equation}
kde $ 0 < \mu < 1 $ je momentum a $ k $ je iterace uèícího algoritmu. Zmìna vah tedy závisí i na zmìnì vah v minulé iteraci\cite{mitchell,duda}.

\subsubsection{Sekvenèní a dávkové uèení}
Jak ji bylo zmínìno, pøi trénování s uèitelem jsou neuronové síti pøedkládány obrazy z trénovací mnoiny, která se pak snaí minimalizovat celkovou chybu mezi vıstupy sítì a oèekávanımi hodnotami. V kadém trénovacím cyklu algoritmu jsou síti pøedloeny všechny obrazy z trénovací mnoiny. V praxi se nejèastìji pouívají dva typy trénování neuronovıch sítí:
\begin{itemize}
	\item \textbf{sekvenèní uèení} - neuronové síti jsou postupnì pøedkládány jednotlivé obrazy z~trénovací mnoiny (vìtšinou v náhodném poøadí), pro kadı obraz je spoètena chyba klasifikace a následnì jsou upraveny parametry sítì.
	\item \textbf{dávkové uèení} - neuronové síti jsou postupnì pøedkládány jednotlivé obrazy z~trénovací mnoiny, jednotlivé chyby klasifikace jsou akumulovány a k úpravì parametrù sítì dojde a na konci trénovacího cyklu s ohledem na celkovou chybu klasifikace\cite{duda}.
\end{itemize}

\newpage
\section{Dynamic Time Warping}
Rozpoznávání øeèi je velmi obtíná úloha, jeliko ádné dvì promluvy nejsou stejné. Hlasy rùznıch osob se liší a stejnì tak se liší i jejich artikulace nebo tempo a barva øeèi. Ani promluvy jedné osoby nejsou stejné - jedna promluva mùe bıt pronesena potichu, druhá nahlas nebo šeptem, mohou bıt proneseny rùznì rychle nebo napø. pod vlivem nachlazení. Na akustickém signálu se dále projevuje pøítomnost šumu a rušení na pozadí\cite{psutka2}.

Jedním z øešení tohoto problému je vyuití klasifikátoru podle minimální vzdálenosti k vzorovım obrazùm. Pøi klasifikaci se slovo zpracovává jako celek a je zaøazeno do té tøídy, k jejímu vzorovému obrazu má nejmenší vzdálenost.
Základním problémem je urèení této vzdálenosti, jeliko obrazy mají rùzné délky v závislosti na délce signálu. Odlišnosti mezi podobnımi signály tedy nejsou ve spektrální oblasti, ale v èasové oblasti. K urèení vzdálenosti mezi dvìma signály se tedy vyuívá algoritmus Dynamic Time Warping (DTW), neboli nelineární "borcení" èasové osy, kterı je zaloen na metodì dynamického programování. "Borcením" èasové osy obrazu jedné z nahrávek dojde k maximalizaci shody mezi nahrávkami.

\subsection{Základní algoritmus}
Pøedpokládejme, e máme dvì nahrávky, které jsou reprezentovány svımi obrazy. Oznaème obraz testovaného slova
\begin{equation}
A = \left\lbrace a_{1}, a_{2}, \dots, a_{n}, \dots, a_{I} \right\rbrace
\end{equation}
a obraz referenèního slova
\begin{equation}
B = \left\lbrace b_{1}, b_{2}, \dots, b_{m}, \dots, b_{J} \right\rbrace,
\end{equation}
kde $ a_n $ je $ n $-tı pøíznak testovaného slova a $ b_m $ je $ m $-tı pøíznak referenèního slova. Algoritmus DTW pak hledá v rovinì ($ n $,$ m $) optimální cestu $ m = \Psi(n) $, která minimalizuje vzdálenost mezi obrazy $ A $ a $ B $
\begin{equation}
D(A,B) = \sum_{n=1}^{I} \hat{d} \left( a_{n}, b_{\Psi(n)} \right),
\end{equation}
kde $ \hat{d} ( a_{n}, b_{\Psi(n)} ) $ je vzdálenost mezi $ n $-tım prvkem testovaného obrazu a $ m $-tım prvkem referenèního obrazu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{dtw_threeway.eps}
    \caption{Prùbìh funkce DTW pro $ sin(x) $ se šumem a $ cos(x) $.}
    \label{fig:dtw_threeway}
\end{figure}

\subsection{Omezení pohybu funkce}
Optimální cesta by mìla zachovávat základní vlastnosti èasovıch os obou obrazù (souvislost, monotónnost, atd.). Z toho dùvodu se zavádí omezení na pohyb funkce DTW. Zavedeme obecnou èasovou promìnnou $ k $ a èasové promìnné $ m $ a $ n $ vyjádøíme jako funkce $ k $
\begin{align}
n &= i(k), \\
m &= j(k),
\end{align}
kde $ k = 1,2,\dots,K $ a $ K $ je délka obecné èasové osy.

\subsubsection{Omezení na hranièní body}
Hranièní body funkce DTW jsou dány podmínkami
\begin{align}
i(1) = 1 \quad i(K)=I, \\
j(1) = 1 \quad j(K)=J.
\end{align}

\subsubsection{Omezení na lokální souvislost}
Pøi prùchodu funkce DTW mùe dojít k nadmìrné expanzi èi kompresi èasové osy. Proto je vhodné omezit lokální monotónnost a souvislost DTW funkce
\begin{align}
0 \leq i(k) - i(k-1) \leq \bar{I}, \\
0 \leq j(k) - j(k-1) \leq \bar{J},
\end{align}
pøièem $ \bar{I} $ a $ \bar{J} $ jsou volitelné konstanty. Nejèastìji volíme hodnoty $ \bar{I}, \bar{J} = 1,2,3 $ s tím, e pøi hodnotì vìtší ne 1 mùe funkce DTW pøi porovnávání nìkteré mikrosegmenty pøeskoèit.

\subsubsection{Omezení na lokální strmost}
Pro funkci není vhodnı pøíliš velkı, ani pøíliš malı pøírùstek, a tak se zavádí omezení na lokální strmost. Pokud se zastupující bod $ \left[ i(k),j(k)\right] $ pohybuje ve smìru jedné osy $ \bar{n} $-krát po sobì pøi rostoucím $ k $, pak se v tomto smìru nesmí nadále pohybovat, dokud neudìlá $ \bar{m} $ krokù v jiném smìru.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{dtw_local.eps}
    \caption{Nejèastìji pouívaná lokální omezení (více v \cite{psutka1,sakoe}).}
    \label{fig:dtw_local}
\end{figure}

\subsubsection{Globální vymezení oblasti pohybu funkce}
Splnìním podmínek pro omezení na hranièní body a zobecnìním podmínek omezení na lokální strmost na celou rovinu $ (n,m) $ lze vymezit pøípustnou oblast prùchodu funkce DTW
\begin{align}
1 + \alpha\left[i(k)-1 \right] &\leq j(k) \leq 1 + \beta\left[ i(k)-1\right], \\
J + \beta\left[ i(k)-I\right] &\leq j(k) \leq J + \alpha\left[i(k)-I \right],
\end{align}
kde $ \alpha $ je minimální smìrnice a $ \beta $ maximální smìrnice pøímky vymezující pøípustnou oblast.

Pøedpokládejme, e pøi porovnání testovaného a referenèního obrazu, které reprezentují stejné slovo, nemùe dojít k zásadním èasovım rozdílùm mezi pøíslušnımi úseky stejnıch obrazù zapøíèinìnıch kolísáním tempa øeèi. S ohledem na tento pøedpoklad lze tedy stanovit podmínku pro druhé globální vymezení oblasti pohybu funkce DTW
\begin{equation}
|i(k) - j(k)| \leq w,
\end{equation}
kde $ w $ je celé èíslo, které urèuje šíøku okénka. Šíøka okénka musí bıt menší ne $ |J-I| $, aby do pøípustné oblasti funkce DTW bylo moné zahrnout i koncovı bod $ (I,J) $.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{dtw_global.eps}
    \caption{Globální vymezení oblasti pohybu funkce.}
    \label{fig:dtw_global}
\end{figure}

\subsection{Minimální vzdálenost}
Celkovou minimální vzdálenost mezi testovacím obrazem $ A $ a referenèním obrazem $ B $ lze vyjádøit vztahem
\begin{equation}
D(A,B) = \min_{\lbrace i(k), j(k), K \rbrace} \left[ \dfrac{\sum_{k=1}^{K}d\left[ i(k),j(k) \right] \hat{W}(k)}{N(\hat{W})} \right],\label{eq:dist1}
\end{equation}
kde $ d[i(k),j(k)] $ je lokální vzdálenost mezi $ n $-tım úsekem testovaného obrazu $ A $ a $ m $-tım úsekem referenèního obrazu $ B $, $ \hat{W}(k) $ je hodnota váhové funkce pro $ k $-tı úsek a $ N(\hat{W}) $ je normalizaèní faktor, jen je funkcí váhové funkce. Váhová funkce je závislá pouze na lokální cestì funkce DTW.

Implementace váhové funkce se volí na základì zvolenıch lokálních omezení funkce DTW. Nejèastìji se vyuívá jeden z tìchto ètyø typù váhovıch funkcí:
\begin{enumerate}[label=\alph*)]
	\item symetrická váhová funkce
\begin{equation}
\hat{W}(k) = \left[ i(k)-i(k-1)\right] + \left[ j(k)-j(k-1)\right],\label{eq:sym}
\end{equation}
	\item asymetrická váhová funkce
	\begin{enumerate}[label=b\arabic*)]
		\item
		\begin{equation}
		\hat{W}(k) = i(k) - i(k-1),\label{eq:asym1}
		\end{equation}
		\item
		\begin{equation}
		\hat{W}(k) = j(k) - j(k-1),\label{eq:asym2}
		\end{equation}
	\end{enumerate}
	\item 
\begin{equation}
\hat{W}(k) = \min \left[ i(k)-i(k-1), j(k)-j(k-1) \right],
\end{equation}
	\item 
\begin{equation}
\hat{W}(k) = \max \left[ i(k)-i(k-1), j(k)-j(k-1) \right],
\end{equation}
\end{enumerate}
pøièem $ i(0)=j(0)=0 $.

\subsection{Normalizaèní faktor}
Normalizaèní faktor $ N(\hat{W}) $ kompenzuje poèet krokù funkce DTW, kterı se pro rùznì dlouhé testovací a referenèní sekvence mùe vıraznì lišit. Lze jej definovat jako
\begin{equation}
N(\hat{W}) = \sum_{k=1}^{K}\hat{W}(k).
\end{equation}
Dosazením vztahù \ref{eq:sym}, \ref{eq:asym1}, \ref{eq:asym2} pro váhové funkce typu a) a b) získáme normalizaèní faktory
\begin{align}
N(\hat{W}_{a}) &= \sum_{k=1}^{K} \left[ i(k)-i(k-1) + j(k)-j(k-1)\right] = \\
&= i(K)-i(0)+j(K)-j(0) = I + J, \\
N(\hat{W}_{b1}) &= \sum_{k=1}^{K} \left[ i(k)-i(k-1)\right] = i(K)-i(0) = I, \\
N(\hat{W}_{b2}) &= \sum_{k=1}^{K} \left[ j(k)-j(k-1)\right] = j(K)-j(0) = J.
\end{align}

Ze vztahù je patrné, e pro váhové funkce typu a) a b) je hodnota normalizaèního faktoru nezávislá na konkrétním prùbìhu funkce DTW. Pro váhové funkce typu c) a d) je hodnota normalizaèního faktoru silnì závislá na prùbìhu funkce DTW a nelze ji urèit pomocí metod dynamického programování. Pro tyto pøípady se hodnota normalizaèního faktoru volí nezávisle na prùbìhu funkce DTW, aby bylo moné pouít rekurzivní algoritmus.
\begin{equation}
N(\hat{W}_{c}) = N(\hat{W}_{d}) = I
\end{equation}

\subsection{Rekurzivní algoritmus}
Díky nezávislosti normalizaèního faktoru na prùbìhu funkce DTW lze vztah \ref{eq:dist1} pro vıpoèet celkové minimální vzdálenosti mezi dvìma obrazy $ A $ a $ B $ zjednodušit do tvaru
\begin{equation}
D(A,B) = \dfrac{1}{N(\hat{W})}\lbrace \min_{\lbrace i(k), j(k), K \rbrace}  \sum_{k=1}^{K}d\left[ i(k),j(k) \right] \hat{W}(k) \rbrace \label{eq:dist2}
\end{equation}
Vıslednou hodnotu vztahu \ref{eq:dist2} lze urèit rekurzivnì algoritmem dynamického programování, kdy zavedeme funkci $ g $ èásteèné akumulované vzdálenosti:
\begin{enumerate}
	\item Inicializace
\begin{equation}
g\left[ i(1), j(1) \right] = d\left[ i(1), j(1) \right]\hat{W}(1)
\end{equation}
	\item Rekurze
\begin{equation}
g\left[ i(k), j(k) \right] = \min_{\lbrace i(k),j(k)\rbrace} \lbrace g\left[ i(k-1), j(k-1) \right] +  d\left[ i(k), j(k) \right]\hat{W}(k) \rbrace
\end{equation}
	\item Koneèná normalizovaná vzdálenost
\begin{equation}
D(A,B) = \dfrac{1}{N(\hat{W})} g\left[ i(K), j(K) \right] = \dfrac{1}{N(\hat{W})} g\left[ I,J \right]
\end{equation}
\end{enumerate}
Rekurzivní vztahy pro rùzné typy lokálních omezení lze odvodit dosazením za $ \hat{W}(k) $\cite{psutka1,sakoe}.

% PRAKTICKÁ ÈÁST
\newpage
\section{Klasifikace izolovanıch slov}
Pro porovnání jednotlivıch metod byla vytvoøena datová sada obsahující 240 nahrávek od šesti øeèníkù. Mezi øeèníky byli ètyøi mui (v tabulkách s vısledky znaèeni èísly 1, 2, 3 a~6) a dvì eny (znaèeny èísly 4, 5). Kadı øeèník pronesl ètyøikrát po sobì èíslovky nula a devìt. První dvì nahrávání probìhla v tichém prostøedí, druhá dvì za mírného okolního šumu, díky èemu lze lépe porovnat robustnost zkoumanıch metod. Tato datová sada byla následnì rozdìlena na referenèní a testovací sadu. Jako referenèní nahrávky byly zvoleny èíslovky nula a devìt z prvního nahrávání kadého øeèníka, ostatní nahrávky tvoøí testovací sadu.

Nahrávky z referenèní sady pak byly vyuity jako trénovací data pro klasifikátor SVM a neuronovou sí. Pro pøíznaky generované neuronovou sítí byla trénovací sada rozšíøena o nahrávky vytvoøené pøi vıvoji hlasového rozhraní pro Škoda Auto, kdy kadı øeèník tøikrát pronesl povely hlasového ovládání navigace, rádia, telefonu a poté promluvy mìsta, ulice a èíslovek nula a devìt.

Porovnávané metody byly implementovány v programovacím jazyce Python s vyuitím knihoven pro vìdecké vıpoèty NumPy a SciPy\cite{scipy}, knihovny pro strojové uèení scikit-learn\cite{sklearn} a knihovny pro neuronové sítì Lasagne\cite{lasagne}.

\subsection{Zpracování akustického signálu}
Prvním krokem pro klasifikaci izolovanıch slov je extrakce pøíznakù z akustického signálu. Pro tyto úèely byl vytvoøen samostatnı modul obsahující metody pro zpracování akustického signálu a transformaci pøíznakovıch vektorù. Modul umoòuje segmentaci signálu s vyuitím pravoúhlého, Hammingova nebo Hanningova okénka s volitelnou délkou v milisekundách a volitelnım posunem. 

Z dùvodu úspornìjšího zápisu vısledkù si zaveïme znaèení pro jednotlivé typy pøíznakù, které modul implementuje:
\begin{table}[H]
\centering
\singlespacing
\def\arraystretch{1.5}
\begin{tabular}{ l|l }
znaèení & typ pøíznakù \\ \hline
ste & krátkodobá energie \\ \hline
sti & krátkodobá intenzita \\ \hline
stzcr & krátkodobé prùchody nulou \\ \hline
ste\_sti\_stzcr & \vtop{\hbox{\strut kombinace pøíznakù krátkodobé energie, intenzity a prùchodù nulou}\hbox{\strut (tvoøí matici, kde sloupce odpovídají jednotlivım typùm pøíznakù v èase)}} \\ \hline
log\_fb\_en & logaritmované energie banky filtrù \\ \hline
mfcc & \vtop{\hbox{\strut mel-frekvenèní kepstrální koeficienty}\hbox{\strut (implementace byla pøevzata z modulu \cite{pymfcc})}} \\
\end{tabular}
\caption{\label{tab:znaceni}Znaèení typù pøíznakù.}
\end{table}
Po vygenerování pøíznakového vektoru je moné aplikovat Z-score nebo Min-Max normalizaci a dopoèítat delta a delta-delta koeficienty (implementace byla pøevzata z modulu LibROSA\cite{librosa}).

Vygenerované parametrizace pøíznakù jsou uvedeny v tabulce \ref{tab:parametrizace}. První èíslo v názvu pøíznakù znaèí délku okénka v milisekundách, druhé posun okénka v milisekundách. Vyuití Hammingova okénka je znaèeno zkratkou \textit{ham} (pokud není uvedeno, pøíznak byl vygenerován za vyuití pravoúhlého okénka), delta a delta-delta koeficienty zkratkou \textit{deltas} a normalizované pøíznaky \textit{norm}, napø.:
\begin{itemize}
\item log\_fb\_en\_25\_10\_ham\_deltas - logaritmované energie banky filtrù, segmentováno pomocí Hammingova okénka o délce 25ms s posunem 10ms, vypoèteny delta a delta-delta koeficienty,
\item stzcr\_10\_10\_norm - krátkodobé prùchody nulou, segmentováno pomocí pravoúhlého okénka o délce 10ms s posunem 10ms, normalizovány
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{ c|c }
ste\_10\_10 & ste\_10\_10\_norm \\ \hline
sti\_10\_10 & sti\_10\_10\_norm \\ \hline
stzcr\_10\_10 & stzcr\_10\_10\_norm \\ \hline
ste\_sti\_stzcr\_10\_10 & ste\_sti\_stzcr\_10\_10\_norm \\ \hline
log\_fb\_en\_25\_10\_ham & log\_fb\_en\_25\_10\_ham\_norm \\ \hline
log\_fb\_en\_25\_10\_ham\_deltas & log\_fb\_en\_25\_10\_ham\_deltas\_norm \\ \hline
mfcc\_25\_10\_ham & mfcc\_25\_10\_ham\_norm \\ \hline
mfcc\_25\_10\_ham\_deltas & mfcc\_25\_10\_ham\_deltas\_norm \\
\end{tabular}
\caption{\label{tab:parametrizace}Vygenerované parametrizace pøíznakù.}
\end{table}

Z tabulky \ref{tab:parametrizace} je patrné, e všechny pøíznaky v èasové oblasti byly vygenerovány s~vyuitím pravoúhlého okénka o délce 10ms, které se posouvá o 10ms. Nedochází tedy k~pøesahu mezi jednotlivımi mikrosegmenty. Pøíznaky ve frekvenèní oblasti byly vygenerovány s~vyuitím Hammingova okénka o délce 25ms s posunem 10ms (jednotlivé mikrosegmenty se èásteènì pøekrıvají) a 512 bodové FFT. Pro vıpoèet logaritmovanıch energií banky filtrù bylo vyuito 40 filtrù, pro vıpoèet MFCC 26 filtrù.

\subsection{Dynamic Time Warping}
Pro vıpoèet DTW vzdálenosti byl vytvoøen modul, kterı obsahuje základní DTW algoritmus bez globálního vymezení pohybu funkce a vyuívá symetrické omezení na lokální strmost (první zleva na obrázku \ref{fig:dtw_local}). Jako vzdálenostní metrika pro všechny typy pøíznakù kromì \textit{ste\_sti\_stzcr} byla zvolena Euklidova vzdálenost.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{dtw_alignment.eps}
    \caption{Krátkodobé energie pro dvì rùzné nahrávky èíslovky 7.}
    \label{fig:dtw_alignment}
    \includegraphics[width=0.7\textwidth]{dtw_test.eps}
    \caption{Prùbìh funkce DTW pro dva rùzné obrazy reprezentující èíslovku 7.}
    \label{fig:dtw_test}
\end{figure}

\subsubsection{Optimalizace parametrù vzdálenostní metriky}
Pro kombinaci pøíznakù sloenou z krátkodobé energie, intenzity a prùchodù nulou byla vyuita vlastní vzdálenostní metrika
\begin{equation}
d(a_{i}, b_{j}) = \alpha |a_{ste,i} - b_{ste,j}| + \beta |a_{sti,i} - b_{sti,j}| + \gamma |a_{stzcr,i} - b_{stzcr,j}|,
\end{equation}
kde $ a_{i} $ je $ i $-tı pøíznak testovaného obrazu $ A $, $ b_{j} $ je $ j $-tı pøíznak referenèního obrazu $ B $ a~$ \alpha $, $ \beta $, $ \gamma $ jsou volitelné parametry.

Pro nenormalizovanou verzi byly zvoleny parametry $ \alpha = \beta = \gamma = 1 $, pro normalizovanou verzi byla provedena optimalizace parametrù, aby bylo moné urèit, kterı z~pøíznakù v~èasové oblasti má nejvìtší informativní hodnotu pro problém klasifikace izolovanıch slov. Pro optimalizaci byla zvolena brute-force metoda, kdy byl procházen seznam monıch parametrù s krokem 0.1 za podmínky $ \alpha + \beta + \gamma = 1 $. Pro kadou trojici parametrù pak byla vyhodnocena pøesnost klasifikace.

Nejvyšší pøesnosti klasifikace v rámci jednoho øeèníka bylo dosaeno s parametry $ \alpha = 0.3 $, $ \beta = 0.3 $, $ \gamma = 0.4 $ - kadı typ pøíznakù se tedy projeví pøiblinì stejnou mírou. Optimalizací mezi všemi øeèníky pak byly získány parametry $ \alpha = 0 $, $ \beta = 0.6 $, $ \gamma = 0.4 $ - je tedy zøejmé, e krátkodobá energie v kombinaci s krátkodobou intenzitou a prùchody nulou negativnì ovlivòuje pøesnost klasifikace (pravdìpodobnì z dùvodu závislosti na øeèníkovi, viz. kapitola Vyhodnocení).

Na tyto dvì parametrizace se dále budeme odkazovat jako \textit{ste\_sti\_stzcr\_10\_10\break\_norm\_single}, resp. \textit{ste\_sti\_stzcr\_10\_10\_norm\_all}.

\subsection{Support Vector Machine}
Ke klasifikaci normalizovanıch pøíznakù byl vyuit také SVM klasifikátor s pøedpoèítanou jádrovou funkcí ve tvaru
\begin{equation}
K(A,B) = 1 - DTW(A,B).
\end{equation}
Jádrová funkce byla vypoètena ze všech referenèních obrazù navzájem a funkcionálnì odpovídá Gramovì matici $ G = X^{T}X $.

\subsection{Neuronová sí}
Pro kadého øeèníka byla natrénována tøívrstvá neuronová sí (znázornìna na obrázku \ref{fig:nn}). Vstupem této sítì je DTW vzdálenost testované nahrávky vùèi referenèním nahrávkám, vıstupem pak vektor obsahující pravdìpodobnosti náleitosti do danıch tøíd (zajištìno aktivaèní funkcí Softmax). Po otestovaní nìkolika rùznıch parametrizací neuronové sítì byly zvoleny dvì skryté vrstvy o 200 neuronech s aktivaèní funkcí ReLU.

Sí byla trénována 1500 trénovacích epoch s vyuitím dávkového uèení s dávkami o~velikosti 10. Váhy sítì byly modifikovány stochastickım gradientním sestupem rozšíøenım o~Nesterovo momentum $ \mu = 0.9 $. Konstanta uèení byla zvolena $ \alpha = 0.01 $. Kvùli pøedpokladùm tohoto klasifikaèního algoritmu byla neuronová sí natrénována pouze pro normalizovaná data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{nn.eps}
    \caption{Tøívrstvá neuronová sí.}
    \label{fig:nn}
\end{figure}

\subsection{Bottleneck}
Pro generování pøíznakù pomocí neuronové sítì byl pouit tzv. bottleneck. Bottleneckem je nazıvána taková struktura neuronové sítì, kdy jedna ze skrytıch vrstev (bottleneck vrstva) obsahuje vıraznì niší poèet neuronù ne její sousední vrstvy. Po natrénování neuronové sítì je bottleneck vrstva vyuita jako vıstupní vrstva a všechny následující vrstvy jsou odstranìny. Tím dochází ke kompresi vstupní informace do pøíznakového vektoru o~konstantní délce.

Bottleneck byl vytvoøen natrénováním ètyøvrstvé neuronové sítì a odstranìním vıstupní a poslední skryté vrstvy (znázornìno na obrázku \ref{fig:bn}). První a tøetí skrytá vrstva se skládá z 1024 neuronù s aktivaèní funkcí Tanh, druhá (bottleneck) vrstva a vıstupní vrstva mají aktivaèní funkci Softmax.  


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{bn.eps}
    \caption{Ètyøvrstvá neuronová sí.}
    \label{fig:bn}
\end{figure}

Neuronová sí byla natrénována pro dvì rùzné datové sady - pùvodní datovou sadu šesti øeèníkù a pùvodní datovou sadu rozšíøenou o nahrávky pro Škoda Auto. Vstupem neuronové sítì je vektor o velikosti 440 sloenı z logaritmované energie banky filtru pro jeden foném (vektor o velikosti 40) a jeho kontextu zleva a zprava (vektory o velikosti $ 5 \cdot 40 = 200 $). 

Sí byla trénována pro klasifikaci 20 fonému pro pùvodní datovou sadu s 8 a 16 neurony v bottleneck vrstvì a pro klasifikaci 40 fonému s 8, 16 a 32 neurony pro rozšíøenou sadu. Pro pøehlednìjší vyhodnocení si opìt zavedeme znaèení pro vygenerované pøíznaky:
\begin{itemize}
\item \textit{bn\_X} - neuronová sí natrénovaná pro pùvodní datovou sadu,
\item \textit{bn\_SA\_X} - neuronová sí natrénovaná pro rozšíøenou datovou sadu,
\end{itemize}
kde \textit{X} znaèí poèet neuronù bottleneck vrstvy.


% VYHODNOCENÍ
\newpage
\section{Vyhodnocení}
Pøi vyhodnocení procentuální pøesnosti klasifikace jednotlivıch metod byly uvaovány tøi rùzné varianty:
\begin{enumerate}
 \item pøesnost klasifikace v rámci jednoho øeèníka (testovací nahrávky jednoho øeèníka vùèi svım referenèním nahrávkám)
 \item pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním (testovací nahrávky všech øeèníkù vùèi referenèním nahrávkám jednoho øeèníka)
 \item pøesnost klasifikace mezi všemi øeèníky (testovací nahrávky všech øeèníkù vùèi referenèním nahrávkám všech øeèníkù) 
\end{enumerate}

\begin{algorithm}[H]
\setstretch{1.25}
\For{speaker \textbf{\emph{in}} speakers}{
	reference\_features, reference\_targets = get\_references(speaker) \\
	test\_features, test\_targets = get\_test(speaker) \\
	model = train(reference\_features, reference\_targets) \\
	prediction = predict(test\_features) \\
	accuracy = calculate\_accuracy(prediction, test\_targets)	}
\end{algorithm}
\begin{center}
Algoritmus 1: Vyhodnocení pøesnosti klasifikace v rámci jednoho øeèníka.
\end{center}

\begin{algorithm}[H]
\setstretch{1.25}
\For{speaker \textbf{\emph{in}} speakers}{
	\tcc{combine test features/targets of all speakers}
	test\_features, test\_targets += get\_test(speaker) \\
}
\For{speaker \textbf{\emph{in}} speakers}{
	reference\_features, reference\_targets = get\_references(speaker) \\
	model = train(reference\_features, reference\_targets) \\
	prediction = predict(test\_features) \\
	accuracy = calculate\_accuracy(prediction, test\_targets)	
}
\end{algorithm}
\begin{center}
Algoritmus 2: Vyhodnocení pøesnosti klasifikace v rámci jednoho øeèníka vùèi ostatním.
\end{center}

\begin{algorithm}[H]
\setstretch{1.25}
\For{speaker \textbf{\emph{in}} speakers}{
	\tcc{combine reference and test features/targets of all speakers}
	reference\_features, reference\_targets += get\_references(speaker) \\
	test\_features, test\_targets += get\_test(speaker) \\
}
model = train(reference\_features, reference\_targets) \\
prediction = predict(test\_features) \\
accuracy = calculate\_accuracy(prediction, test\_targets)
\end{algorithm}
\begin{center}
Algoritmus 3: Vyhodnocení pøesnosti klasifikace mezi všemi øeèníky.
\end{center}

\subsection{Pøesnost klasifikace v rámci jednoho øeèníka}
Z tabulky \ref{tab:dtw_single_speaker} je zøejmé, e nejvyšší pøesnosti klasifikace v rámci jednoho øeèníka je dosaeno vyuitím pøíznakù generovanıch neuronovou sítí. Pro pøíznaky generované neuronovou sítí natrénovanou nad pùvodní sadou s bottleneck vrstvou o 8 neuronech je dokonce dosaeno nejvyšší pøesnosti ze všech zkoumanıch metod.

Velice vysoké pøesnosti také dosahují pøíznaky ve frekvenèní oblasti s tím, e Z-score normalizace jejich pøesnost mírnì sniuje. Aèkoliv by pøidáním delta a delta-delta koeficientù mìlo dojít k nárùstu pøesnosti klasifikace, z neznámıch dùvodù došlo k jejímu poklesu. V pøípadì normalizovanıch pøíznakù se tento pokles pohybuje dokonce mezi 25-30\%.

Jednotlivé pøíznaky v èasové oblasti dle oèekávání nedosahují vysokıch pøesností. Pro krátkodobou energii a intenzitu ovšem velmi pomáhá normalizace dat. Dále si mùeme povšimnout, e pøesnost nenormalizované krátkodobé energie je stejná jako pøesnost nenormalizované kombinace pøíznakù krátkodobé energie, intenzity a prùchodù nulou. To je zpùsobeno tím, e nenormalizované hodnoty krátkodobé energie se pohybují v øádech desetitisícù, zatímco hodnoty krátkodobé intenzity a prùchodù nulou v øádech desítek a oproti krátkodobé energii se projeví jen minimálnì. Normalizací a kombinací tìchto tøí typù pøíznakù pak dostáváme typ pøíznaku s pomìrnì vysokou informaèní hodnotou. Optimalizací parametrù vzdálenostní metriky skuteènì došlo k navıšení pøesnosti a to o~1.1\%.

\begin{table}[H]
\centering
\resizebox{0.9\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 90.0 & 93.3 & 96.7 & 96.7 & 93.3 & 100.0 & 95.0 \\
\hline
bn\_16 & 100.0 & 90.0 & 93.3 & 96.7 & 86.7 & 90.0 & 92.8 \\
\hline
bn\_SA\_8 & 90.0 & 83.3 & 86.7 & 83.3 & 83.3 & 93.3 & 86.7 \\
\hline
bn\_SA\_16 & 96.7 & 93.3 & 93.3 & 96.7 & 86.7 & 80.0 & 91.1 \\
\hline
bn\_SA\_32 & 93.3 & 93.3 & 96.7 & 93.3 & 76.7 & 93.3 & 91.1 \\
\hline
log\_fb\_en\_25\_10\_ham & 80.0 & 83.3 & 100.0 & 93.3 & 83.3 & 93.3 & 88.9 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 86.7 & 86.7 & 93.3 & 90.0 & 76.7 & 93.3 & 87.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 70.0 & 83.3 & 96.7 & 86.7 & 80.0 & 93.3 & 85.0 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 63.3 & 60.0 & 73.3 & 76.7 & 66.7 & 76.7 & 69.4 \\
\hline
mfcc\_25\_10\_ham & 86.7 & 83.3 & 100.0 & 93.3 & 90.0 & 90.0 & 90.6 \\
\hline
mfcc\_25\_10\_ham\_norm & 90.0 & 90.0 & 96.7 & 86.7 & 86.7 & 90.0 & 90.0 \\
\hline
mfcc\_25\_10\_ham\_deltas & 86.7 & 83.3 & 100.0 & 86.7 & 80.0 & 90.0 & 87.8 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 70.0 & 60.0 & 76.7 & 70.0 & 66.7 & 76.7 & 70.0 \\
\hline
ste\_10\_10 & 60.0 & 20.0 & 33.3 & 46.7 & 43.3 & 26.7 & 38.3 \\
\hline
ste\_10\_10\_norm & 56.7 & 40.0 & 43.3 & 50.0 & 46.7 & 56.7 & 48.9 \\
\hline
ste\_sti\_stzcr\_10\_10 & 60.0 & 20.0 & 33.3 & 46.7 & 43.3 & 26.7 & 38.3 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 96.7 & 86.7 & 80.0 & 80.0 & 66.7 & 76.7 & 81.1 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm\_single & 96.7 & 90.0 & 83.3 & 83.3 & 66.7 & 73.3 & 82.2 \\
\hline
sti\_10\_10 & 63.3 & 36.7 & 40.0 & 66.7 & 70.0 & 66.7 & 57.2 \\
\hline
sti\_10\_10\_norm & 76.7 & 43.3 & 56.7 & 70.0 & 56.7 & 70.0 & 62.2 \\
\hline
stzcr\_10\_10 & 56.7 & 70.0 & 60.0 & 63.3 & 60.0 & 60.0 & 61.7 \\
\hline
stzcr\_10\_10\_norm & 60.0 & 66.7 & 53.3 & 60.0 & 43.3 & 53.3 & 56.1 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka pro DTW.}
\label{tab:dtw_single_speaker}
\end{table}

Vısledky dosaené klasifikátorem SVM (tabulka \ref{tab:svm_single_speaker}) jak pro pøíznaky ve frekvenèní oblasti, tak pro bottleneck pøíznaky, jsou niší ne vısledky dosaené metodou DTW. Pøidáním dynamickıch koeficientù opìt došlo k vıraznému poklesu pøesnosti - 48.9\% pro logaritmované energie banky filtrù a 32.3\% pro MFCC. Pøesnosti dosaené u pøíznakù v~èasové oblasti jsou srovnatelné s pøesnostmi metody DTW.

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 83.3 & 86.7 & 93.3 & 90.0 & 90.0 & 93.3 & 89.4 \\
\hline
bn\_16 & 90.0 & 83.3 & 90.0 & 93.3 & 86.7 & 86.7 & 88.3 \\
\hline
bn\_SA\_8 & 86.7 & 73.3 & 80.0 & 70.0 & 80.0 & 90.0 & 80.0 \\
\hline
bn\_SA\_16 & 83.3 & 80.0 & 76.7 & 90.0 & 76.7 & 76.7 & 80.6 \\
\hline
bn\_SA\_32 & 80.0 & 70.0 & 86.7 & 80.0 & 70.0 & 80.0 & 77.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 80.0 & 90.0 & 80.0 & 83.3 & 76.7 & 86.7 & 82.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 60.0 & 46.7 & 20.0 & 20.0 & 26.7 & 30.0 & 33.9 \\
\hline
mfcc\_25\_10\_ham\_norm & 70.0 & 56.7 & 76.7 & 76.7 & 70.0 & 73.3 & 70.6 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 60.0 & 46.7 & 30.0 & 30.0 & 26.7 & 36.7 & 38.3 \\
\hline
ste\_10\_10\_norm & 60.0 & 33.3 & 46.7 & 43.3 & 50.0 & 53.3 & 47.8 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 90.0 & 76.7 & 73.3 & 60.0 & 66.7 & 73.3 & 73.3 \\
\hline
sti\_10\_10\_norm & 73.3 & 40.0 & 43.3 & 60.0 & 53.3 & 66.7 & 56.1 \\
\hline
stzcr\_10\_10\_norm & 60.0 & 63.3 & 53.3 & 50.0 & 40.0 & 50.0 & 52.8 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka pro SVM.}
\label{tab:svm_single_speaker}
\end{table}

Pøesnost klasifikace pomocí neuronové sítì (tabulka \ref{tab:nn_single_speaker}) pro pøíznaky ve frekvenèní oblasti je srovnatelná s pøesností pøíznakù generovanıch neuronovou sítí a klasifikovanıch pomocí DTW. Pøidání dynamickıch koeficientù tuto pøesnost opìt sniuje. Vysokıch pøesností také dosahují bottleneck pøíznaky. Pøesnost pøíznakù v èasové oblasti je mírnì horší ne u SVM.

\begin{table}[H]
\centering
\resizebox{0.9\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 83.3 & 90.0 & 93.3 & 96.7 & 90.0 & 96.7 & 91.7 \\
\hline
bn\_16 & 96.7 & 90.0 & 86.7 & 83.3 & 83.3 & 83.3 & 87.2 \\
\hline
bn\_SA\_8 & 86.7 & 90.0 & 93.3 & 80.0 & 93.3 & 83.3 & 87.8 \\
\hline
bn\_SA\_16 & 93.3 & 93.3 & 96.7 & 96.7 & 83.3 & 80.0 & 90.6 \\
\hline
bn\_SA\_32 & 90.0 & 86.7 & 100.0 & 96.7 & 83.3 & 76.7 & 88.9 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 86.7 & 93.3 & 93.3 & 100.0 & 93.3 & 90.0 & 92.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 86.7 & 73.3 & 80.0 & 66.7 & 66.7 & 76.7 & 75.0 \\
\hline
mfcc\_25\_10\_ham\_norm & 93.3 & 96.7 & 93.3 & 96.7 & 86.7 & 90.0 & 92.8 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 93.3 & 73.3 & 76.7 & 86.7 & 83.3 & 86.7 & 83.3 \\
\hline
ste\_10\_10\_norm & 53.3 & 20.0 & 50.0 & 36.7 & 36.7 & 53.3 & 41.7 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 96.7 & 66.7 & 76.7 & 86.7 & 66.7 & 73.3 & 77.8 \\
\hline
sti\_10\_10\_norm & 53.3 & 50.0 & 56.7 & 53.3 & 46.7 & 56.7 & 52.8 \\
\hline
stzcr\_10\_10\_norm & 53.3 & 50.0 & 50.0 & 46.7 & 30.0 & 56.7 & 47.8 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka pro neuronovou sí.}
\label{tab:nn_single_speaker}
\end{table}

\subsection{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním}
Stejnì jako pøi klasifikaci v rámci jednoho øeèníka dosahuje nejlepší vısledkù metoda DTW (tabulka \ref{tab:dtw_all_test_per_speaker}) s vyuitím pøíznakù vygenerovanıch neuronovou sítí. Nejvyšší pøesnosti dosahuje bottleneck o 8 a 16 neuronech vytvoøenı natrénováním neuronové sítì nad pùvodní datovou sadou. 

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 86.1 & 85.0 & 88.9 & 89.4 & 83.3 & 95.6 & 88.1 \\
\hline
bn\_16 & 90.6 & 87.8 & 86.1 & 90.0 & 86.1 & 92.2 & 88.8 \\
\hline
bn\_SA\_8 & 85.6 & 71.7 & 80.0 & 67.8 & 80.0 & 81.7 & 77.8 \\
\hline
bn\_SA\_16 & 86.7 & 81.1 & 85.6 & 80.0 & 87.2 & 81.1 & 83.6 \\
\hline
bn\_SA\_32 & 77.8 & 76.7 & 78.9 & 80.0 & 83.9 & 87.2 & 80.7 \\
\hline
log\_fb\_en\_25\_10\_ham & 68.9 & 70.6 & 82.8 & 70.6 & 77.8 & 82.8 & 75.6 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 67.8 & 70.6 & 74.4 & 72.2 & 76.1 & 78.9 & 73.3 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 61.1 & 66.7 & 77.8 & 65.0 & 71.1 & 72.2 & 69.0 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 49.4 & 52.8 & 58.3 & 53.9 & 57.2 & 55.0 & 54.4 \\
\hline
mfcc\_25\_10\_ham & 70.6 & 72.8 & 81.7 & 72.8 & 75.6 & 87.2 & 76.8 \\
\hline
mfcc\_25\_10\_ham\_norm & 59.4 & 63.3 & 66.7 & 59.4 & 56.1 & 73.9 & 63.1 \\
\hline
mfcc\_25\_10\_ham\_deltas & 67.8 & 68.9 & 80.0 & 70.0 & 74.4 & 83.3 & 74.1 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 47.8 & 46.7 & 52.2 & 42.8 & 48.3 & 47.2 & 47.5 \\
\hline
ste\_10\_10 & 18.3 & 16.1 & 22.8 & 22.8 & 18.9 & 23.3 & 20.4 \\
\hline
ste\_10\_10\_norm & 28.3 & 22.8 & 25.6 & 29.4 & 22.2 & 27.2 & 25.9 \\
\hline
ste\_sti\_stzcr\_10\_10 & 18.3 & 16.1 & 22.8 & 22.8 & 18.9 & 23.3 & 20.4 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 70.0 & 60.6 & 65.6 & 60.6 & 59.4 & 66.7 & 63.8 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm\_all & 70.0 & 60.6 & 68.3 & 65.6 & 61.7 & 64.4 & 65.1 \\
\hline
sti\_10\_10 & 20.6 & 30.6 & 26.1 & 38.9 & 30.0 & 39.4 & 30.9 \\
\hline
sti\_10\_10\_norm & 34.4 & 33.3 & 40.6 & 41.1 & 36.1 & 47.2 & 38.8 \\
\hline
stzcr\_10\_10 & 52.2 & 49.4 & 48.3 & 41.7 & 54.4 & 56.1 & 50.4 \\
\hline
stzcr\_10\_10\_norm & 50.6 & 47.8 & 49.4 & 44.4 & 47.8 & 51.1 & 48.5 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním pro DTW.}
\label{tab:dtw_all_test_per_speaker}
\end{table}

Porovnáním tabulek \ref{tab:dtw_all_test_per_speaker}, \ref{tab:svm_all_test_per_speaker} a \ref{tab:nn_all_test_per_speaker} je patrné, e pøíznaky ve frekvenèní oblasti nejhùøe klasifikuje SVM a pøíznaky v èasové oblasti neuronová sí. Normalizace a aplikace dynamickıch koeficientù má na pøesnost klasifikace obdobnı vliv jako v pøípadì klasifikace v~rámci jednoho øeèníka.

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 81.1 & 80.0 & 83.9 & 82.2 & 82.8 & 93.9 & 84.0 \\
\hline
bn\_16 & 86.1 & 78.3 & 81.1 & 82.2 & 82.2 & 89.9 & 83.2 \\
\hline
bn\_SA\_8 & 80.6 & 60.0 & 75.6 & 60.0 & 70.0 & 77.8 & 70.7 \\
\hline
bn\_SA\_16 & 75.0 & 60.6 & 71.1 & 73.3 & 75.0 & 77.8 & 72.1 \\
\hline
bn\_SA\_32 & 68.9 & 58.3 & 69.4 & 69.4 & 62.8 & 70.0 & 66.5 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 62.2 & 65.0 & 64.4 & 62.2 & 65.6 & 66.7 & 64.4 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 39.4 & 37.2 & 13.3 & 18.3 & 20.0 & 27.8 & 26.0 \\
\hline
mfcc\_25\_10\_ham\_norm & 50.6 & 46.7 & 43.3 & 41.1 & 35.0 & 53.3 & 45.0 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 37.2 & 35.0 & 15.0 & 21.1 & 18.9 & 24.4 & 25.3 \\
\hline
ste\_10\_10\_norm & 28.3 & 23.9 & 28.9 & 27.2 & 21.1 & 27.2 & 26.1 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 66.7 & 56.7 & 59.4 & 48.3 & 55.6 & 60.6 & 57.9 \\
\hline
sti\_10\_10\_norm & 36.7 & 33.3 & 35.6 & 37.2 & 33.3 & 43.9 & 36.7 \\
\hline
stzcr\_10\_10\_norm & 50.6 & 47.2 & 44.4 & 42.8 & 46.1 & 48.9 & 46.7 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním pro SVM.}
\label{tab:svm_all_test_per_speaker}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 77.2 & 80.6 & 83.9 & 88.3 & 78.9 & 88.3 & 82.9 \\
\hline
bn\_16 & 85.0  & 88.9 & 78.9 & 89.4 & 82.8 & 83.9 & 84.8 \\
\hline
bn\_SA\_8 & 76.1 & 70.6 & 81.7 & 72.2 & 82.2 & 77.8 & 76.8 \\
\hline
bn\_SA\_16 & 74.4 & 80.6 & 78.9 & 75.6 & 87.8 & 76.1 & 78.9 \\
\hline
bn\_SA\_32 & 70.0  & 77.2 & 78.3 & 87.8 & 82.8 & 73.9 & 78.3 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 70.0 & 74.4 & 80.0 & 74.4 & 80.0 & 85.6 & 77.4 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 57.8 & 49.4 & 53.3 & 57.2 & 42.2 & 48.9 & 51.5 \\
\hline
mfcc\_25\_10\_ham\_norm & 68.3 & 72.8 & 68.3 & 66.7 & 68.9 & 78.3 & 70.6 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 63.9 & 58.9 & 63.9 & 54.4 & 62.8 & 67.2 & 61.9 \\
\hline
ste\_10\_10\_norm & 23.3 & 19.4 & 26.1 & 28.3 & 22.2 & 25.6 & 24.2 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 56.7 & 46.7 & 64.4 & 58.3 & 58.3 & 66.1 & 58.4 \\
\hline
sti\_10\_10\_norm & 27.2 & 33.9 & 30.0 & 33.9 & 27.8 & 35.6 & 31.4 \\
\hline
stzcr\_10\_10\_norm & 45.6 & 40.6 & 45.6 & 45.6 & 40.6 & 44.4 & 43.7 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním pro neuronovou sí.}
\label{tab:nn_all_test_per_speaker}
\end{table}

\subsection{Pøesnost klasifikace mezi všemi øeèníky}
Pøi klasifikaci mezi všemi øeèníky (tabulka \ref{tab:all_speakers}) navzájem dosahují opìt nejlepších vısledkù bottleneck pøíznaky a to zejména 8 neuronovı natrénovanı nad pùvodní datovou sadou a~16 neuronovı natrénovanı nad rozšíøenou sadou. Velmi vysokıch pøesností také dosahují pøíznaky ve frekvenèní oblasti a kombinace pøíznakù v èasové oblasti.

Jak ji bylo zmínìno v teoretické èásti, krátkodobá energie je silnì ovlivòována vıkyvy v amplitudì akustického signálu, zatímco krátkodobá intenzita tento problém nemá. Všimnìme si tedy, e krátkodobá intenzita dosahuje témìø dvakrát vìtší pøesnosti ne krátkodobá energie.

\begin{table}[H]
\centering
\resizebox{0.6\textwidth}{!}{\begin{tabular}{|l|r|r|r|}
\cline{2-4}
\multicolumn{1}{c}{} & \multicolumn{3}{|c|}{Prùmìrná pøesnost [\%]} \\
\hline
\multicolumn{1}{|c|}{Typ pøíznakù} & \multicolumn{1}{c|}{DTW} & \multicolumn{1}{c|}{SVM} & \multicolumn{1}{c|}{NN} \\
\hline
bn\_8 & 98.9 & 94.4 & 98.9 \\
\hline
bn\_16 & 96.7 & 92.2 & 95.6 \\
\hline
bn\_SA\_8 & 90.6 & 76.7 & 91.1 \\
\hline
bn\_SA\_16 & 98.9 & 73.9 & 92.8 \\
\hline
bn\_SA\_32 & 94.4 & 75.0 & 91.1 \\
\hline
log\_fb\_en\_25\_10\_ham & 90.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 92.8 & 74.4 & 91.1 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 86.1 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 73.3 & 30.0  & 10.0 \\
\hline
mfcc\_25\_10\_ham & 91.7 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_norm & 90.6 & 52.8 & 65.0 \\
\hline
mfcc\_25\_10\_ham\_deltas & 89.4 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 74.4 & 33.3 & 10.0 \\
\hline
ste\_10\_10 & 32.8 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_10\_10\_norm & 36.1 & 29.4 & 52.2 \\
\hline
ste\_sti\_stzcr\_10\_10 & 32.8 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 82.2 & 76.1 & 77.2 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm\_all & 85.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
sti\_10\_10 & 60.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
sti\_10\_10\_norm & 62.2 & 46.1 & 57.2 \\
\hline
stzcr\_10\_10 & 63.9 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
stzcr\_10\_10\_norm & 56.7 & 53.3 & 58.9 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace mezi všemi øeèníky.}
\label{tab:all_speakers}
\end{table}

Pøesnost klasifikátoru pro pøíznaky v èasové a frekvenèní oblasti SVM je vıraznì horší ne pøesnost DTW. Neuronová sí se zdá bıt velice robustní pro normalizované logaritmované energie banky filtrù, nicménì pro pøíznaky s dynamickımi koeficienty se nepodaøilo sí s danou strukturou úspìšnì natrénovat a pøesnost je pouhıch 10\%. Pøíznaky generované pomocí neuronové sítì s bottleneck vrstvou o 8 neuronech nad pùvodní datovou sadou se zdají bıt nezávislé na klasifikaèní metodì a dosahují velmi vysokıch pøesností. Z hlediska vıpoèetních nárokù je ovšem vhodnìjší volit pouze metodu DTW, jeliko klasifikátor SVM i neuronová sí vyuívají pøedpoèítané DTW vzdálenosti. 

% ZÁVÌR
\newpage
\section{Závìr}
Cílem této práce bylo porovnat rùzné typy pøíznakù pro úlohu klasifikace izolovanıch slov. V první èástí práce byly pøedstaveny metody zpracování akustického signálu vèetnì jednotlivıch typù pøíznakù a byly odvozeny klasifikaèní algoritmy. Ve druhé èásti pak byly pøedstaveny testované parametrizace pøíznakù a navrené algoritmy vyuité ke klasifikaci.

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{|l|r|r|r||r|r|r||r|r|r|}
\cline{2-10}
\multicolumn{1}{c}{} & \multicolumn{9}{|c|}{Prùmìrná pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{3}{|c||}{DTW} & \multicolumn{3}{|c||}{SVM} & \multicolumn{3}{|c|}{NN} \\
\cline{2-10}
 & \multicolumn{1}{c|}{1to1} & \multicolumn{1}{c|}{AlltoAll} & \multicolumn{1}{c||}{Rozdíl} & \multicolumn{1}{c|}{1to1} & \multicolumn{1}{c|}{AlltoAll} & \multicolumn{1}{c||}{Rozdíl} & \multicolumn{1}{c|}{1to1} & \multicolumn{1}{c|}{AlltoAll} & \multicolumn{1}{c|}{Rozdíl} \\
\hline
bn\_8 & 95.0 & 98.9 & 3.9 & 89.4 & 94.4 & 5 & 91.7 & 98.9 & 7.2 \\
\hline
bn\_16 & 92.8 & 96.7 & 3.9 & 88.3 & 92.2 & 3.9 & 87.2 & 95.6 & 8.4 \\
\hline
bn\_SA\_8 & 86.7 & 90.6 & 3.9 & 80.0 & 76.7 & -3.3 & 87.8 & 91.1 & 3.3 \\
\hline
bn\_SA\_16 & 91.1 & 98.9 & 7.8 & 80.6 & 73.9 & -6.7 & 90.6 & 92.8 & 2.2 \\
\hline
bn\_SA\_32 & 91.1 & 94.4 & 3.3 & 77.8 & 75.0 & -2.8 & 88.9 & 91.1 & 2.2 \\
\hline
log\_fb\_en\_25\_10\_ham & 88.9 & 90.6 & 1.7 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 87.8 & 92.8 & 5.0 & 82.8 & 74.4 & -8.4 & 92.8 & 91.1 & -1.7 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 85.0 & 86.1 & 1.1 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 69.4 & 73.3 & 3.9 & 33.9 & 30.0 & -3.9 & 75.0 & 10.0 & -65.0 \\
\hline
mfcc\_25\_10\_ham & 90.6 & 91.7 & 1.1 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_norm & 90.0 & 90.6 & 0.6 & 70.6 & 52.8 & -17.8 & 92.8 & 65.0 & -27.8 \\
\hline
mfcc\_25\_10\_ham\_deltas & 87.8 & 89.4 & 1.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 70.0 & 74.4 & 4.4 & 38.3 & 33.3 & -5 & 83.3 & 10.0 & -73.3 \\
\hline
ste\_10\_10 & 38.3 & 32.8 & -5.5 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_10\_10\_norm & 48.9 & 36.1 & -12.8 & 47.8 & 29.4 & -18.4 & 41.7 & 52.2 & 10.5 \\
\hline
ste\_sti\_stzcr\_10\_10 & 38.3 & 32.8 & -5.5 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 81.1 & 82.2 & 1.1 & 73.3 & 76.1 & 2.8 & 77.8 & 77.2 & -0.6 \\
\hline
sti\_10\_10 & 57.2 & 60.6 & 3.4 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
sti\_10\_10\_norm & 62.2 & 62.2 & 0.0 & 56.1 & 46.1 & -10.0 & 52.8 & 57.2 & 4.4 \\
\hline
stzcr\_10\_10 & 61.7 & 63.9 & 2.2 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
stzcr\_10\_10\_norm & 56.1 & 56.7 & 0.6 & 52.8 & 53.3 & 0.5 & 47.8 & 58.9 & 11.1 \\ 
\hline
\end{tabular}}
\caption{Porovnání prùmìrné pøesnosti klasifikace  v rámci jednoho øeèníka (\textit{1to1}) a~mezi všemi øeèníky (\textit{AlltoAll}).}
\label{tab:mean}
\end{table}

Porovnáním prùmìrnıch pøesností klasifikace (tabulka \ref{tab:mean}) se ukázalo, e nejpøesnìjším a nejrobustnìjším pøíznakem (nezávislı na øeèníkovi) je pøíznak vygenerovanı neuronovou sítí s bottleneck vrstvou. Velice dobrıch vısledkù také dosahovaly pøíznaky zaloené na logaritmované energii banky filtrù klasifikovanıch jak pomocí metody DTW, tak pomocí neuronové sítì.

Mezi nejménì pøesné typy pøíznakù pak patøily pøíznaky v èasové oblasti. Ukázalo se ovšem, e zkombinováním jednotlivıch pøíznakù v èasové oblasti lze vytvoøit pøíznak s~pomìrnì vysokou informaèní hodnotou. Aèkoliv se èekalo, e pøíznaky v èasové oblasti budou silnì závislé na øeèníkovi a pøi klasifikaci mezi všemi øeèníky dojde k poklesu pøesnosti, došlo k této situaci pouze pro krátkodobou energii u metod DTW a SVM a pro krátkodobou intenzitu u SVM. U ostatních pøíznakù došlo naopak k navıšení pøesnosti.

Pro další zlepšení dosaenıch vısledkù by bylo vhodné zamìøit se na pøíznaky generované neuronovou sítí a pokusit se optimalizovat její strukturu. Dalším krokem by pak bylo otestování rekurentních neuronovıch sítí (zejména typu LSTM), které v dnešní dobì pro podobné úlohy dosahují velice dobrıch vısledkù. Pro vyuití v praxi by pak bylo potøeba tento systém propojit se systémem pro detekci hlasové aktivity (voice activity detection).

% SEZNAM OBRÁZKÙ A TABULEK
\newpage
\listoffigures
\listoftables

% LITERATURA
\newpage
\bibliographystyle{unsrt}
\bibliography{literatura}


\end{document}