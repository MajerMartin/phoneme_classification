\documentclass[12pt]{article}
\usepackage[czech]{babel}
\usepackage[cp1250]{inputenc}
\usepackage[a4paper,left=30mm,right=20mm,top=25mm,bottom=25mm]{geometry}

% pseudocodes
\usepackage[tworuled]{algorithm2e}

% tables and cline
\usepackage{makecell}
\usepackage{multirow}
\usepackage{regexpatch}
\makeatletter
% Change the `-` delimiter to an active character
\xpatchparametertext\@@@cmidrule{-}{\cA-}{}{}
\xpatchparametertext\@cline{-}{\cA-}{}{}
\makeatother

% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{lmodern}
\usepackage{interval}

% equation numbering
\numberwithin{equation}{section}

% line-spacing 1.5
\usepackage{setspace}
\renewcommand{\baselinestretch}{1.5}

% indentation
\usepackage{indentfirst}

% citations
\usepackage{cite}
\usepackage[nottoc]{tocbibind}
\usepackage{url}

% images
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{float}
\graphicspath{{imgs/}}

% enumerate
\usepackage{enumitem}

\begin{document}

% TITULNÍ STRÁNKA
\begin{titlepage}
\thispagestyle{empty}
\begin{center}
\vspace*{0.5cm}
\Large Západoèeská univerzita v Plzni

\Large Fakulta aplikovanıch vìd

\Large Katedra kybernetiky

\vspace{8cm}
\LARGE DIPLOMOVÁ PRÁCE
\end{center}
\vfill
\large Plzeò, 2018
\hfill \large Martin Majer
\vspace*{1cm}
\end{titlepage}

% PROHLÁŠENÍ
\section*{Prohlášení}
\noindent Pøedkládám tímto k posouzení a obhajobì diplomovou práci zpracovanou na závìr studia na Fakultì aplikovanıch vìd Západoèeské univerzity v Plzni. \\[12pt]
\noindent Prohlašuji, e jsem diplomovou práci vypracoval samostatnì a vıhradnì s pouitím odborné literatury a pramenù, jejich úplnı seznam je její souèástí. \\[24pt]
\noindent V Plzni dne 25. bøezna 2018
\hfill
$ \dots \dots \dots \dots \dots \dots \dots $
\thispagestyle{empty}
\newpage

% PODÌKOVÁNÍ
\section*{Podìkování}
Tímto bych rád podìkoval vedoucímu diplomové práce, Ing. Luboši Šmídlovi, Ph.D., za cenné rady a pøipomínky.
\thispagestyle{empty}
\newpage

\section*{Anotace}
\noindent Tato práce se zabıvá klasifikací fonémù pomocí rùznıch architektur neuronovıch sítí. V~první èásti práce je pøedstavena obecná teorie dopøednıch a rekurentních neuronovıch sítí a následnì metoda CTC (connectionist temporal classification). Ve druhé èásti je pak vyhodnocena pøesnost rozpoznání šesti navrenıch architektur nad ètyømi parametrizacemi pro dvì datové sady o rùzné velikosti. Ukázalo se, e rekurentní neuronová sí vyuívající dvì obousmìrné LSTM vrstvy a metodu CTC dosahuje velmi vysoké pøesnosti, ale pouze díky vyuití informace z celé nahrávky. Proto byla testována i její varianta s~omezenou délkou vstupní sekvence, která pro tuto úlohu rovnì ukázala velkı potenciál a mohla by bıt vyuita pro rozpoznávání v reálném èase.\\[12pt]
\noindent \textbf{Klíèová slova:} rozpoznání fonémù, dopøedná neuronová sí, rekurentní neuronová sí, long short-term memory, gated recurrent unit, connectionist temporal classification

\section*{Abstract}
\noindent This thesis focuses on the phoneme recognition using various architectures of neural networks. The first part introduces theory of feedforward and recurrent neural networks followed by the introduction of the method CTC (connectionist temporal classification). The second part presents comparison of accuracy of recognition between six architectures on four parametrizations generated from two datasets of various size. It was shown that the recurrent neural network using two bidirectional LSTM layers combined with CTC achieved high accuracy although only due to using information from the whole recording. Therefore its alternative version which used a limited length of the input sequence was tested and also showed large potential and could be possibly used for recognition in real-time.\\[12pt]
\noindent \textbf{Keywords:} phoneme recognition, feedforward neural network, recurrent neural network, long short-term memory, gated recurrent unit, connectionist temporal classification 
\thispagestyle{empty}
\newpage

% OBSAH
\tableofcontents
\thispagestyle{empty}
\newpage

% TEORETICKÁ ÈÁST
\pagenumbering{arabic}
\section{Úvod}
Neuronové sítì byly vyvinuty ji v polovinì minulého století, ale kvùli nedostateèné vıpoèetní kapacitì nemohly bıt plnì vyuity pro øešení reálnıch problémù. A v posledních letech, kdy došlo k vıznamnému vıvoji v oblasti hardwaru jak pro poèítaèe, tak i pro mobilní a vestavìná zaøízení, zaèaly bıt plnì vyuívány, a to zejména pro komplexní úlohy v oblasti zpracování obrazu a øeèi, kde stabilnì pøekonávají ostatní algoritmy strojového uèení. Vıvoj vıkonnıch grafickıch karet a optimalizovanıch knihoven pak umonil rychlé trénování tìchto modelù na velkém mnoství dat, a díky vıkonnım èipùm lze provádìt predikci v reálném èase i na mobilních zaøízeních.

Tato práce se vìnuje vyuití neuronovıch sítí pro zpracování øeèi, a to zejména úloze klasifikace fonémù s vyuitím základních metod zpracování akustického signálu pøedstavenıch v \cite{bp}. Ve zpracování øeèi jsou bìnì vyuívány jak sítì dopøedné, tak i~rekurentní, které byly vytvoøeny pro klasifikaci èasovıch øad èi sekvencí a jsou schopny vyuívat kontextu. Pro trénování neuronovıch sítí je potøeba velké mnoství dat, aby byla zajištìna robustnost a schopnost generalizace s tím, e v úloze klasifikace fonémù jsou tato data ve formì zvukovıch nahrávek a odpovídajících pøepisù neboli transkripcí. 

Cílem práce je porovnat rùzné architektury neuronovıch sítí, které by mohly bıt vyuity pro pøepis mluvené øeèi do textové podoby (speech-to-text) v reálném èase. Za tímto úèelem byly voleny pøedevším jednodušší sítì s nízkım poètem parametrù. 

V první èásti práce je uvedena obecná teorie neuronovıch sítí a optimalizaèních algoritmù. Druhá èást se pak vìnuje metodì trénování rekurentních neuronovıch sítí, která nevyaduje pøedsegmentovaná trénovací data (více o segmentaci v \cite{bp}). Nakonec jsou porovnány rùzné architektury jak dopøednıch, tak i rekurentních neuronovıch sítí, na nìkolika datovıch sadách, které vznikly vyuitím rùznıch typù pøíznakù.

\newpage
\section{Neuronové sítì}
Neuronová sí je algoritmus, kterı je schopen aproximovat i silnì nelineární funkce a~zároveò je schopen dosáhnout vysoké míry statistické generalizace. Tento parametrickı model bıvá zpravidla sloen z nìkolika vrstev reprezentovanıch vektory, jejich dimenze udává šíøku modelu. Prvky tìchto vektorù, jednotky èi perceptrony, pracují paralelnì a~reprezentují funkce zobrazující vstupní vektor na skalár. Cílem je natrénovat parametry tohoto modelu tak, aby dokázal splnit zadanou úlohu s minimální chybou. Jedná se tedy o optimalizaèní úlohu.

Základním rozdílem mezi bìnou optimalizací a optimalizací v neuronovıch sítích je, e optimalizace v neuronovıch sítích a ve strojovém uèení obecnì probíhá nepøímo. To znamená, e aèkoliv optimalizujeme zvolenou metriku $ P $, která v jistém smyslu kvantifikuje vıkon algoritmu na dané úloze, minimalizujeme jinou cenovou funkci $ J(\boldsymbol{\theta}) $ za úèelem minimalizace metriky $ P $. V bìném optimalizaèním problému bychom minimalizovali pøímo cenovou funkci $ J(\boldsymbol{\theta}) $ za úèelem její minimalizace \cite{dl, karpathy, bishop}.

\subsection{Rozpoznávání øeèi pomocí neuronovıch sítí}
Ve zpracování øeèi jsou neuronové sítì vyuívány témìø ve všech jejích oblastech, a~to zejména v rozpoznávání øeèi a její syntéze, kde postupnì nahradily klasické statistické modely. Rozpoznávání øeèi je úloha, jejím cílem je získat pøepis mluvené øeèi v~textové podobì. V závislosti na aplikaci systému rozpoznávání øeèi a dalších omezujících podmínkách (doba odezvy, vıpoèetní nároènost) se pak nabízí dva pøístupy modelování øeèi - modelování po jednotlivıch fonémech èi po celıch slovech.

K rozpoznávání øeèi se v poslední letech zaèala vyuívat rekurentní neuronová sí, která je schopná vyuívat kontextu v øeèi, èi hluboká dopøedná sí. Vhodné architektury sítí jako Seq2Seq \cite{seq} èi rekurentní neuronová sí se ztrátovou funkcí CTC pak umoòují pøímı pøepis mluvené øeèi do textové podoby bez potøeby dekódování vıstupu modelu, jako tomu bylo napøíklad u skrytıch Markovovıch modelù. V posledních letech se ovšem také zaèaly k rozpoznávání øeèi vyuívat konvoluèní neuronové sítì, a to buï s vyuitím 1D konvoluce nebo 2D konvoluce, kdy se sí uèí ze spektrogramù, a jsou voleny podobné architektury jako pøi zpracování obrazu \cite{conv}.

Mezi další zajímavá vyuití neuronovıch sítí ve zpracování øeèi patøí napøíklad syntéza øeèi, pøeklad mluvené øeèi, verifikace øeèníka èi detekce klíèovıch frází. Neuronové sítì se také bìnì vyuívají ke kompresi pøíznakového vektoru (mùe bıt i rùzné délky) do pøíznakového vektoru o niší pevnì dané délce. K tomu se vyuívá tzv. bottleneck vrstva, která má vıraznì niší poèet jednotek ne její sousední vrstvy a po natrénování sítì slouí jako vıstupní vrstva. Tuto metodu komprese pøíznakového vektoru lze napøíklad vyuít pro rychlou detekci klíèovıch frází v kombinaci s algoritmem DTW.

Tato práce se vìnuje rozpoznávání øeèi po fonémech s vyuitím jak dopøednıch, tak rekurentních neuronovıch sítí s dekódováním i pøímım pøepisem.

\subsection{Cenová funkce}
Jedním ze zásadních aspektù pøi návrhu neuronovıch sítí je vıbìr cenové funkce. Vıbìr cenové funkce závisí na dané úloze a poadovaném vıstupu. Cenovou funkci definujeme stejnım zpùsobem jako u ostatních parametrickıch modelù, které generují hustotu pravdìpodobnosti $ p(\boldsymbol{y} \mid \boldsymbol{x} ; \boldsymbol{\theta}) $, kde $ \boldsymbol{y} $ je vıstupní tøída, $ \boldsymbol{x} $ je vstup modelu, a které pøi trénování vyuívají principu maximální vìrohodnosti. Vìtšinou cenovou funkci definujeme jako vzájemnou entropii mezi trénovacími daty a hustotou pravdìpodobnosti predikcí modelu
\begin{equation}
J(\boldsymbol{\theta}) = -\mathbb{E}_{\boldsymbol{x}, \boldsymbol{y} \sim \hat{p}_{data}} \log p_{model} (\boldsymbol{y} \mid \boldsymbol{x}).
\end{equation}
Pøedpis cenové funkce se liší model od modelu v závislosti na tvaru $ \log p_{model} $ a kromì definice ceny také mùe obsahovat regularizaèní prvky. Vıhodou vyuití principu maximální vìrohodnosti je, e specifikací modelu $ p(\boldsymbol{y} \mid \boldsymbol{x}) $ zároveò definujeme cenovou funkci $ \log p(\boldsymbol{y} \mid \boldsymbol{x}) $ \cite{dl, bishop}.

\subsection{Dopøedné neuronové sítì}
Dopøedné neuronové sítì, obèas také nazıvané vícevrstvé perceptrony, jsou základním typem neuronovıch sítí. Jejich cílem je aproximovat urèitou funkci $ f^{*} $, napø. klasifikátor $ \boldsymbol{y} = f^{*}(\boldsymbol{x}) $ zobrazuje vstup $ \boldsymbol{x} $ na vıstupní tøídu $ \boldsymbol{y} $. Dopøedná neuronová sí definuje zobrazení $ \boldsymbol{y} = f^{*}(\boldsymbol{x} ; \boldsymbol{\theta}) $ a uèí se optimální hodnoty parametrù $ \boldsymbol{\theta} $, pøi kterıch by mìla bıt dosaena nejlepší aproximace funkce.

Jak název napovídá, tok informací smìøuje od vstupu $ \boldsymbol{x} $ pøes nìkolik po sobì jdoucích vıpoèetních vrstev definujících $ f $ a k vıstupu $ \boldsymbol{y} $. Jedná se tedy o sloení nìkolika rùznıch funkcí do øetìzové struktury. Takto definovanı model lze také popsat jako orientovanı acyklickı graf, kterı urèuje závislosti mezi funkcemi. Mìjme napøíklad sí sloenou ze tøí funkcí $ f^{(1)} $, $ f^{(2)} $ a $ f^{(3)} $ spojenıch do øady $ f( \boldsymbol{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\boldsymbol{x}))) $. V tomto pøípadì nazıváme  $ f^{(1)} $ první skrytou vrstvou, $ f^{(2)} $ druhou skrytou vrstvou a $ f^{(3)} $ vıstupní vrstvou. Celková délka tohoto øetìzce pak udává hloubku modelu \cite{dl}.

Cílem trénování sítì je, aby se nauèila odpovídající zobrazení mezi $ f(\boldsymbol{x}) $ a $ f^{*}(\boldsymbol{x}) $. Uèení probíhá na základì trénovacích dat, která poskytují zašumìná aproximovaná pozorování $ f^{*}(\boldsymbol{x}) $ vyhodnocena v rùznıch bodech. Ke kadému pozorování $ \boldsymbol{x} $ je k dispozici také skuteèná hodnota $ \boldsymbol{y} \approx f^{*}(\boldsymbol{x}) $. Vıstupní vrstva sítì se pro kadou trénovací hodnotu $ \boldsymbol{x} $ snaí vyprodukovat hodnotu blízkou $ \boldsymbol{y} $. Chování skrytıch vrstev není pøímo dáno trénovacími daty a sí se je musí nauèit vyuívat k získání poadovaného vısledku, tj. k~aproximaci $ f^{*}(\boldsymbol{x}) $ \cite{dl, bishop}.

\subsubsection{Architektura}
Klíèovım prvkem pøi návrhu neuronovıch sítí je jejich architektura. Architekturou sítì èi modelu rozumíme celkovou strukturu sítì - kolik má mít jednotek a jak mají bıt tyto jednotky mezi sebou propojeny.

Jak ji bylo uvedeno, neuronové sítì jsou tvoøeny skupinami jednotek, které jsou uspoøádány do jednotlivıch vrstev. Vìtšina architektur tyto vrstvy uspoøádává do øetìzové struktury, kde kadá vrstva je funkcí vrstvy, která ji pøedchází. V této struktuøe je pak první vrstva definována jako
\begin{equation}
\boldsymbol{h}^{(1)} = g^{(1)}\left( \boldsymbol{W}^{(1)\top} \boldsymbol{x} + \boldsymbol{b}^{(1)} \right),
\end{equation}
druhá vrstva jako 
\begin{equation}
\boldsymbol{h}^{(2)} = g^{(2)}\left( \boldsymbol{W}^{(2)\top} \boldsymbol{h}^{(1)} + \boldsymbol{b}^{(2)} \right),
\end{equation}
a tak dále, kde $ g^{(i)} $ je aktivaèní funkce vrstvy $ i $, $ \boldsymbol{W}^{(i)\top} $ je váhová matice vrstvy $ i $ a $ \boldsymbol{b}^{(i)} $ je prahovı vektor vrstvy $ i $. 

Pro takto definovanou øetìzovou architekturu je pak hlavním problémem urèení hloubky sítì a šíøky kadé vrstvy. Vhodnou architekturu pro danou úlohu je tøeba nalézt pomocí experimentù zaloenıch na sledování chyby na validaèní datové sadì a apriorní znalosti o úloze a datech \cite{dl, karpathy, bishop, hastie}.

\subsubsection{Vıstupní jednotky}
Reprezentace vıstupu je úzce spojena s danou úlohou a tím i s vıbìrem cenové funkce. Pøedpokládejme, e dopøedná neuronová sí poskytuje vıstupní vrstvì soubor skrytıch pøíznakù  $ \boldsymbol{h} = f(\boldsymbol{x} ; \boldsymbol{\theta}) $, tj. vıstup poslední skryté vrstvy. Cílem vıstupní vrstvy je pak provést urèitou transformaci tìchto pøíznakù, aby sí plnila úlohu, pro kterou byla navrena. Tato transformace je provedena pouitím aktivaèní funkce $ g(\boldsymbol{h}) $. Aèkoliv existuje mnoho aktivaèních funkcí, které lze ve vıstupní vrstvì vyuít, zde se zamìøíme pouze na aktivaèní funkci softmax, která je vyuívána v experimentech provedenıch v rámci této práce.

Aktivaèní funkce softmax je vyuívána pro úlohy, kde je tøeba reprezentovat vıstup jako hustotu pravdìpodobnosti diskrétní promìnné s $ n $ monımi hodnotami (vıstupními tøídami). K odvození aktivaèní funkce softmax vyuijeme znalosti o úloze binární klasifikace, pøi které predikujeme hodnotu
\begin{equation}
\hat{\boldsymbol{y}} = P(\boldsymbol{y} = 1 \mid \boldsymbol{x}),
\end{equation}
kde $ \hat{\boldsymbol{y}} \in \interval{0}{1} $. Aby byla zajištìna numerická stabilita pøi optimalizaci, budeme radìji predikovat hodnotu
\begin{equation}
\boldsymbol{z} = \log \tilde{P} (\boldsymbol{y} = 1 \mid \boldsymbol{x}).
\end{equation}
Aplikací exponenciální funkce a následnou normalizací bychom pak dostali Bernoulliho rozloení pravdìpodobnosti øízeného sigmoidální funkcí.

Pro generalizaci tohoto postupu pro diskrétní promìnnou s $ n $ hodnotami je potøeba získat vektor $ \hat{\boldsymbol{y}} $, kde $ \hat{\boldsymbol{y}}_{i} = P(\boldsymbol{y} = i \mid \boldsymbol{x}) $. Pro kadı prvek $ \hat{\boldsymbol{y}}_{i}  $ musí platit $ \hat{\boldsymbol{y}}_{i} \in \interval{0}{1} $ a zároveò $ \sum_{i} \hat{\boldsymbol{y}}_{i}  = 1 $, aby bylo moné tento vektor interpretovat za hustotu pravdìpodobnosti. Nejprve je potøeba provést predikci vrstvou s lineární aktivaèní funkcí, která predikuje nenormalizované logaritmované pravdìpodobnosti
\begin{equation}
\boldsymbol{z} = \boldsymbol{W}^{\top} \boldsymbol{h} + \boldsymbol{b},
\end{equation}
kde $ \boldsymbol{z}_{i} = \log \tilde{P}(\boldsymbol{y} = i \mid \boldsymbol{x}) $. Softmax funkce pak aplikuje exponenciální funkci a normalizuje $ \boldsymbol{z} $, èím získáme poadované $ \hat{\boldsymbol{y}} $.
\begin{equation}
softmax(\boldsymbol{z}_{i}) = \dfrac{\exp(\boldsymbol{z}_{i})}{\sum_{j} \exp(\boldsymbol{z}_{j})}
\end{equation}
Natrénováním parametrù modelu pak bude vıstupní vrstva s aktivaèní funkcí softmax predikovat podíly poètù všech pozorovanıch vısledkù v trénovací datové sadì \cite{dl}.
\begin{equation}
softmax(\boldsymbol{z}(\boldsymbol{x} ; \boldsymbol{\theta}))_{i} \approx \dfrac{\sum_{j=1}^{m} 1_{\boldsymbol{y}^{(j)} = i, \boldsymbol{x}^{(j)} = \boldsymbol{x}}}{\sum_{j=1}^{m} 1_{\boldsymbol{x}^{(j)} = \boldsymbol{x}}}
\end{equation}

\subsubsection{Skryté jednotky}
Jak název napovídá, skryté jednotky jsou jednotky skryté vrstvy, jejich vstupem je vektor $ \boldsymbol{x} $, kterı je transformován na $ \boldsymbol{z} = \boldsymbol{W}^{\top} \boldsymbol{x} + \boldsymbol{b} $. Na takto transformovanı vstup je pak po prvcích aplikována nelineární aktivaèní funkce  $ g(\boldsymbol{z}) $. Volba aktivaèních funkcí skrytıch vrstev vyaduje mnoho experimentù a vyhodnocení pøesnosti modelu na validaèní datové sadì. V dnešní dobì se pro dopøedné neuronové sítì vìtšinou volí jednotky s aktivaèní funkcí ReLU (z anglického "rectified linear unit") èi její modifikace, pro sítì rekurentní jsou pak voleny funkce hyperbolickı tangens a hard sigmoid.
\begin{itemize}
\item \textbf{ReLU} - tyto jednotky vyuívají aktivaèní funkci $ g(\boldsymbol{z}) = \max\{0, \boldsymbol{z}\} $. Jedná se o~lineární jednotky s prahem v bodì nula - levá polovina jejich definièního oboru je rovna nule. To zaruèuje rychlı vıpoèet a vysokou hodnotu gradientu, kdykoliv je jednotka aktivní. Zásadním nedostatkem je, e tyto jednotky se pomocí gradientních metod nemohou uèit z pozorování, které mají aktivaèní hodnotu rovnou nule. Existuje proto nìkolik modifikací, které zajistí, e jednotky budou mít gradient všude (napø. Leaky ReLU, PReLU, Maxout).
\item \textbf{sigmoid} a \textbf{tanh} - tyto jednotky vyuívají logistickou funkci (sigmoid) $ g(\boldsymbol{z}) = \sigma (\boldsymbol{z}) $, resp. hyperbolickı tangens $ \tanh (\boldsymbol{z}) = 2\sigma (2\boldsymbol{z}) - 1 $. Hlavním nedostatkem sigmoidální funkce je její citlivost a náchylnost k saturaci, kdy mùe dojít k tzv. explozi èi vymizení gradientu a sí nebude schopná se uèit.
\item \textbf{hard sigmoid} - tyto jednotky vyuívají aktivaèní funkci $ g(\boldsymbol{z}) = \max(0, \min(1, \frac{\boldsymbol{z} + 1}{2})) $. Jedná se o lineární aproximaci funkce sigmoid a díky své vıpoèetní nenároènosti jsou vyuívány v sítích typu LSTM \cite{dl, karpathy}.
\end{itemize}

\subsubsection{Algoritmus zpìtného šíøení}
Dopøedná neuronová sí pøijme na vstupu poèáteèní informaci o pozorování $ \boldsymbol{x} $, kterou poté šíøí skrz skryté jednotky ve všech skrytıch vrstvách a k vıstupní vrstvì, která vygeneruje odhad $ \hat{\boldsymbol{y}} $. Informaèní tok tedy proudí skrz sí smìrem dopøedu a tomuto procesu se øíká dopøedné šíøení. V trénovací fázi probíhá dopøedné šíøení tak dlouho, dokud není vygenerována skalární cena $ J(\boldsymbol{\theta}) $. Algoritmus zpìtného šíøení (anglicky backpropagation) pak umoní zpìtnı tok informace od ceny skrz sí za úèelem vıpoètu gradientu.

Analytickı vıpoèet gradientu je pomìrnì pøímoèarı, nicménì numericky mùe bıt velice nároènı. Algoritmus zpìtného šíøení provádí vıpoèet gradientu $ \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) $ pomocí jednoduché a vıpoèetnì nenároèné procedury. Takto vypoètenı gradient je pak vyuit pøi uèení pomocí gradientních metod \cite{dl, karpathy}.

Algoritmus zpìtného šíøení vyuívá k vıpoètu gradientu øetìzové pravidlo. Øetìzové pravidlo se bìnì vyuívá k vıpoètu derivací sloenıch funkcí, které jsou sloeny z funkcí, jejich derivace jsou známé. Mìjme reálné èíslo $ x $ a funkce $ f : \mathbb{R} \mapsto \mathbb{R} $ a $ g: \mathbb{R} \mapsto \mathbb{R} $. Dále pøedpokládejme, e $ y = g(x) $ a $ z = f(g(x)) = f(y) $. Øetìzové pravidlo je pak dáno jako
\begin{equation}
\dfrac{dz}{dx} = \dfrac{dz}{dy}\dfrac{dy}{dx}.
\end{equation}
Toto pravidlo lze snadno rozšíøit ze skalárního pøípadu. Pøedpokládejme, e $ \boldsymbol{x} \in \mathbb{R}^{n} $, $ \boldsymbol{y} \in \mathbb{R}^{n}  $, $ g: \mathbb{R}^{m} \mapsto \mathbb{R}^{n} $ a $ f : \mathbb{R}^{n} \mapsto \mathbb{R} $. Pokud  $ \boldsymbol{y} = g(\boldsymbol{x}) $ a $ \boldsymbol{z} =  f(\boldsymbol{y}) $, pak
\begin{equation}\label{eq:chain}
\dfrac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}_{i}} = \sum_{j} \dfrac{\partial \boldsymbol{z}}{\partial \boldsymbol{y}_{j}} \dfrac{\partial \boldsymbol{y}_{j}}{\partial \boldsymbol{x}_{i}}
\end{equation}
Zapíšeme-li rovnici \eqref{eq:chain} vektorovì, získáme tvar
\begin{equation}
\nabla_{\boldsymbol{x}} \boldsymbol{z} = \left( \dfrac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \right)^{\top} \nabla_{\boldsymbol{y}} \boldsymbol{z},
\end{equation}
kde $ \dfrac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \in \mathbb{R}^{n \times m} $ je Jakobián $ \boldsymbol{x} $.
Toto pravidlo lze dále snadno rozšíøit i na tensory, jen jsou v neuronovıch sítích èasto vyuívány (podrobnìji v \cite{dl, chain}).

Pøedpokládejme sí o hloubce $ l $, kde kadé vrstvì náleí váhová matice $ \boldsymbol{W}^{(i)} $, $ i \in \{ 1, \ldots, l \} $ a prahovı vektor $ \boldsymbol{b}^{(i)} $. Ztrátová funkce $ L(\boldsymbol{y}, \hat{\boldsymbol{y}}) $ závisí na skuteèné hodnotì $ \boldsymbol{y} $ a~na vıstupu sítì $ \boldsymbol{\hat{y}} $, kterı sí vygeneruje pro vstup $ \boldsymbol{x} $. Celková cena $ J $ pro zjednodušení odpovídá pøímo ztrátové funkci $ L $ a neobsahuje ádnou regularizaèní sloku.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
$ \boldsymbol{h}^{(0)} = \boldsymbol{x} $ \\
\For{$ k = 1,\ldots,l $}{
	$ \boldsymbol{a}^{(k)} = \boldsymbol{W}^{(k)} \boldsymbol{h}^{(k-1)} + \boldsymbol{b}^{(k)}  $ \\
	$ \boldsymbol{h}^{(k)} = f(\boldsymbol{a}^{(k)}) $
}
$ \hat{\boldsymbol{y}} = \boldsymbol{h}^{(l)} $ \\
$ J = L(\boldsymbol{y}, \hat{\boldsymbol{y}}) $
\end{algorithm}
\begin{center}
Algoritmus 1: Dopøedné šíøení.
\end{center}

Pøi zpìtném prùchodu jsou poèítány gradienty aktivací $ \boldsymbol{a}^{(k)} $ pro kadou vrstvu $ k $ poèínaje vıstupní vrstvou a k první skryté vrstvì. Tyto gradienty indikují, jak by se mìly zmìnit vıstupy jednotlivıch vrstev, aby došlo ke sníení chyby. Spoètené gradienty vah a prahù mohou bıt rovnou vyuity pro zmìnu parametrù pomocí optimalizaèního algoritmu \cite{dl}.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
vıpoèet gradientu vıstupní vrstvy (po dopøedném prùchodu sítì) \\
$ g \leftarrow \nabla_{\hat{\boldsymbol{y}}} J = \nabla_{\hat{\boldsymbol{y}}} L (\boldsymbol{y}, \hat{\boldsymbol{y}}) $ \\
\For{$ k = l, l-1, \ldots, 1 $}{
	pøevedení gradientu do tvaru pøed aplikací nelineární aktivaèní funkce \\
	$ g \leftarrow \nabla_{\boldsymbol{a}^{(k)}} J = g \odot f'(\boldsymbol{a}^{(k)}) $ \\
	vıpoèet gradientù prahovıch vektorù a váhovıch matic \\
	$ \nabla_{\boldsymbol{b}^{(k)}} J = g $ \\
	$ \nabla_{\boldsymbol{W}^{(k)}} J = g \boldsymbol{h}^{(k-1)\top} $ \\
	šíøení gradientu do niší vrstvy \\
	$ g \leftarrow \nabla_{\boldsymbol{h}^{(k-1)}} J = \boldsymbol{W}^{(k)\top} g $
}
\end{algorithm}
\begin{center}
Algoritmus 2: Zpìtné šíøení.
\end{center}

\subsection{Rekurentní neuronové sítì}
Rekurentní neuronové sítì (dále jen RNN z anglického "recurrent neural networks") jsou typem neuronovıch sítí, které umí zpracovávat sekvenèní data, tj. sekvenci hodnot $ \boldsymbol{x}^{(1)}, \ldots , \boldsymbol{x}^{(\tau)} $, a pøedávat si informace mezi jednotlivımi èasovımi kroky. Základním rozdílem oproti dopøednım neuronovım sítím je zavedení zpìtné smyèky a vyuití sdílení parametrù. RNN tedy netrénuje pro kadı èasovı krok samostatnı soubor parametrù, ale tyto parametry jsou sdíleny pøes všechny prvky sekvence. Díky tomu jsou RNN schopny generalizovat na rùzné délky sekvencí a na rùzné pozice dùleitıch informací v èase (napø. dvì témìø stejné vìty, kde jedna obsahuje datum na zaèátku vìty a druhá na konci).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{rnn_general.png}
    \caption{Schéma rekurentní neuronové sítì. Pøevzato z \cite{colahNN}.}
    \label{fig:rnn_general}
\end{figure}

Sdílení parametrù je zajištìno "rozvinutím" \ rekurentního vıpoètu do grafu s opakující se strukturou. Prvek sekvence v kadém èasovém kroku je tedy zpracováván stejnou sítí. Pro jednoduchost pøedpokládejme, e RNN pracuje se sekvencí obsahující vektory $ \boldsymbol{x}^{(t)} $, $ t = 1, \ldots, \tau $ a model je ve tvaru
\begin{equation}\label{eq:unfold}
\boldsymbol{s}^{(t)} = f(\boldsymbol{s}^{(t-1)}; \boldsymbol{\theta}),
\end{equation}
kde $ \boldsymbol{s} $ je stav systému. Tento rekurzivní vztah je moné pro koneènı poèet krokù $ \tau $ rozvinout $ \tau $-krát. Napøíklad pro model danı vztahem \eqref{eq:unfold} a $ \tau = 3 $ získáme
\begin{equation}
\boldsymbol{s}^{(3)} = f(\boldsymbol{s}^{(2)}; \boldsymbol{\theta}) = f(f(\boldsymbol{s}^{(1)}; \boldsymbol{\theta}); \boldsymbol{\theta}).
\end{equation}
Tímto rozvinutím zajistíme, e vztah neobsahuje ádnou rekurenci a je moné ho reprezentovat jako acyklickı orientovanı graf \cite{dl, colahLSTM}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{rnn_unrolled.png}
	\caption{Ilustrace rozvinutí rekurentní neuronové sítì. Pøevzato z \cite{colahLSTM}.}
    \label{fig:rnn_unrolled}
\end{figure}

Základní rovnici RNN získáme pøidáním závislosti na externím vstupu  $ \boldsymbol{x}^{(t)} $
\begin{equation}
\boldsymbol{h}^{(t)} = f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)}; \boldsymbol{\theta}),
\end{equation}
kde $ \boldsymbol{h} $ je stav skrytıch jednotek. Pøi trénování sítì na danou úlohu se pak sí uèí vyuívat $ \boldsymbol{h}^{(t)} $ jako zobrazení relevantních informací z minulıch èasovıch krokù od vstupu a po $ t $. Toto zobrazení je ztrátové, jeliko se jedná o zobrazení sekvence o promìnné délce $ (\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}) $ na vektor pevnì dané délky $ \boldsymbol{h}^{(t)} $.

Definujme si rovnice pro dopøedné šíøení základní RNN s aktivaèní funkcí hyperbolickı tangens ve skryté vrstvì. Sí je urèena ke klasifikaci $ n $ tøíd, kde vıstup $ \boldsymbol{o} $ reprezentuje nenormalizované pravdìpodobnosti jednotlivıch tøíd vıstupní diskrétní promìnné a~aplikací operace softmax je pak získán vıslednı vektor $ \hat{\boldsymbol{y}} $ normalizovanıch pravdìpodobností nad vıstupem. Dopøedné šíøení vyaduje specifikaci poèáteèního stavu $ \boldsymbol{h}^{(0)} $, poté jsou pro kadı èasovı krok $ t = 1, \ldots, \tau $ vypoèteny následující rovnice
\begin{align}
\boldsymbol{a}^{(t)} &= \boldsymbol{b} + \boldsymbol{W} \boldsymbol{h}^{(t-1)} + \boldsymbol{U} \boldsymbol{x}^{(t)} , \\
\boldsymbol{h}^{(t)} &= \tanh(\boldsymbol{a}^{(t)}) , \\
\boldsymbol{o}^{(t)} &= \boldsymbol{c} + \boldsymbol{V} \boldsymbol{h}^{(t)} , \\
\hat{\boldsymbol{y}}^{(t)} &= \text{softmax}(\boldsymbol{o}^{(t)}) ,
\end{align}
kde parametry $ \boldsymbol{b} $ a $ \boldsymbol{c} $ jsou prahové vektory a váhové matice $ \boldsymbol{U} $, $ \boldsymbol{V} $ a $ \boldsymbol{W} $ popoøadì odpovídají skrytım spojením mezi vstupem a skrytou vrstvou, mezi skrytou vrstvou a vıstupem a~mezi skrytou vrstvou a pøedcházející skrytou vrstvou \cite{dl}.

\subsubsection{Algoritmus zpìtného šíøení èasem}
Pro vıpoèet gradientu RNN se vyuívá algoritmus zpìtného šíøení èasem. Jedná se o generalizovanı algoritmus zpìtného šíøení nad rozvinutım vıpoèetním grafem a jeho èasová nároènost je $ O(\tau) $. Èasovou nároènost tohoto algoritmu není moné sníit paralelizací, jeliko dopøednıch prùchod je sekvenèní a hodnoty v aktuálním èasovém kroku jsou závislé na pøedchozích hodnotách.

Pøedpokládejme reprezentaci rozvinuté RNN ve tvaru acyklického orientovaného grafu, jeho uzly obsahují parametry $ \boldsymbol{U} $, $ \boldsymbol{V} $, $ \boldsymbol{W} $, $ \boldsymbol{b} $, $ \boldsymbol{c} $, a sekvenci uzlù indexovanıch èasem $ t $ pro $ \boldsymbol{x}^{(t)} $, $ \boldsymbol{h}^{(t)} $, $ \boldsymbol{o}^{(t)} $ a $ L^{(t)} $. Pro kadı uzel grafu $ N $ pak potøebujeme rekurzivnì spoèítat gradient $ \nabla_{N} L $ v závislosti na uzlech grafu, které tento uzel následují. Zaèneme rekurzi v uzlu, kterı bezprostøednì pøedchází vıslednou ztrátu $ L $ 
\begin{align}
L(\{ \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(\tau)}  \}, \{ \boldsymbol{y}^{(1)}, \ldots, \boldsymbol{y}^{(\tau)} \}) &= - \sum_{t} \log_{p_{model}} \left( \boldsymbol{y}^{(t)} \mid \{ \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)} \}  \right) , \\
&\dfrac{\partial L}{\partial L^{(t)}} = 1.
\end{align}
Pro vıpoèet gradient $ \nabla_{\boldsymbol{o}^{(t)}} L $ nad všemi vıstupy v èase $ t $ pro všechna $ i, t $ je opìt vyuito øetìzové pravidlo
\begin{equation}
(\nabla_{\boldsymbol{o}^{(t)}} L)_{i} = \dfrac{\partial L}{\partial \boldsymbol{o}_{i}^{(t)}} = \dfrac{\partial L}{\partial L^{(t)}} \dfrac{\partial L^{(t)}}{\partial \boldsymbol{o}_{i}^{(t)}} = \hat{\boldsymbol{y}}_{i}^{(t)} - \boldsymbol{1}_{i, \boldsymbol{y}^{(t)}}.
\end{equation}
Vıpoèet gradientu dále pokraèuje pøes další uzly poèínaje koncem sekvence. V koneèném èasovém kroku $ \tau $ má $ \boldsymbol{h}^{(\tau)} $ za následníka pouze $ \boldsymbol{o}^{(\tau)} $ a gradient je
\begin{equation}
\nabla_{\boldsymbol{h}^{(\tau)}} L = \boldsymbol{V}^{\top} \nabla_{\boldsymbol{o}^{(\tau)}} L.
\end{equation}
Gradient se dál šíøí od $ t = \tau - 1 $ a k $ t = 1 $ s tím, e $ \boldsymbol{h}^{(t)} $ (pro $ t < \tau $) má jako své pøedchùdce $ \boldsymbol{o}^{(t)} $ a $ \boldsymbol{h}^{(t+1)} $. Gradient skryté vrstvy je pak
\begin{align}
\nabla_{\boldsymbol{h}^{(t)}} L &= \left( \dfrac{\partial \boldsymbol{h}^{(t+1)}}{\partial \boldsymbol{h}^{(t)}}  \right)^{\top} (\nabla_{\boldsymbol{h}^{(t+1)}} L) + \left( \dfrac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{h}^{(t)}}  \right)^{\top} ( \nabla_{\boldsymbol{o}^{(t)}} L ) \notag \\
&= \boldsymbol{W}^{\top} (\nabla_{\boldsymbol{h}^{(t+1)}} L) \text{ diag} \left(  1 - (\boldsymbol{h}^{(t+1)})^{2} \right) + \boldsymbol{V}^{\top} (\nabla_{\boldsymbol{o}}^{(t)} L),
\end{align}
kde $  \text{diag} \left(  1 - (\boldsymbol{h}^{(t+1)})^{2} \right) $ je Jakobián hyperbolického tangensu pro skryté jednotky $ i $ v èase $ t+1 $. Na základì gradientù skrytıch stavù lze dopoèítat gradienty parametrù. Vzhledem k tomu, e parametry jsou sdíleny mezi jednotlivımi èasovımi kroky, zavedeme nové promìnné $ \boldsymbol{W}^{(t)} $, resp. $ \boldsymbol{U}^{(t)} $, které jsou kopiemi váhovıch matic $ \boldsymbol{W} $, resp. $ \boldsymbol{U} $ a znaèí, jakou mírou tyto váhy pøispívají ke gradientu v èase $ t $ \cite{dl}.
\begin{align}
\nabla_{\boldsymbol{c}} L &= \sum_{t} \left( \dfrac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{c}} \right)^{\top} \nabla_{\boldsymbol{o}^{(t)}} L = \sum_{t} \nabla_{\boldsymbol{o}^{(t)}} L  \\
\nabla_{\boldsymbol{b}} L &= \sum_{t} \left( \dfrac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{b}^{(t)}} \right)^{\top} \nabla_{\boldsymbol{h}^{(t)}} L = \sum_{t} \text{ diag} \left( 1 - (\boldsymbol{h}^{(t)})^{2} \right)  \nabla_{\boldsymbol{h}^{(t)}} L \\
\nabla_{\boldsymbol{V}} L &= \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial \boldsymbol{o}_{i}^{(t)}} \right) \nabla_{\boldsymbol{V}} \boldsymbol{o}_{i}^{(t)} = \sum_{t} (\nabla_{\boldsymbol{o}^{(t)}} L) \boldsymbol{h}^{(t)\top} \\
\nabla_{\boldsymbol{W}} L &= \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial \boldsymbol{h}_{i}^{(t)}} \right) \nabla_{\boldsymbol{W}^{(t)}} \boldsymbol{h}_{i}^{(t)} = \sum_{t} \text{ diag} \left( 1 - (\boldsymbol{h}^{(t)})^{2} \right) (\nabla_{\boldsymbol{h}}^{(t)} L) \boldsymbol{h}^{(t-1)\top} \\
\nabla_{\boldsymbol{U}} L &= \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial \boldsymbol{h}_{i}^{(t)}} \right) \nabla_{\boldsymbol{u}^{(t)}} \boldsymbol{h}_{i}^{(t)} = \sum_{t} \text{ diag} \left( 1 - (\boldsymbol{h}^{(t)})^{2} \right) (\nabla_{\boldsymbol{h}^{(t)}} L) \boldsymbol{x}^{(t)\top} 
\end{align}

\subsubsection{Obousmìrné rekurentní neuronové sítì}
Rekurentní neuronové sítì zachycují do stavu v èase $ t $ pouze informaci z minulosti, tj. informace ze vstupù $ \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t-1)} $ a aktuálního vstupu $ \boldsymbol{x}^{(t)} $. V mnoha úlohách ovšem mùe správná predikce $ \boldsymbol{y}^{(t)} $ záviset i na budoucích vstupech èi na celé sekvenci. Tento problém øeší obousmìrná rekurentní neuronová sí (dále BRNN z anglického "bidirectional recurrent neural network").

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{rnn_bidirectional.png}
    \caption{Schéma obousmìrné rekurentní neuronové sítì. Pøevzato z \cite{colahNN}.}    
    \label{fig:rnn_bidirectional}
\end{figure}

BRNN kombinuje dvì RNN, kde jedna sí se pohybuje v èase kupøedu od zaèátku sekvence, zatímco druhá se pohybuje v èase zpìt od konce sekvence. Tyto dvì sítì mezi sebou sdílí skryté stavy $ \boldsymbol{h}^{(t)} $ (sí dopøedná) a $ \boldsymbol{g}^{(t)} $ (sí zpìtná), co zajistí, e vıstupní jednotky $ \boldsymbol{o}^{(t)} $ jsou závislé jak na minulosti, tak na budoucnosti. Vıstupy skrytıch stavù  $ \boldsymbol{h}^{(t)} $ a  $ \boldsymbol{g}^{(t)} $ nejsou mezi sebou nijak propojeny. Pro vıpoèet gradientù lze opìt vyuít algoritmus zpìtného šíøení èasem dle postupu, kterı je shrnut v algoritmu è. 4 \cite{dl, brnn}.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
\For{$ t = 1,  \ldots, \tau $}{
	dopøedné šíøení dopøednou sítí od $ t = 1 $ do $ t = \tau $ \\
	dopøedné šíøení zpìtnou sítí od $ t = \tau $ do $ t = 1 $ \\
	dopøedné šíøení nad vıstupními jednotkami obou sítí \\
}
\For{$ t = 1,  \ldots, \tau $}{
	zpìtné šíøení nad vıstupními jednotkami obou sítí \\
	zpìtné šíøení dopøednou  sítí od $ t = \tau $ do $ t = 1 $ \\
	zpìtné šíøení zpìtnou sítí od $ t = 1 $ do $ t = \tau $
}
zmìna parametrù
\end{algorithm}
\begin{center}
Algoritmus 3: Postup trénování obousmìrné rekurentní neuronové sítì.
\end{center}

\subsubsection{LSTM}
Rekurentní sítì jsou velice náchylné na problém vymizení èi saturace gradientu. K~tìmto problémùm dochází zejména pøi modelování dlouhodobıch závislostí, kdy jsou pøi vıpoètu váhy násobeny nìkolikrát samy sebou a v závislosti na jejich øádu mohou nabıvat velmi malıch èi velmi velkıch hodnot. To má pak za následek trvalou deaktivaci nìkterıch neuronù èi neschopnost sítì se uèit. Tento problém øeší sítì LSTM (z anglického "long short-term memory").

LSTM sítì, na rozdíl od bìnıch RNN, nejsou tvoøeny jednotkami, ale buòkami, které kromì vnìjší rekurence obsahují i rekurenci vnitøní (smyèky). Kadá buòka má stejnı vstup a vıstup jako obyèejná rekurentní jednotka, ale má více parametrù díky systému jednotek opatøenıch bránou, které øídí tok informací stavem buòky. Jedná se o vstupní bránu, zapomínací bránu a vıstupní bránu, které urèují, jaké vstupní informace mají bıt vpuštìny do stavu buòky, jaké informace ze stavu buòky odstranit a jaké informace ze stavu buòky mají bıt vypuštìny na její vıstup.

Nejdùleitìjší komponentou LSTM je stavová jednotka $ \boldsymbol{s}_{i}^{(t)} $ (pro buòku $ i $ v èase $ t $), která je opatøena lineární smyèkou. Váha této smyèky je øízena jednotkou se zapomínací bránou $ \boldsymbol{f}_{i}^{(t)} $, která díky aktivaèní funkci sigmoid umoòuje nastavovat tuto váhu na hodnoty mezi 0 a 1
\begin{equation}
\boldsymbol{f}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{f} + \sum_{j} \boldsymbol{U}_{i,j}^{f} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{f} h_{j}^{(t-1)}  \right),
\end{equation}
kde $ \boldsymbol{x}^{(t)} $ je vstupní vektor v èase $ t $, $ \boldsymbol{h}^{(t)} $ vektor skryté vrstvy obsahující vıstupy všech LSTM buòek, $ \boldsymbol{b}^{f} $, $ \boldsymbol{U}^{f} $ a $ \boldsymbol{W}^{f} $ jsou popoøadì prahovı vektor, váhová matice vstupu a~rekurentní váhová matice zapomínací brány.  Stav buòky se pak øídí pravidlem
\begin{equation}
\boldsymbol{s}_{i}^{(t)} = \boldsymbol{f}_{i}^{(t)} \boldsymbol{s}_{i}^{(t-1)} + \boldsymbol{g}_{i}^{(t)} \sigma \left( \boldsymbol{b}_{i} + \sum_{j} \boldsymbol{U}_{i,j} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j} \boldsymbol{h}_{j}^{(t-1)}  \right),
\end{equation}
kde $ \boldsymbol{b} $, $ \boldsymbol{U} $, $ \boldsymbol{W} $ jsou popoøadì prahovı vektor, váhová matice vstupu a rekurentní váhová matice buòky. Stav buòky je také závislı na jednotce s vstupní bránou $ \boldsymbol{g}_{i}^{(t)} $, která se chová podobnì jako jednotka se zapomínací branou
\begin{equation}
\boldsymbol{g}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{g} + \sum_{j} \boldsymbol{U}_{i,j}^{g} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{g} h_{j}^{(t-1)}  \right),
\end{equation}
kde $ \boldsymbol{b}^{g} $, $ \boldsymbol{U}^{g} $ a $ \boldsymbol{W}^{g} $ jsou popoøadì prahovı vektor, váhová matice vstupu a rekurentní váhová matice vstupní brány. Vıstup buòky  $ \boldsymbol{h}_{i}^{(t)} $ je pak øízen pomocí vıstupní brány $ \boldsymbol{q}_{i}^{(t)} $, která opìt øídí propustnost pomocí aktivaèní funkce sigmoid
\begin{align}
\boldsymbol{h}_{i}^{(t)} &= \tanh (\boldsymbol{s}_{i}^{(t)}) \boldsymbol{q}_{i}^{(t)}, \\
\boldsymbol{q}_{i}^{(t)} &= \sigma \left( \boldsymbol{b}_{i}^{o} + \sum_{j} \boldsymbol{U}_{i,j}^{o} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{o} \boldsymbol{h}_{j}^{(t-1)}  \right),
\end{align}
kde $ \boldsymbol{b}^{o} $, $ \boldsymbol{U}^{o} $ a $ \boldsymbol{W}^{o} $ jsou popoøadì prahovı vektor, váhová matice vstupu a rekurentní váhová matice vıstupní brány \cite{dl, colahLSTM, lstm}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{lstm.png}
    \caption{Schéma LSTM buòky. Pøevzato z \cite{colahLSTM}.}
    \label{fig:lstm}
\end{figure}

\subsubsection{GRU}
Sítì GRU (z anglického "gated recurrent unit") dále zjednodušují LSTM architekturu ponecháním pouze nezbytnıch komponent. Vıstup buòky a stav buòky je slouèen do jednoho stavu, kterı je øízen jednotkou vzniklou slouèením jednotky s vstupní bránou a~zapomínací bránou. Rovnice dopøedného šíøení má tvar
\begin{equation}
\boldsymbol{h}_{i}^{(t)} = \boldsymbol{u}_{i}^{(t-1)} \boldsymbol{h}_{i}^{(t-1)} + (1 - \boldsymbol{u}_{i}^{(t-1)}) \sigma \left( \boldsymbol{b}_{i} + \sum_{j} \boldsymbol{U}_{i,j} \boldsymbol{x}_{j}^{(t-1)} + \sum_{j} \boldsymbol{W}_{i,j} r_{j}^{(t-1)} \boldsymbol{h}_{j}^{(t-1)}  \right) ,
\end{equation}
kde jednotka $ \boldsymbol{u} $ modifikuje stav buòky
\begin{equation}
\boldsymbol{u}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{u} + \sum_{j} \boldsymbol{U}_{i,j}^{u} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{u} \boldsymbol{h}_{j}^{(t)} \right)
\end{equation}
a jednotka $ \boldsymbol{r} $ øídí, které informace stavu budou vyuity pro vıpoèet pøíštího cílového stavu
\begin{equation}
\boldsymbol{r}_{i}^{(t)} = \sigma \left( \boldsymbol{b}_{i}^{r} + \sum_{j} \boldsymbol{U}_{i,j}^{r} \boldsymbol{x}_{j}^{(t)} + \sum_{j} \boldsymbol{W}_{i,j}^{r} \boldsymbol{h}_{j}^{(t)} \right ).
\end{equation}
Takto definované jednotky s branami umoòují vhodnì vybírat èásti stavového prostoru a uchovávat je ve stavu buòky. Zároveò slouèením vıstupu a stavu buòky dochází k~vıznamnému sníení pamìové a vıpoèetní nároènosti \cite{dl, colahLSTM}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{gru.png}
    \caption{Schéma GRU buòky. Pøevzato z \cite{colahLSTM}.}
    \label{fig:gru}
\end{figure}

\newpage
\section{Optimalizace parametrù neuronovıch sítí}
Vìtšina uèících se algoritmù vyuívá jistou formu optimalizace. Optimalizací rozumíme úlohu, kdy minimalizujeme èi maximalizujeme pøedem danou funkci $ f(\boldsymbol{x}) $ zmìnou parametru $ \boldsymbol{x} $. Hledáme tedy hodnotu $ \boldsymbol{x} $ takovou, aby funkce funkce $ f(\boldsymbol{x}) $ nabıvala minimální èi maximální hodnoty. Vìtšina optimalizaèních metod uvauje minimalizace funkce $ f(\boldsymbol{x}) $ a její pøípadná maximalizace se provádí minimalizací funkce $ -f(\boldsymbol{x}) $.

Funkce, kterou optimalizujeme, nazıváme kritérium (v terminologii strojového uèení se také èasto objevují názvy cenová, ztrátová èi chybová funkce). Tato práce se zabıvá pøedevším optimalizací pro neuronové sítì, kde jsou nejèastìji vyuívány optimalizaèní metody zaloené na gradientu.

Základní metodou zaloenou na gradientu je tzv. gradientní sestup. Pøedpokládejme cenovou funkci $ J(\boldsymbol{\theta}) $ parametrizovanou souborem parametrù $ \boldsymbol{\theta} $. Gradientní sestup hledá optimální soubor parametrù $ \boldsymbol{\theta}^{*} = \text{argmin } J(\boldsymbol{\theta}) $ pomocí iterativního pravidla pro zmìnu parametrù
\begin{equation}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}),
\end{equation}
kde $ \epsilon $ je tzv. konstanta uèení, která udává velikost kroku v opaèném smìru nejvìtšího gradientu. Gradientní sestup pro konvexní cenové funkce vdy nalezne globální minimum a pro nekonvexní cenové funkce lokální minimum èi sedlovı bod \cite{dl,karpathy,ruder}.

\subsection{Stochastickı gradientní sestup}
Aèkoliv je gradientní sestup efektivní optimalizaèní metoda, pøi optimalizaci nad velkım objemem dat mùe bıt velice pomalá a vıpoèetnì nároèná, jeliko pro jednu zmìnu parametrù je tøeba spoèítat gradient nad celou datovou sadou. Jednou z nejpouívanìjších modifikací gradientního sestupu, která tyto problémy øeší, je stochastickı gradientní sestup.

Pøedpokládejme cenovou funkci ve tvaru záporného logaritmu podmínìné pravdìpodobnosti
\begin{equation}
J(\boldsymbol{\theta}) =  \mathbb{E}_{\boldsymbol{x},  \boldsymbol{y} \sim p_{data}} L(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{\theta}) = \dfrac{1}{m} \sum_{i=1}^{m} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}),
\end{equation}
kde $ p_{data} $ je mnoina trénovacích dat o velikost $ m $ a $ L(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{\theta}) = -\log p(\boldsymbol{y} \mid \boldsymbol{x}; \boldsymbol{\theta}) $ je cena pro jedno pozorování v závislosti na souboru parametrù $ \boldsymbol{\theta} $. Pro takto definovanou cenovou funkci by gradientní sestup musel spoèítat gradient
\begin{equation}
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \dfrac{1}{m} \sum_{i=1}^{m} \nabla_{\boldsymbol{\theta}} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}),
\end{equation}
jeho komplexita je $ O(m) $. Stochastickı gradientní sestup nahlíí na gradient jako na støední hodnotu a tudí se pøedpokládá, e mùe bıt aproximován menšími soubory pozorování náhodnì vybíranımi z datové sady, tzv. dávky. V optimalizaèním kroku je náhodnì vybrána dávka pozorování $ \boldsymbol{b} = \{ x^{1}, \ldots, x^{m'} \} $, kde $ m' $ se volí jako malé èíslo v~závislosti na vıpoèetní kapacitì a velikosti $ m $ úplné datové sady. Pøi trénování modelu na grafické kartì (GPU) je vhodné volit velikost dávky jako mocninu dvou, a to v rozmezí 8 a 256, aby mohly bıt plnì vyuity vektorizované operace GPU. Menší dávky mohou mít zároveò regularizaèní efekt díky šumu, kterı vnášejí do uèícího se procesu. Odhad gradientu je pak vypoèten s vyuitím dávky $ \boldsymbol{b} $
\begin{equation}
g = \dfrac{1}{m'} \nabla_{\boldsymbol{\theta}} \sum_{i=1}^{m'} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}).
\end{equation}
Zmìna parametrù je dána dle pravidla
\begin{equation}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon g.
\end{equation}
Pro pevnì danou velikost modelu tedy vıpoèetní cena nezávisí na velikosti datové sady $ m $  \cite{dl,ruder}.

\subsection{Další modifikace gradientního sestupu}
Jedním z nejvìtších problémù pøi vyuití gradientního sestupu a gradientního stochastického sestupu je vıbìr konstanty uèení $ \epsilon $. Vysoká hodnota $ \epsilon $ mùe zpùsobit fluktuaci okolo lokálního minima nebo dokonce divergenci optimalizaèního procesu a nízká hodnota $ \epsilon $ mùe mít za následek vırazné zpomalení uèení. Konstanta uèení se v~praxi dynamicky mìní v závislosti na poètu epoch $ k $, tedy $ \epsilon_{k} $. Dalším bìnım problémem je uvíznutí v~mìlkém lokálním minimu èi sedlovém bodì, co znemoní další uèení. Bylo vytvoøeno nìkolik modifikací základních gradientních algoritmù, které tyto problémy do jisté míry øeší \cite{dl, ruder}.

\subsubsection{Moment}
Moment byl navren za úèelem zrychlení trénování a to pøedevším pøi optimalizaci ztrátovıch funkcí, které
mají mnoho mìlkıch lokálních minim nebo v pøípadech, kdy jsou gradienty znaènì zašumìné. Algoritmus momentu vyuívá plovoucí prùmìr minulıch gradientù s exponenciálním zapomínáním a pokraèuje v jejich smìru.

Tento algoritmus zavádí parametr $ \alpha $, kterı udává, jakou rychlostí mají bıt minulé gradienty zapomínány. Pravidlo pro zmìnu parametrù pak vypadá následovnì
\begin{align}
\boldsymbol{v} &\leftarrow \alpha \boldsymbol{v} - \epsilon \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) \right), \\
\boldsymbol{\theta} &\leftarrow \boldsymbol{\theta} + \boldsymbol{v}.
\end{align}
Promìnná $ \boldsymbol{v} $ akumuluje prvky gradientu $ \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) \right) $ s tím, e èím vìtší je parametr $ \alpha $ vùèi konstantì uèení $ \epsilon $, tím více minulé gradienty ovlivòují aktuální smìr kroku. Stejnì jako u konstanty uèení, i parametr $ \alpha $ mùe bıt mìnìn v závislosti na èase. Vìtšinou je jako poèáteèní hodnota zvolena 0.5 a je navyšována a na 0.99 \cite{dl, karpathy, ruder}.

\subsubsection{Nesterovùv moment}
Dalším variantou je Nesterovùv moment, kterı dále rozšiøuje algoritmus momentu. Pravidlo pro zmìnu parametrù se zmìní na
\begin{align}
\boldsymbol{v} &\leftarrow \alpha \boldsymbol{v} - \epsilon \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta} + \alpha \boldsymbol{v}), \boldsymbol{y}^{(i)}) \right), \\
\boldsymbol{\theta} &\leftarrow \boldsymbol{\theta} + \boldsymbol{v},
\end{align}
kde parametry $ \alpha $ a $ \epsilon $ odpovídají stejnım parametrùm jako u algoritmu momentu. Hlavním rozdílem oproti algoritmu momentu je, e gradient je vyhodnocen a po aplikaci minulıch gradientù. Nejprve je proveden krok ve smìru akumulovanıch minulıch gradientù a~následnì je provedena korekce \cite{dl, karpathy, ruder, nesterov}.

\subsection{ADAM}
Cenová funkce bıvá èasto citlivá na urèité smìry v prostoru parametrù a naopak necitlivá na smìry jiné. Moment tento problém do urèité míry øeší, ale za cenu zavedení nového parametru, kterı je tøeba správnì nastavit. ADAM je adaptivní metoda, která tento problém øeší tím, e zavede vlastní konstantu uèení pro kadı parametr, a tyto konstanty pak automaticky v prùbìhu uèení adaptuje.

ADAM pouívá ke zmìnì parametrù plovoucí prùmìr (první centrální moment) gradientù s exponenciálním zapomínáním $ \boldsymbol{r} $ a druhı necentrální moment $ \boldsymbol{v} $. Poèáteèní hodnota obou momentù je nastavena na nulu a zejména v prvních krocích algoritmu je potøeba momenty opravit korekèním faktorem. Parametry $ \beta_{1} $ a $ \beta_{2} $ udávají rychlost zapomínání a~$ \delta $ je malá konstanta  \cite{dl, karpathy, ruder, adam}.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
inicializace \\
$ \boldsymbol{r} = 0 $, $ \boldsymbol{v} = 0 $, $ \boldsymbol{t} = 0 $  \\
\While{zastavovací podmínka není splnìna}{
 $ t \leftarrow t + 1 $  \\
 vıpoèet gradientu \\
$ g = \dfrac{1}{m} \nabla_{\boldsymbol{\theta}} \sum_{i} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) $ \\ 
 zmìna odhadu prvního centrálního momentu \\
 $ \boldsymbol{r} \leftarrow \beta_{1}\boldsymbol{r} + (1 - \beta_{1})g $ \\
 zmìna odhadu druhého necentrálního momentu \\
 $ \boldsymbol{v} \leftarrow \beta_{2}\boldsymbol{v} + (1 - \beta_{2})g \odot g $ \\
 korekce odhadu prvního centrálního momentu \\
 $ \hat{\boldsymbol{r}} \leftarrow \dfrac{\boldsymbol{r}}{1- \beta_{1}^{t}} $ \\
 korekce odhadu druhého necentrálního momentu \\
 $ \hat{\boldsymbol{v}} \leftarrow \dfrac{\boldsymbol{v}}{1- \beta_{2}^{t}} $ \\
 zmìna parametrù \\
 $ \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + -\epsilon \dfrac{\hat{\boldsymbol{r}}}{\sqrt{\hat{\boldsymbol{v}} + \delta}} $
}
\end{algorithm}
\begin{center}
Algoritmus 4: ADAM.
\end{center}

\newpage
\section{Kapacita modelu}
Základní vlastností kadého modelu by mìla bıt generalizace. To znamená, e model musí bıt schopnı správnì klasifikovat nejen pozorování, na kterıch byl natrénován, ale i~vìtšinu novıch, døíve nevidìnıch pozorování. Z tohoto dùvodu se pøi trénování modelu datová sada  bìnì dìlí na tøi - trénovací, validaèní a testovací, kdy pøedpokládáme, e tyto sady podléhají stejnému rozloení a jednotlivá pozorování jsou nezávislá. Cílem trénování modelu je nalézt takové parametry, pro které bude mít model podobnou chybu na všech tøech datovıch sadách. Model by mìl splòovat tyto vlastnosti:
\begin{enumerate}
\item musí minimalizovat chybu na trénovací sadì,
\item musí minimalizovat rozdíl mezi chybou nad trénovací a testovací (popø. validaèní) sadou.
\end{enumerate}
Bìhem optimalizace parametrù èasto dochází ke dvìma neádoucím jevùm:
\begin{itemize}
\item \textbf{underfitting} (podtrénování) - k underfittingu dochází, pokud model není schopnı dostateènì minimalizovat chybu na trénovací sadì, tj. není schopnı se nauèit informaèní souvislosti obsaené v trénovacích pozorováních;
\item  \textbf{overfitting} (pøetrénování) - k overfittingu dochází, pokud model není schopnı minimalizovat rozdíl mezi chybou nad trénovací a testovací sadou, tj. model si "zapamatuje" \ trénovací data místo toho, aby se nauèil souvislosti obsaené v trénovacích pozorováních a na novıch, dosud nevidìnıch pozorováních selhává \cite{dl, karpathy, hastie}.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{overfitting.eps}
    \caption{Pøíklad overfittingu a underfittingu.}
    \label{fig:overfitting}
\end{figure}


Oba tyto jevy lze ovlivnit pomocí tzv. kapacity modelu nebo pomocí regularizaèních metod. Modely s nízkou kapacitou jsou náchylné na underfitting a naopak modely s~vysokou kapacitou jsou náchylné na overfitting. Kapacita modelu je vìtšinou dána pøímo zvolenou architekturou, kde sníením poètu parametrù sítì omezíme poèet volnıch stupòù volnosti a tím lze získat vyšší generalizaci \cite{dl, karpathy}.

\subsection{Regularizace}
Regularizace umoòuje navıšit generalizaci modelu bez toho, aby byla ovlivnìna kapacita modelu - nevyaduje zmìnu architektury modelu. Jedná se o modifikaci uèícího se algoritmu, která sniuje generalizaèní chybu, ale zároveò nesniuje chybu trénovací \cite{dl}.

\subsubsection{Penalizace velikosti parametrù}
Populární metodou pro regularizaci jsou penalizaèní metody, a to zejména L1 a L2 penalizace. Tyto penalizaèní metody nepøímo omezují kapacitu modelu pøidáním penalizace $ \Omega(\boldsymbol{\theta}) $, která odpovídá normì parametrù, k cenové funkci $ J $. Regularizovaná cenová funkce pak vypadá následovnì
\begin{equation}
\tilde{J}(\boldsymbol{\theta} ; \boldsymbol{x}, \boldsymbol{y}) = J(\boldsymbol{\theta} ; \boldsymbol{x}, \boldsymbol{y}) + \alpha \Omega(\boldsymbol{\theta}),
\end{equation}
kde síla regularizace je øízena parametrem $ \alpha $ (s vyšší hodnotou regularizace roste). Trénovací algoritmus pak minimalizuje jak pùvodní cenovou funkci $ J $, tak i zvolenou metriku velikosti parametrù $  \Omega(\boldsymbol{\theta}) $. V pøípadì neuronovıch sítí jsou regularizovány pouze váhy a prahy jsou ponechány bez regularizace  \cite{dl, karpathy}.

\subsubsection{Zanesení šumu}
Další metodou regularizace, která neovlivòuje model samotnı, je augmentace trénovacích dat ve formì aditivního šumu. Aèkoliv neuronové sítì nejsou obecnì robustní vùèi šumu, vìtšinu klasifikaèních i regresních úloh jsou schopny øešit i pokud jsou vstupní data zatíena malım náhodnım šumem. Trénovací pozorování jsou tedy vdy pøed vstupem do modelu zatíena novım náhodnım šumem, kterı vede k vyšší generalizaci vısledného modelu \cite{dl}.

\subsubsection{Pøedèasné ukonèení}
Pøi trénování velkıch modelù s dostateènì velkou reprezentaèní kapacitou èasto dochází k jevu, kdy chyba na trénovací sadì pomalu bìhem èasu klesá, zatímco chyba na validaèní sadì pomalu roste. Vrátíme-li se k nastavení parametrù v èase s nejniší validaèní chybou, mùeme získat lepší model s niší chybou i na testovací sadì.

Algoritmus pøedèasného ukonèení (anglicky early stopping) je snadno implementovatelnı a~vıpoèetnì nenároènı. Pokadé, kdy dojde k poklesu chyby na validaèní sadì, uloíme si kopii parametrù modelu. Jakmile trénování modelu skonèí, vrátíme se k nejlepšímu souboru parametrù místo ponechání souboru parametrù z poslední trénovací iterace. Trénování je pøedèasnì ukonèeno, pokud ádná zmìna parametrù nevede k niší chybì na validaèní sadì, ne které bylo dosaeno s aktuálnì nejlepším souborem parametrù, pro pøedem danı poèet trénovacích iterací.

Tato forma regularizace nevyaduje témìø ádné zmìny trénovacího procesu èi modifikaci cenové funkce. Tento algoritmus lze vyuít buï samostatnì nebo spolu s libovolnou regularizaèní metodou \cite{dl}.

\subsubsection{Dropout}
Dropout je vıpoèetnì nenároènou metodou regularizace, která nejlépe funguje pro modely, které vyuívají dávkové uèení s malımi kroky. Pokadé, kdy pozorování vstoupí do dávky, je vytvoøena binární maska, která je pak aplikována na všechny vstupy skrytıch jednotek v síti. Tato maska je pro kadou jednotku vytvoøena náhodnì nezávisle na ostatních. Jednotky, jejich odpovídající prvek v masce je roven nule, jsou pak pro dané pozorování vylouèeny z uèícího se procesu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{dropout.png}
    \caption{Vlevo - pøed aplikací dropoutu, vpravo - po aplikaci dropoutu. Pøevzato z \cite{dropout}.}
    \label{fig:overfitting}
\end{figure}

Pravdìpodobnost, e v masce na danou pozici bude vybrána hodnota 1, je dána pøedem zvolenım parametrem. Vìtšinou se volí pravdìpodobnost 0.8, e bude pøi trénování vyuita vstupní jednotka, a pravdìpodobnost 0.5, e bude vyuita skrytá jednotka.

Maskováním se do uèícího procesu vnáší jistá forma šumu, která má za následek adaptivní poškození informaèního obsahu na vstupu sítì a na vstupu skrytıch jednotek. Tím je zamezeno tomu, aby se urèitá jednotka zamìøila na rozpoznávání jednoho signifikantního pøíznaku, zatímco jiná jednotka by byla témìø nevyuita. Vylouèením náhodnıch jednotek z uèícího se procesu jsou tak jednotky donuceny "rozdìlit" \ si informace mezi sebou.

Dropout opìt mùe bıt vyuit spolu s ostatními metodami regularizace \cite{dl, karpathy, dropout}.

\clearpage
\section{CTC}
Tato práce se vìnuje úloze klasifikace fonémù, jejím cílem je pro danou zvukovou nahrávku získat odpovídající sekvenci fonému. Zvuková nahrávka je rozdìlena na malé segmenty a tìmto segmentùm jsou pak pøiøazeny fonémy, které se v jednotlivıch segmentech vyskytují (více v \cite{bp}). K tomuto oznaèkování segmentù se vìtšinou vyuívají pøedtrénované modely, které ovšem vyadují jisté pøedpoklady, mohou bıt náchylné vùèi šumu a vısledné oznaèkování mùe bıt nepøesné, jeliko ne vdy lze pøesnì urèit hranici mezi jednotlivımi fonémy.

Bìnì vyuívané cenové funkce jsou ovšem definovány pro jednotlivé prvky trénovací sekvence (segmenty) a umoòují pouze klasifikaci nezávislıch fonémù. To znamená, e takto definované cenové funkce je moné vyuít pouze pro nezávislou predikci fonémù v~jednotlivıch segmentech, kdy vıstupní sekvence má stejnou délku jako vstupní sekvence. K získání vısledné sekvence fonémù je tøeba vıstupy sítì dále zpracovat pomocí dekodéru (napø. pro vstupní sekvenci o dvanácti segmentech: $ aahahhoohooj \rightarrow  ahoj $).

V této kapitole si uvedeme metodu CTC (z anglického "connectionist temporal classification"), díky které lze vyuít rekurentní neuronové sítì pøímo k predikci vısledné sekvence fonémù bez potøeby dalšího zpracování vıstupu sítì \cite{bp, distill, ctc}.


\subsection{Formální definice úlohy}
Mìjme trénovací mnoinu $ S $ z pevnì daného rozloení $ \mathcal{D}_{\mathcal{X} \times \mathcal{Z}} $. Prostor vstupních hodnot $ \mathcal{X} = (\mathcal{R})^{*} $ je mnoina všech sekvencí tvoøenıch $ m $ dimenzionálními reálnımi vektory. Prostor vıstupních hodnot $ \mathcal{Z} = L^{*} $ je mnoina všech sekvencí nad koneènou abecedou $ L $ znaèek (fonémù). Prvky $ L^{*} $ budeme nazıvat sekvencí znaèek (sekvence fonému) èi znaèkováním. Kadé pozorování v $ S $ je tvoøeno párem $ (\boldsymbol{x}, \boldsymbol{z}) $, kde cílová sekvence $ \boldsymbol{z} = (z_{1}, z_{2}, \ldots , z_{U}) $ je nejvıše tak dlouhá, jako vstupní sekvence $ \boldsymbol{x} = (x_{1}, x_{2}, \ldots , x_{T}) $, tedy $ U \leq T $. Vzhledem k tomu, e vstupní a cílové sekvence nejsou obecnì stejnì dlouhé, není moné je pøedem zarovnat, tj. pøiøadit prvky cílové sekvence k vstupním prvkùm sekvence.

Cílem je natrénovat klasifikátor $ h: \mathcal{X} \mapsto \mathcal{Z} $ za vyuití $ S $, kterı minimalizuje vhodnì zvolenou metriku pro danou úlohu (napø. minimalizace poètu transkripèních chyb) \cite{distill, ctc}.

\subsection{Reprezentace vıstupu}
Aby bylo moné vyuít RNN spolu s CTC, je tøeba vhodnì zvolit reprezentaci vıstupu sítì. Vyuijeme-li aktivaèní funkci softmax pro vıstupní jednotky, mùeme transformovat vıstupy sítì do tvaru podmínìné hustoty pravdìpodobnosti nad danou abecedou znaèek a vyuít sí jako klasifikátor, kdy vstupní sekvence klasifikujeme vıbìrem nejvíce pravdìpodobného znaèkování. Vıstupní vrstva zároveò musí obsahovat
o jednu jednotku více, ne je poèet znaèek v $ L $. Aktivace prvních $ |L| $ jednotek je interpretována jako pravdìpodobnost pozorování odpovídajících znaèek a aktivace $ |L| + 1 $ jednotky je interpretována jako pravdìpodobnost pozorování prázdné znaèky. Prázdná znaèka umoòuje zobrazit více prvkù vstupní sekvence na jednu znaèku èi ignorovat prvky sekvence, které odpovídají tichu, a z vısledné sekvence je odstranìna. Celkovou pravdìpodobnost libovolného znaèkování pak mùeme vyèíslit seètením pravdìpodobností jeho rùznıch zarovnání.

Mìjme RNN s $ m $ vstupními jednotkami, $ n $ vıstupními jednotkami a váhovım vektorem $ \boldsymbol{w} $, která slouí jako zobrazení $ \mathcal{N}_{\boldsymbol{w}}: (\mathbb{R}^{m})^{T} \mapsto (\mathbb{R}^{n})^{T} $. Dále mìjme vıstupní sekvenci sítì $ \boldsymbol{y} = \mathcal{N}_{\boldsymbol{w}}(\boldsymbol{x}) $, její prvky $ \boldsymbol{y}_{k}^{(t)} $ odpovídají aktivaci vıstupní jednotky $ k $ v~èase $ t $ a lze je interpretovat jako pravdìpodobnost pozorování znaèky $ k $ v~èase $ t $. Hustota pravdìpodobnosti znaèkování nad mnoinou $ L^{'T} $ sekvencí délky $ T $ nad abecedou $ L^{'} = L \cup \{ \textit{prázdná znaèka} \} $,  je pak
\begin{equation}
p(\boldsymbol{\pi} \mid \boldsymbol{x}) = \prod _{t=1}^{T} \boldsymbol{y}_{\boldsymbol{\pi}_{t}}^{(t)}, \forall \boldsymbol{\pi} \in L^{'T},
\end{equation}
kde $ \boldsymbol{\pi} $ budeme nazıvat cestami a odpovídají prvkùm $ L^{'T} $.

Dále definujme zobrazení $ \mathcal{B}: L^{'T} \mapsto L^{\leq T} $, kde $ L^{\leq T} $ je mnoina všech monıch znaèkování, tj. mnoina vıstupních sekvencí o délce menší èi rovné $ T $ nad pùvodní abecedou znaèek $ L $. Toto zobrazení získáme odstranìním opakujících se znaèek a prázdnıch znaèek ze získanıch cest (napø. $ \mathcal{B}(aa-ab-) = \mathcal{B}(a-a-bb) = aab $) a vyuijeme ho k~získání podmínìné pravdìpodobnosti daného znaèkování $ \boldsymbol{l} \in L^{\leq T} $ \cite{distill, ctc}
\begin{equation}
p(\boldsymbol{l} \mid \boldsymbol{x}) = \sum_{\boldsymbol{\pi} \in \mathcal{B}^{-1}(\boldsymbol{l})} p(\boldsymbol{\pi} \mid \boldsymbol{x}),
\end{equation}

\subsection{Dekódování}
Vıstupem klasifikátoru by mìlo bıt nejvíce pravdìpodobné znaèkování pro vstupní sekvenci
\begin{equation}
h(\boldsymbol{x}) = \underset{\boldsymbol{l} \in L^{\leq T}}{\mathrm{argmax}}\text{ } p(\boldsymbol{l} \mid \boldsymbol{x} ).
\end{equation}
Proces hledání znaèkování nazıváme dekódováním a pro metodu CTC lze vyuít dvì aproximativní metody.

První metoda, dekódování nejlepší cesty, je zaloena na pøedpokladu, e cesta s nejvyšší pravdìpodobností odpovídá nejvíce pravdìpodobnému znaèkování
\begin{align}
h(\boldsymbol{x}) &\approx \mathcal{B}(\boldsymbol{\pi}^{*}) , \\
\boldsymbol{\pi}^{*} &= \underset{\boldsymbol{\pi} \in N^{t}}{\mathrm{argmax}}\ \text{ } p(\boldsymbol{\pi} \mid \boldsymbol{x}).
\end{align}
Dekódování nejlepší cesty lze snadno najít zøetìzením vıstupu s nejvyšší aktivaèní hodnotou v kadém èasovém kroku. Není ovšem zaruèeno, e tato metoda nalezne nejlepší moné znaèkování.

Druhá metoda vyuívá sluèování cest, které procházejí stejnou sekvencí znaèek, a~zaruèuje nalezení nejlepšího znaèkování. Je však vıpoèetnì nároèná a aby tato metoda byla upoèitatelná, je tøeba vyuít další heuristiky (napø. BEAM proøezávání, které umoní kompromis mezi rychlostí vıpoètu a pøesností).

Obì tyto metody mohou bıt zároveò obohaceny o jazykovı model
\begin{equation}
h(\boldsymbol{x}) = \underset{\boldsymbol{l} \in L^{\leq T}}{\mathrm{argmax}}\text{ } p(\boldsymbol{l} \mid \boldsymbol{x} ) \cdot p(\boldsymbol{l})^{\gamma} \cdot g(\boldsymbol{l})^{\zeta},
\end{equation}
kde $ p(\boldsymbol{l}) $ je pravdìpodobnost znaèkování daná jazykovım modelem, $ g(\boldsymbol{l}) $ je penalizace za vloení slova a parametry $ \gamma $ a $ \zeta $ jsou pøedem dané váhy \cite{distill, ctc}.

\subsection{Dopøednı a zpìtnı algoritmus}
Pro získání vısledného znaèkování je tøeba spoèítat podmínìnou pravdìpodobnost $ p(\boldsymbol{l} \mid \boldsymbol{x}) $ jednotlivıch znaèkování. To mùe bıt znaènì problematické, jeliko je potøeba spoèítat sumu pøes všechny cesty odpovídající danému znaèkování. Vyuijeme k tomu algoritmus dynamického programování, kterı vychází z pøedpokladu, e suma  všech cest odpovídajících znaèkování mùe bıt rozloena na iterativní sumu pøes cesty, které v daném èasovém kroku dosáhly stejné znaèky. Tyto iterace pak mohou bıt snadno spoèteny pomocí  rekurzivních dopøednıch a zpìtnıch promìnnıch.

Mìjme sekvenci $ \boldsymbol{q} $ délky $ r $, kde $ \boldsymbol{q}_{1:p} $ a $ \boldsymbol{q}_{r-p:r} $ znaèí prvních a posledních $ p $ znaèek. Potom pro znaèkování $ \boldsymbol{l} $ zavedeme dopøednou promìnnou $ \alpha_{t}(s) $, která odpovídá celkové pravdìpodobnosti $ \boldsymbol{l}_{1:s} $ v èase $ t $
\begin{equation}
\alpha_{t}(s) = \sum_{\substack{\boldsymbol{\pi} \in N^{T}: \\ \mathcal{B}(\boldsymbol{\pi}_{1:t}) = \boldsymbol{l}_{1:s}}} \prod_{t^{'}=1}^{t} \boldsymbol{y}_{\boldsymbol{\pi}_{t^{'}}}^{(t)^{'}} ,
\end{equation}
kde $ \alpha_{t}(s) $ mùe bıt rekurzivnì spoètena z $ \alpha_{t-1}(s) $ a $ \alpha_{t-1}(s-1) $. Aby bylo moné vyuít prázdné znaèky ve vıstupní cestì, je tøeba upravit znaèkování $ \boldsymbol{l} $ na $ \boldsymbol{l}^{'}  $ vloením prázdné znaèky na zaèátek a konec sekvence a mezi kadé dvì znaèky. Délka $ \boldsymbol{l}^{'}  $ je $ 2 |\boldsymbol{l}| + 1  $. Pøi vıpoètu pravdìpodobnosti znaèkování $ \boldsymbol{l}^{'}  $ umoníme pøechody mezi prázdnımi a neprázdnımi znaèkami a také mezi kadım párem unikátních neprázdnıch znaèek. Zároveò musí všechny sekvence zaèínat buï prázdnou znaèkou $ b $, nebo prvním znaèkou v~$ l $. Tìmito omezeními jsou definovány inicializaèní hodnoty algoritmu
\begin{align}
\alpha_{1}(1) &= \boldsymbol{y}_{b}^{(1)} , \\
\alpha_{1}(2) &= \boldsymbol{y}_{\boldsymbol{l}_{1}}^{(1)} , \\
\alpha_{1}(s) &= 0, \forall s > 2
\end{align}
a rekurze
\begin{equation}
\alpha_{t}(s) = 
\begin{cases}
\bar{\alpha}_{t}(s) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{pokud } \boldsymbol{l}_{s}^{'} = b \text{ nebo } \boldsymbol{l}_{s-2}^{'} = \boldsymbol{l}_{s}^{'} \\
(\bar{\alpha}_{t}(s) + \alpha_{t-1}(s-2)) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{jinak}
\end{cases} ,
\end{equation}
kde
\begin{equation}
\bar{\alpha}_{t}(s) = \alpha_{t-1}(s) + \alpha_{t-1}(s-1) .
\end{equation}
Pravdìpodobnost $ \boldsymbol{l} $ je pak dána sumou vıslednıch pravdìpodobností $ \boldsymbol{l}^{'} $ s prázdnou znaèkou a bez prázdné znaèky v èase $ T $
\begin{equation}
p(\boldsymbol{l} \mid \boldsymbol{x}) = \alpha_{T}(|\boldsymbol{l}^{'}|) + \alpha_{T}(|\boldsymbol{l}^{'}| - 1) . 
\end{equation}

Obdobnì zadefinujeme zpìtnou promìnnou $ \beta_{t}(s) $ jako celkovou pravdìpodobnost $ \boldsymbol{l}_{s:|\boldsymbol{l}|} $ v èase $ t $
\begin{align}
&\beta_{t}(s) = \sum_{\substack{ \boldsymbol{\pi} \in N^{T}: \\ \mathcal{B}(\boldsymbol{\pi}_{t:T})=\boldsymbol{l}_{s:|\boldsymbol{l}|}}} \prod_{t^{'}=t}^{T} \boldsymbol{y}_{\boldsymbol{\pi}_{t^{'}}}^{(t)^{'}} , \\
&\beta_{T}(|\boldsymbol{l}^{'}|) = \boldsymbol{y}_{b}^{(T)} , \\
&\beta_{T}(|\boldsymbol{l}^{'}| - 1) = \boldsymbol{y}_{\boldsymbol{l}_{\boldsymbol{l}}}^{(T)} , \\
&\beta_{T}(s) = 0, \forall s < |\boldsymbol{l}^{'}| - 1 , \\
&\beta_{t}(s) =
\begin{cases}
\bar{\beta}_{t}(s) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{pokud } \boldsymbol{l}_{s}^{'} = b \text{ nebo } \boldsymbol{l}_{s+2}^{'} = \boldsymbol{l}_{s}^{'} \\
(\bar{\beta}_{t}(s) + \beta_{t+1}(s+2)) \boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} & \text{jinak}
\end{cases} , \\
&\bar{\beta}_{t}(s) = \beta_{t+1}(s) + \beta_{t+1}(s+1) .
\end{align}

Aby byla pøi vıpoètech zajištìna numerická stabilita, je tøeba dopøedné a zpìtné promìnné pøeškálovat
\begin{align}
\hat{\alpha}_{t}(s) &= \dfrac{\alpha_{t}(s)}{\sum_{s} \alpha_{t}(s)} , \\
\hat{\beta}_{t}(s) &= \dfrac{\beta_{t}(s)}{\sum_{s} \beta_{t}(s)} .
\end{align}
Dosazením pøeškálovanıch promìnnıch do vzorcù a jejich úpravou (podrobnìji v \cite{ctc}) mùeme sí natrénovat pomocí metod zaloenıch na maximální vìrohodnosti. Zpìtné šíøení gradientu skrz vıstupní vrstvu s aktivaèní funkcí softmax je definováno následovnì
\begin{equation}
\dfrac{\partial O^{ML} (\{ \boldsymbol{x}, \boldsymbol{z} \},\mathcal{N}_{\boldsymbol{w}})}{\partial \boldsymbol{u}_{k}^{(t)}} = \boldsymbol{y}_{k}^{(t)} - \dfrac{1}{\boldsymbol{y}_{k}^{(t)} Z_{(t)}} \sum_{s \in lab(\boldsymbol{z}, k)} \hat{\alpha}_{t}(s) \hat{\beta}_{t}(s) ,
\end{equation}
kde $ \boldsymbol{u}_{k}^{(t)} $ jsou nenormalizované pravdìpodobnosti na vıstupu sítì, $ lab(\boldsymbol{z}, k) = lab(\boldsymbol{l}, k) = \{ s : \boldsymbol{l}_{s}^{'} = k \} $ je mnoina pozic, na kterıch se ve znaèkování $ \boldsymbol{l} $ vyskytuje znaèka $ k $, a~kde \cite{distill, ctc}
\begin{equation}
Z_{t} = \sum_{s=1}^{|\boldsymbol{l}^{'}|} \dfrac{\hat{\alpha}_{t}(s) \hat{\beta}_{t}(s)}{\boldsymbol{y}_{\boldsymbol{l}_{s}^{'}}^{(t)} } .
\end{equation}

% PRAKTICKÁ ÈÁST
\newpage
\section{Klasifikace fonémù}
V této práci bylo porovnáno šest architektur neuronovıch sítí pro úlohu klasifikace fonémù. Experimenty byly provedeny nad dvìma datovımi sady pro ètyøi rùzné parametrizace pøíznakù. K implementaci byl vyuit programovací jazyk Python 3.4 spolu s~knihovnami pro neuronové sítì Tensorflow a Keras (kód dostupnı na \cite{repo}). Sítì byly trénovány na grafické kartì Nvidia Tesla K20m s pamìtí 5GB a knihovnami CUDA 8.0 a~CuDNN 6.0.

\subsection{Pøedzpracování dat}
Architektury byly testovány na dvou datovıch sadách, ŠkodaAuto a SpeechDat-E, které obsahují èeské nahrávky se vzorkovací frekvencí 8 kHz. Obì tyto sady byly dále rozdìleny na sadu trénovací (75\%), validaèní (12.5\%) a testovací (12.5\%) tak, aby kadı øeèník byl v právì jedné sadì. Základní informace o datovıch sadách jsou uvedeny v~tabulce \ref{tab:datasety}.

\begin{table}[H]
\centering
\begin{tabular}{ |c|c|c| }
\cline{2-3}
\multicolumn{1}{c|}{} & ŠkodaAuto & SpeechDat-E \\
\hline
poèet øeèníkù & 47 & 924  \\
\hline
poèet nahrávek & 14523 & 39560 \\
\hline
poèet øeèníkù v trénovací sadì & 35 & 693 \\
\hline
poèet øeèníkù ve validaèní sadì & 6 &  115 \\
\hline
poèet øeèníkù v testovací sadì & 6 & 116  \\
\hline
\end{tabular}
\caption{Popis datovıch sad a jejich rozdìlení.}
\label{tab:datasety}
\end{table}

Pro obì datové sady pak byly spoèteny ètyøi parametrizace pøíznakù \cite{bp}, na které se dále budeme odkazovat tuènì uvedenımi zkratkami:
\begin{itemize}
\item \textbf{LFE} - logaritmované energie banky filtrù, pro vıpoèet vyuito 40 filtrù,
\item \textbf{LFE} $ \Delta\Delta $ - logaritmované energie banky filtrù, pro vıpoèet vyuito 40 filtrù, vypoèteny delta a delta-delta koeficienty, 
\item \textbf{MFCC} - mel-frekvenèní kepstrální koeficienty, pro vıpoèet vyuito 26 filtrù, spoèteno 13 koeficientù,
\item \textbf{MFCC} $ \Delta\Delta $ - mel-frekvenèní kepstrální koeficienty, pro vıpoèet vyuito 26 filtrù, spoèteno 13 koeficientù, vypoèteny delta a delta-delta koeficienty.
\end{itemize}
Všechny parametrizace byly vygenerovány s vyuitím Hammingova okénka o délce 25ms s posunem 10ms (jednotlivé segmenty se pøekrıvají) a 512 bodové FFT.

Takto spoètené pøíznaky pak byly po sloupcích normalizovány tak, aby mìly nulovou støední hodnotu a jednotkovı rozptyl. K tomu byla vyuita Z-score normalizace, která pøetransformuje vstupní data $ \boldsymbol{x} $ na data $ \boldsymbol{z} $ dle vzorce
\begin{equation}
\boldsymbol{z} = \dfrac{\boldsymbol{x} - \mu_{\boldsymbol{x}}}{\sigma_{\boldsymbol{x}}} , 
\end{equation}
kde $ \mu_{\boldsymbol{x}} $ je støední hodnota $ \boldsymbol{x} $ a $ \sigma_{\boldsymbol{x}} $ je rozptyl $ \boldsymbol{x} $.

\subsection{Architektury a trénování sítí}
Celkem bylo navrhnuto šest architektur sítí, na které se dále budeme odkazovat tuènì uvedenımi názvy:
\begin{itemize}
\item \textbf{dopøedná}
\begin{table}[H]
\centering
\begin{tabular}{ c|c|c }
typ vrstvy & poèet neuronù & aktivaèní funkce \\
\hline
plnì propojená & 1024 & ReLU \\
dropout (0.5) & - & - \\
plnì propojená & 512 & ReLU \\
dropout (0.5) & - & - \\
plnì propojená & 256 & ReLU \\
dropout (0.5) & - & - \\
plnì propojená & 40 & softmax
\end{tabular}
\end{table}

\item \textbf{LSTM}
\begin{table}[H]
\centering
\begin{tabular}{ c|c|c }
typ vrstvy & poèet neuronù/bunìk & aktivaèní funkce \\
\hline
LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
plnì propojená & 40 & softmax
\end{tabular}
\end{table}

\item \textbf{GRU}
\begin{table}[H]
\centering
\begin{tabular}{ c|c|c }
typ vrstvy & poèet neuronù/bunìk & aktivaèní funkce \\
\hline
GRU & $ \{ 100, 150, 200, 250 \} $ & tanh \\
GRU & $ \{ 100, 150, 200, 250 \} $ & tanh \\
plnì propojená & 40 & softmax
\end{tabular}
\end{table}

\item \textbf{obousmìrná CTC LSTM}
\begin{table}[H]
\centering
\begin{tabular}{ c|c|c }
typ vrstvy & poèet neuronù/bunìk & aktivaèní funkce \\
\hline
obousmìrná LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
obousmìrná LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
plnì propojená & 41 & softmax
\end{tabular}
\end{table}

\item \textbf{dávková CTC LSTM}
\begin{table}[H]
\centering
\begin{tabular}{ c|c|c }
typ vrstvy & poèet neuronù/bunìk & aktivaèní funkce \\
\hline
LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
plnì propojená & 41 & softmax
\end{tabular}
\end{table}

\item \textbf{obousmìrná dávková CTC LSTM}
\begin{table}[H]
\centering
\begin{tabular}{ c|c|c }
typ vrstvy & poèet neuronù/bunìk & aktivaèní funkce \\
\hline
obousmìrná LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
obousmìrná LSTM & $ \{ 100, 150, 200, 250 \} $ & tanh \\
plnì propojená & 41 & softmax
\end{tabular}
\end{table}

\end{itemize}

Dopøedná sí byla trénována pomocí stochastického gradientního sestupu s konstantou uèení $ \epsilon = 0.01 $ a Nesterovovım momentem $ \alpha = 0.9 $. Pokud bìhem 5 trénovacích epoch nedošlo ke sníení ztráty na validaèní sadì, konstanta uèení byla sníena na polovinu. Všechny rekurentní neuronové sítì pak vyuívaly optimalizaèní algoritmus ADAM s doporuèenımi hodnotami \cite{adam}. Všechny sítì byly trénovány po dávkách o velikosti 64 pozorování.

Obousmìrná CTC LSTM vyuívá k predikci celou trénovací sekvenci. Celá sekvence ovšem v pøípadì predikce v reálném èase není dostupná, a proto byly také testovány dávkové CTC sítì, které stejnì jako dopøedné èi LSTM a GRU sítì vyuívají k predikci pouze kratší sekvence o pøedem definované délce .

Dopøedná sí, sí LSTM a sí GRU vyuívají k dekódování vısledné sekvence Viterbiho algoritmus se zerogramovım jazykovım modelem. Viterbiho dekodér byl implementován pomocí algoritmu cestování etonù (anglicky token passing), kde byly vyuity jednostavové skryté Markovovy modely reprezentující jednotlivé fonémy. CTC sítì vyuívají dekódování nejlepší cesty.

Jako regularizaèní metody byly vyuity zanesení šumu do trénovacích dat a pøedèasné ukonèení o toleranci 10 epoch. Gaussovskı šum o smìrodatné odchylce 0.6 \cite{graves} byl novì vygenerován pro kadé pozorování pøed vstupem do sítì v kadé epoše. Metoda dropout mohla bıt vyuita pouze pro dopøednou sí, jeliko pro rekurentní sítì byly vyuity optimalizované vrstvy pro knihovnu CuDNN, které tuto metodu zatím nepodporují. Nicménì se ukázalo, e regularizace metodou zanesení šumù je pro tuto úlohu dostaèující a dropout, jeho rekurentní implementace bıvá vıpoèetnì velice pomalá, není tøeba.

% VYHODNOCENÍ
\newpage
\section{Experimenty a vyhodnocení}
Nejprve byly provedeny experimenty nad datovou sadou ŠkodaAuto a její parametrizací LFE. Nejlepší nastavení jednotlivıch architektur pak bylo testováno nad zbylımi parametrizacemi jak na datové sadì ŠkodaAuto, tak na datové sadì SpeechDat-E. Byly uvaovány tøi metriky (všechny na testovací sadì), na které se dále budeme odkazovat tuènì uvedenımi názvy:
\begin{itemize}
\item \textbf{frameAcc [\%]} - pøesnost klasifikace modelu vyhodnocována po jednotlivıch segmentech nahrávek
\begin{equation}
\text{frameAcc } = \dfrac{C_{s}}{N_{s}} \cdot 100,
\end{equation}
kde $ C_{s} $ je poèet správnì klasifikovanıch pozorování (segmentù) a $ N_{s} $ je poèet všech pozorování,
\item \textbf{phoneCorr [\%]} - procento správnì klasifikovanıch fonémù po dekódování
\begin{equation}
\text{phoneCorr } = \dfrac{C_{f}}{N_{f}} \cdot 100,
\end{equation}
kde $ C_{f} $ je poèet správnì klasifikovanıch fonémù a $ N_{f} $ je poèet všech fonémù,
\item \textbf{phoneAcc [\%]} - pøesnost modelu po dekódování
\begin{equation}
\text{phoneAcc } = \dfrac{C_{f} - I}{N_{f}} \cdot 100 ,
\end{equation}
kde $ C_{f} $ je poèet správnì klasifikovanıch fonémù, $ I $ je poèet navíc vloenıch znakù (fonémù) a $ N_{f} $ je poèet všech fonémù.
\end{itemize}
Pro sítì vyuívající metodu CTC nebyla sledována pøesnost modelu (frameAcc), ale pouze pøesnost dekódování (phoneAcc). Bìhem trénování modelù pak docházelo ke tøem jevùm
\begin{enumerate}
\item pøedèasné ukonèení z dùvodu stagnace uèícího se procesu, tj. chyba na validaèní sadì se po dobu 10 epoch témìø nezmìnila (*),
\item pøedèasné ukonèení z dùvodu overfittingu, tj. chyba na validaèní sadì zaèala rùst a~trénování bylo ukonèeno (**),
\item sí se nepodaøilo natrénovat (***).
\end{enumerate}
Zároveò je tøeba rozlišit tøi dùvody, proè se sí nepodaøilo natrénovat:
\begin{enumerate}
\item sí se nepodaøilo natrénovat kvùli vysoké vıpoèetní a èasové nároènosti (napø. pro datovou sadu SpeechDat-E s parametrizací LFE $ \Delta\Delta $ trvala jedna epocha trénování dopøedné sítì pøes 45 hodin),
\item sí se nepodaøilo natrénovat kvùli jevùm jako je exploze èi vymizení gradientu (napø. sítì GRU pro datovou sadu ŠkodaAuto s parametrizací LFE),
\item sí se nepodaøilo natrénovat kvùli neznámé chybì v knihovnì Keras, kdy bìhem jedné epochy dojde k vıraznému poklesu pøesnosti - pravdìpodobnì se jedná o~numerickou nestabilitu (napø. sítì LSTM a GRU pro datovou sadu SpeechDat-E s~parametrizacemi LFE).
\end{enumerate}

\subsection{Experimenty nad datovou sadou ŠkodaAuto}
První testovanou architekturou byla dopøedná sí, která na rozdíl od rekurentních sítí nemùe implicitnì vyuívat kontextu v datech. Proto bylo k aktuálnì klasifikovanému segmentu pøidáno $ n $ pøedešlıch segmentù zleva (levı kontext - minulost) a $ m $ následujících segmentù zprava (pravı kontext - budoucnost). Z tabulky \ref{tab:sa_lfe_dopredna} je patrné, e èím vìtší kontext má model k dispozici, tím je model a dekódování pøesnìjší. Aèkoliv nejlepších vısledkù dosahuje model s levım kontextem $ n = 20 $ a pravım kontextem $ m = 10 $, k dalším experimentùm byl vyuit model s levım kontextem $ n = 10 $ a pravım kontextem $ m = 5 $, kterı byl vybrán na základì nìkolika kritérií:
\begin{itemize}
\item vyuívá kontextu o srovnatelné délce (15) s délkou sekvence zvolenou pro vybrané rekurentní sítì (20),
\item dosahuje relativnì vysoké pøesnosti,
\item  vyuívá niší poèet parametrù (rychlejší trénování a "spravedlivìjší" \ srovnání s vybranımi rekurentními sítìmi, které vyuívají 100000 - 500000 parametrù).
\end{itemize} 

Jeliko cílem práce je porovnat vybrané architektury a ne optimalizace topologie a~natrénování jedné sítì za úèelem co nejvyšší pøesnosti, trénování dopøednıch sítí bylo omezeno na 150 epoch. Lze pøedpokládat, e s vyšším poètem epoch by bylo moné dosáhnout ještì lepších vısledkù.

\begin{table}[H]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c| }
\hline
\makecell{levı \\ kontext} & \makecell{pravı \\ kontext} & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
0 & 0 & 60.54 & 64.79 & 	53.17 & 150 & 710604 \\
\hline
10 & 5 & 81.27 & 87.26 & 79.99 & 150 & 1327404 \\
\hline
20 & 5 & 82.07 & 87.46 & 81.19 & 150 & 1738604 \\
\hline
20 & 10 & 83.34 & 87.13 & 82.09 & 150 & 1994204 \\
\hline
\end{tabular}
\caption{Vısledky experimentù pro dopøednou sí pro parametrizaci LFE nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfe_dopredna}
\end{table}

Jak pro sí LSTM, tak pro sí GRU, bylo testováno nìkolik rùznıch kombinací poètu bunìk v jednotlivıch vrstvách a délek vstupní sekvence. U všech sítí s délkou vstupní sekvence 20 a 30 segmentù, kromì sítì s délkou vstupní sekvence 20 a 100 buòkami v jednotlivıch vrstvách, docházelo k silnému overfittingu ji od tøetí epochy a sítì byly vyhodnoceny jako nevhodné kvùli nízké schopnosti generalizace (viz. tabulka \ref{tab:sa_lfe_lstm}). Pro další experimenty byla tedy zvolena sí, která obsahuje 100 bunìk v kadé vrstvì a pøijímá vstupní sekvenci o délce 20 segmentù. Tím byl zároveò  splnìn poadavek na malou sí, která by mohla bıt vyuita pro predikci v reálném èase i na mobilních zaøízeních. 

I experimenty se sítìmi GRU potvrdily, e vıše zmínìné parametry sítí, které trpìly overfittingem, nejsou pro tuto úlohu a datovou sadu vhodné.  Sítì GRU s tìmito parametry se nepodaøilo natrénovat z dùvodu saturace gradientu (viz. tabulka \ref{tab:sa_lfe_gru}). To zároveò ukazuje na jednu z pøedností LSTM vrstev a to, e u nich nemùe dojít k saturaci gradientu pøi zpracování dlouhıch sekvencí. Stejnì jako pro sí LSTM byla zvolena sí GRU se 100 buòkami v kadé vrstvì, která pøijímá vstupní sekvenci o délce 20 segmentù. Díky sjednocení komponent v GRU buòce zmínìného v teoretické èásti si mùeme všimnout, e sítì GRU mají vıraznì niší poèet parametrù oproti odpovídajícím LSTM sítím.

\begin{table}[H]
\centering
\resizebox{0.9\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c|c| }
\hline
\makecell{poèet \\ bunìk} & \makecell{délka \\ sekvence} & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
100 & 10 & 70.37* & 79.35 & 67.73 & 31 & 141640 \\
\hline
150 & 10 & 72.12* & 81.83 & 69.54 & 30 & 302440 \\
\hline
200 & 10 & 72.80* & 82.89 & 70.39 & 25 & 523240 \\
\hline
250 & 10 & 73.49* & 83.35 & 69.79 & 31 & 804040 \\
\hline
100 & 20 & 76.03* & 84.37 & 76.45 & 58 & 141640 \\
\hline
150 & 20 & 77.73**	 & 85.11 & 76.31 & 48 & 302440 \\
\hline
200 & 20 & 78.51**	 & 86.42 & 77.66 & 27 & 523240 \\
\hline
250 & 20 & 78.63**	 & 86.72 & 77.76 & 19 & 804040 \\
\hline
100 & 30 & 79.59**	 & 78.45 & 73.12 & 39 & 141640 \\
\hline
150 & 30 & 81.57**	 & 79.95 & 74.76 & 24 & 302440 \\
\hline
200 & 30 & 82.02** & 	80.93 & 75.23 & 25 & 523240 \\
\hline
250 & 30 & 82.70**	 & 80.93 & 75.24 & 26 & 804040 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro LSTM sí pro parametrizaci LFE nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfe_lstm}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.9\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c|c| }
\hline
\makecell{poèet \\ bunìk} & \makecell{délka \\ sekvence} & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
100 & 10 & 69.22 & 77.99 & 66.81 & 41 & 107240 \\
\hline
150 & 10 & 70.32* & 79.91 & 68.30 & 34 & 228340 \\
\hline
200 & 10 & 70.86* & 80.75 & 68.70 & 39 & 394440 \\
\hline
250 & 10 & 71.14* & 81.05 & 68.46 & 56 & 605540 \\
\hline
100 & 20 & 73.63* & 80.40 & 72.84 & 30 & 107240 \\
\hline
150 & 20 & 56.13*** & 62.28 & 55.41 & 25 & 228340 \\
\hline
200 & 20 & 50.45***$ ^{2} $ & 55.69 & 49.19 & 16 & 394440 \\
\hline
250 & 20 & 52.18***$ ^{2} $ & 57.89 & 51.03 & 12 & 605540 \\
\hline
100 & 30 & 53.80***$ ^{2} $ & 53.23 & 47.22 & 43 & 107240 \\
\hline
150 & 30 & 56.00***$ ^{2} $ & 56.92 & 49.57 & 18 & 228340 \\
\hline
200 & 30 & 48.02***$ ^{2} $ & 45.62 & 40.04 & 15 & 394440 \\
\hline
250 & 30 & 49.97***$ ^{2} $ & 48.87 & 43.43 & 12 & 605540 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro GRU sí pro parametrizaci LFE nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfe_gru}
\end{table}

Dále byly provedeny experimenty s LSTM sítìmi, které vyuívají metodu CTC. Obousmìrná CTC LSTM sí, která pøi klasifikaci vyuívá celou délku sekvence, dosahovala velmi vysoké pøesnosti jak na trénovací, tak na validaèní a testovací sadì (viz. tabulka \ref{tab:sa_lfe_ctclstm}). Z obdobnıch dùvodù jako u dopøedné sítì nebyla pro další experimenty zvolena sít dosahující nejvyšší pøesnosti, ale sí se 100 buòkami v kadé dopøedné i zpìtné LSTM vrstvì.

\begin{table}[H]
\centering
\begin{tabular}{ |c|c|c|c|c| }
\hline
\makecell{poèet \\ bunìk} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
100 & 94.03 & 92.97** & 124 & 363441 \\
\hline
150 & 94.07 & 92.65** & 68 & 785141 \\
\hline
200 & 94.32 & 93.36** & 60 & 1366841 \\
\hline
250 & 95.18 & 94.43** & 67 & 2108541 \\
\hline
\end{tabular}
\caption{Vısledky experimentù pro obousmìrnou CTC LSTM sí pro parametrizaci LFE nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfe_ctclstm}
\end{table}

Sí, která k zahájení predikce vyaduje celou sekvenci, je pro úèely predikce v reálném èase nevhodná. Proto byly provedeny experimenty se sítìmi vyuívající metodu CTC, které k predikci vyuívají pouze kratší sekvence (úseky z celé sekvence). Byly uvaovány dvì varianty - sí s obyèejnımi LSTM vrstvami a sí s obousmìrnımi LSTM vrstvami. Zde ji byly provedeny experimenty pouze pro dvì nejlepší nastavení sítì LSTM, a to buï s~pùvodní trénovací datovou sadou nebo s rozšíøenou datovou sadou. Rozdílem mezi tìmito trénovacími sadami je, e jednotlivé trénovací sekvence v pùvodní datové sadì na sebe navazují a jejich zøetìzením lze získat celou sekvenci, zatímco v rozšíøené trénovací sadì dochází k 50\% pøesahu mezi jednotlivımi trénovacími sekvencemi, tj. pro trénovací sekvence o délce $ m $ segmentù je posledních $ \frac{m}{2} $ segmentù trénovací sekvence stejnıch, jako prvních $ \frac{m}{2} $ segmentù následující trénovací sekvence). Tímto rozšíøením získá sí vìtší mnoství trénovacích dat a je schopna pøesnìjší klasifikace.

Z tabulek \ref{tab:sa_lfe_batchctclstm} a \ref{tab:sa_lfe_batchbictclstm} je patrné, e pøesnost obousmìrné sítì vdy pøevyšuje pøesnost sítì obyèejné, ovšem za cenu dvojnásobného poètu trénovatelnıch parametrù.

Pro obì architektury bylo zvoleno 100 bunìk v kadé vrstvì (pro obousmìrnou sí 100 bunìk v kadé dopøedné i zpìtné vrstvì), délka vstupní sekvence 20 a trénování pomocí rozšíøené trénovací sady. 

\begin{table}[H]
\centering
\begin{tabular}{ |c|c|c|c|c|c| }
\hline
\makecell{poèet \\ bunìk} & \makecell{délka \\ sekvence} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
100 & 20 (pøesah 10) & 81.31 & 63.04* & 	73 & 141741 \\
\hline
100 & 20 & 78.61 & 61.55* & 63 & 141741 \\
\hline
200 & 10 (pøesah 5) & 85.8 & 51.13* & 72 & 523441 \\
\hline
200 & 10 & 83.76 & 45.61* & 54 & 523441 \\
\hline
\end{tabular}
\caption{Vısledky experimentù pro dávkovou CTC LSTM sí pro parametrizaci LFE nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfe_batchctclstm}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ |c|c|c|c|c|c| }
\hline
\makecell{poèet \\ bunìk} & \makecell{délka \\ sekvence} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
100 & 20 (pøesah 10) & 87.55 & 72.31* & 73 & 363441 \\
\hline
100 & 20 & 86.42 & 67.93** & 	65 & 363441 \\
\hline
200 & 10 (pøesah 5) & 87.34 & 53.22* & 67 & 1366841 \\
\hline
200 & 10 & 85.83 & 49.39** & 42 & 1366841 \\
\hline
\end{tabular}
\caption{Vısledky experimentù pro obousmìrnou dávkovou CTC LSTM sí pro parametrizaci LFE nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfe_batchbictclstm}
\end{table}

Vybraná nastavení jednotlivıch architektur uvedenıch vıše, která byla vyuita pro další experimenty, jsou shrnuty v tabulce \ref{tab:architektury}.

\begin{table}[H]
\centering
\begin{tabular}{ |c|c|c| }
\hline
architektura & levı kontext & pravı kontext  \\
\hline
dopøedná & 10 & 5 \\
\hline \hline
architektura & poèet bunìk & délka sekvence  \\
\hline
LSTM & 100 & 20 \\
\hline
GRU & 100 & 20 \\
\hline
obousmìrná CTC LSTM & 100 & - \\
\hline
dávková CTC LSTM & 100 & 20 (pøesah 10) \\
\hline
obousmìrná dávková CTC LSTM & 100 & 20 (pøesah 10) \\
\hline
\end{tabular}
\caption{Vybraná nastavení jednotlivıch architektur získanıch experimenty.}
\label{tab:architektury}
\end{table}

Modely se zvolenım nastavením pak byly natrénovány pro všechny parametrizace nad datovou sadou ŠkodaAuto. Na vısledcích pro jednotlivé parametrizace (viz. tabulky \ref{tab:sa_lfe} a \ref{tab:sa_mfccdd}) je moné pozorovat jasnı trend, co se pøesnosti jednotlivıch architektur tıèe. Nejvyšší pøesnosti dosáhla obousmìrná CTC LSTM sí a to pøes 90\% pro všechny parametrizace. Druhé nejvyšší pøesnosti pak stabilnì dosahovala dopøedná sí, která se pohybovala okolo pøesnosti 80\%. Ukázalo se, e sí LSTM ve vìtšinì pøípadech dosahuje lepších vısledkù ne sí GRU a po dalším odlazení architektury by mohla konkurovat dopøedné síti. Obousmìrná dávková CTC LSTM sí pak dle pøedpokladù dosahuje vyšší pøesnosti ne obyèejná dávková CTC LSTM sí, jeliko vyuívá kontext i z takto krátkıch sekvencí. S rostoucí délkou sekvence pak lze oèekávat vyšší pøesnost blíící se k pøesnosti obousmìrné CTC LSTM sítì - délku sekvence je tedy vhodné volit na základì poadované pøesnosti rozpoznání fonémù a na základì omezení jako je napøíklad doba odezvy (kompromis mezi delší nebo kratší délkou sekvence).

Pro velké mnoství trénovanıch modelù byly experimenty provedeny vdy jednou, tudí není známá støední hodnota a standardní odchylka pøesností jednotlivıch architektur nad danımi parametrizacemi. Nicménì na základì vısledkù jak na datové sadì ŠkodaAuto, tak datové sadì SpeechDat-E, lze pøedpokládat, e vıše zmínìné závìry jsou reprezentativní a vyšší poèet experimentù by je pouze potvrdil.

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & 81.27 & 87.26 & 79.99 & 150 & 1327404 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & 76.03* & 84.37 & 76.45 & 58 & 141640 \\
\hline
GRU & 73.63* & 80.40	 & 72.84 & 30 & 107240 \\
\hline
obousmìrná CTC LSTM & -** & 94.03 & 92.97 & 124 & 363441 \\
\hline
dávková CTC LSTM & -* & 81.31 & 63.04 & 73 & 141741 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -* & 87.55 & 72.31 & 73 & 363441 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci LFE nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfe}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & 80.15 & 87.10 & 80.15 & 150 & 2643244 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & 77.13** & 85.39 & 78.17 & 59 & 173640 \\
\hline
GRU & 72.53* & 81.23 & 73.49 & 22 & 131240 \\
\hline
obousmìrná CTC LSTM & -** & 94.22 & 93.34 & 120 & 427441 \\
\hline
dávková CTC LSTM & -* & 86.05 & 68.58 & 98 & 173741 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -** & 89.97 & 72.74 & 83 & 427441 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci LFE $ \Delta\Delta $ nad datovou sadou ŠkodaAuto.}
\label{tab:sa_lfedd}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & 81.16 & 87.06 & 79.63 & 150 & 883308 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & 71.53** & 81.01 & 67.10 & 32 & 501640 \\
\hline
GRU & 73.70* & 80.56 & 73.12 & 35 & 	99140 \\
\hline
obousmìrná CTC LSTM & -** & 94.20 & 93.03 & 116 & 341841 \\
\hline
dávková CTC LSTM & -* & 80.34 & 65.28 & 69 & 130941 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -* & 87.54 & 72.19 & 68 & 341841 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci MFCC nad datovou sadou ŠkodaAuto.}
\label{tab:sa_mfcc}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & 82.35 & 87.76 & 81.06 & 150 & 1310956 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & 78.82** & 86.58 & 79.56 & 44 & 141240 \\
\hline
GRU & 77.03** & 84.61 & 77.65 & 71 & 106940 \\
\hline
obousmìrná CTC LSTM & -**	 & 94.66 & 93.70 & 12 & 5	362641 \\
\hline
dávková CTC LSTM & -* & 87.02 & 70.80 & 73 & 141341 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -** & 90.93 & 75.19 & 45 & 362641 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci MFCC $ \Delta\Delta $ nad datovou sadou ŠkodaAuto.}
\label{tab:sa_mfccdd}
\end{table}

\subsection{Experimenty nad datovou sadou SpeechDat-E}
Aèkoliv jsou vısledné pøesnosti nad parametrizacemi datové sady ŠkodaAuto pomìrnì vysoké, jedná se o malou datovou sadu, kterou se vhodnì zvolené architektury mohou pomìrnì snadno nauèit. Proto byly provedeny stejné experimenty i nad parametrizacemi komplexnìjší datové sady SpeechDat-E. Z tabulek \ref{tab:se_lfe} a \ref{tab:se_mfccdd} je na první pohled patrné, e pøesnost všech sítí je niší, a to o 5 a 15\%. Vıše zmínìné závìry jsou ovšem stále platné i~pro tuto datovou sadu.

Pro parametrizaci LFE se bohuel nepodaøilo natrénovat sítì LSTM a GRU kvùli neznámé a dosud neopravené chybì v knihovnì Keras. Trénovaní tìchto modelù bylo spuštìno vícekrát, nicménì vdy došlo k chybì døíve, ne byl model natrénován do takové míry, aby bylo moné reprezentativnì vyhodnotit jeho pøesnost. Pro parametrizaci LFE a LFE + $ \Delta\Delta $ se nepodaøilo natrénovat sítì LSTM, GRU a dopøednou sí kvùli vysoké èasové nároènosti (a dva dny na jednu epochu).

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & 72.5 & 71.68 & 68.65 & 150 & 1329717 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & -***$ ^{3} $ & - & - & - & 142549 \\
\hline
GRU & -***$ ^{3} $ & - & - & - & 108149 \\
\hline
obousmìrná CTC LSTM & -** & 83.10 & 	81.22 & 104 & 365250 \\
\hline
dávková CTC LSTM & -* & 69.40 & 58.32 & 50 & 142650 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -* & 73.12 & 62.42 & 40 & 365250 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci LFE nad datovou sadou SpeechDat-E.}
\label{tab:se_lfe}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & -***$ ^{1} $ & - & - & - & 2645557 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & -***$ ^{1} $ &	- & - & - & 174549 \\
\hline
GRU & -***$ ^{1} $ & - & - & - & 132149 \\
\hline
obousmìrná CTC LSTM & -** & 83.79 & 82.11 & 112 & 429250 \\
\hline
dávková CTC LSTM & -* & 73.28 & 60.87 & 61 & 174650 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -* & 77.85 & 66.81 & 28 & 429250 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci LFE $ \Delta\Delta $ nad datovou sadou SpeechDat-E.}
\label{tab:se_lfedd}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & 72.45 & 71.99 & 68.92 & 150 & 885621 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & 65.41* & 66.64 & 62.87 & 41 & 131749 \\
\hline
GRU & 63.37* & 63.31 & 59.86 & 23 & 100049 \\
\hline
obousmìrná CTC LSTM & -** & 81.73 & 80.42 & 96 & 343650 \\
\hline
dávková CTC LSTM & -* & 70.70 & 60.05 & 86 & 131850 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -* & 73.66 & 63.42 & 25 & 343650 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci MFCC nad datovou sadou SpeechDat-E.}
\label{tab:se_mfcc}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c| }
\hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
dopøedná & 73.13 & 72.64 & 69.88 & 150 & 1313269 \\
\hline \hline
architektura & \makecell{frameAcc [\%]} & \makecell{phoneCorr [\%]} & \makecell{phoneAcc [\%]} & \makecell{poèet \\ epoch} & \makecell{poèet \\ parametrù} \\
\hline
LSTM & 69.94* & 72.18 & 68.80 & 73 & 142149 \\
\hline
GRU & 67.34 & 70.23 & 65.53 & 42 & 107489 \\
\hline
obousmìrná CTC LSTM & -* & 84.82 & 83.23 & 71 & 364450 \\
\hline
dávková CTC LSTM & -* & 75.07 & 63.81 & 91 & 142250 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & -* & 78.21 & 67.87 & 48 & 364450 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro parametrizaci MFCC $ \Delta\Delta $ nad datovou sadou SpeechDat-E.}
\label{tab:se_mfccdd}
\end{table}

\newpage
\subsection{Porovnání vısledku nad ŠkodaAuto a SpeechDat-E}
Pro snadnìjší srovnání jednotlivıch architektur nad všemi parametrizacemi obou datovıch sad byly pøesnosti architektur a jejich pøesnosti dekódování shrnuty do tabulek \ref{tab:sa} a \ref{tab:se}.

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
\cline{2-9}
\multicolumn{1}{c}{} & \multicolumn{4}{|c|}{frameAcc [\%]} & \multicolumn{4}{c|}{phoneAcc [\%]} \\
\hline
architektura & LFE & \makecell{LFE \\ $ \Delta\Delta$} & MFCC &  \makecell{MFCC \\ $ \Delta\Delta$} & LFE &  \makecell{LFE \\ $ \Delta\Delta$} & MFCC &  \makecell{MFCC \\ $ \Delta\Delta$} \\
\hline
dopøedná & 81.27 & 80.15 & 81.16 & 82.35 & 79.99 & 80.15 & 79.63 & 81.06 \\
\hline
LSTM & 76.03 & 77.13 & 71.53 & 78.82 & 76.45 & 78.17 & 67.10 & 79.56 \\
\hline
GRU & 73.63 & 72.53 & 73.70 & 77.03 & 72.84 & 73.49 & 73.12 & 77.65 \\
\hline
obousmìrná CTC LSTM & - & - & - & - & 92.97 & 93.34 & 93.03 & 93.70 \\
\hline
dávková CTC LSTM & - & - & - & - & 63.04 & 68.58 & 65.28 & 70.80 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & - & - & - & - & 72.31 & 72.74 & 72.19 & 75.19 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro datovou sadu ŠkodaAuto.}
\label{tab:sa}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
\cline{2-9}
\multicolumn{1}{c}{} & \multicolumn{4}{|c|}{frameAcc [\%]} & \multicolumn{4}{c|}{phoneAcc [\%]} \\
\hline
architektura & LFE & \makecell{LFE \\ $ \Delta\Delta$} & MFCC &  \makecell{MFCC \\ $ \Delta\Delta$} & LFE &  \makecell{LFE \\ $ \Delta\Delta$} & MFCC &  \makecell{MFCC \\ $ \Delta\Delta$} \\
\hline
dopøedná & 72.50 & - & 72.45 & 73.13 & 68.65 & - & 68.92 & 69.88 \\
\hline
LSTM & - & - & 65.41 & 69.94 & - & - & 62.87 & 68.80 \\
\hline
GRU & - & - & 63.37 & 67.34 & - & - & 59.86 & 65.53 \\
\hline
obousmìrná CTC LSTM & - & - & - & - & 81.22 & 82.11 & 80.42 & 83.23 \\
\hline
dávková CTC LSTM & - & - & - & - & 58.32 & 60.87 & 60.05 & 63.81 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & - & - & - & - & 62.42 & 66.81 & 63.42 & 67.87 \\
\hline
\end{tabular}}
\caption{Vısledky experimentù pro datovou sadu SpeechDat-E.}
\label{tab:se}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{skoda_results.eps}
    \caption{Srovnání pøesností dekódování jednotlivıch architektur pro parametrizace nad datovou sadou ŠkodaAuto.}
    \label{fig:overfitting}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{speechdat_results.eps}
    \caption{Srovnání pøesností dekódování jednotlivıch architektur pro parametrizace nad datovou sadou SpeechDat-E.}
    \label{fig:overfitting}
\end{figure}

% ZÁVÌR
\newpage
\section{Závìr}
Cílem této práce bylo porovnat rùzné architektury neuronovıch sítí pro úlohu klasifikace fonémù.  V první èásti práce byly pøedstaveny dopøedné a rekurentní neuronové sítì a bìnì vyuívané optimalizaèní algoritmy. Zároveò byla odvozena metoda CTC, která umoòuje pøímı pøepis fonémù do vısledné podoby bez potøeby dalšího zpracování vıstupù sítì. Ve druhé èásti pak byly pøedstaveny navrené architektury a byla porovnána jejich pøesnost na ètyøech rùznıch parametrizacích dvou datovıch sad. Pøi návrhu architektur bylo zároveò zohlednìno jejich pøípadné pouití pro rozpoznávání v reálném èase, a proto byly voleny pøedevším sítì o dvou vrstvách s nízkım poètem parametrù.

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
\cline{2-9}
\multicolumn{1}{c}{} & \multicolumn{4}{|c|}{ŠkodaAuto} & \multicolumn{4}{c|}{SpeechDat-E} \\
\hline
architektura & LFE & \makecell{LFE \\ $ \Delta\Delta$} & MFCC &  \makecell{MFCC \\ $ \Delta\Delta$} & LFE &  \makecell{LFE \\ $ \Delta\Delta$} & MFCC &  \makecell{MFCC \\ $ \Delta\Delta$} \\
\hline
dopøedná & 79.99 & 80.15 & 79.63 & 81.06 & 68.65 & - & 68.92 & 69.88 \\
\hline
LSTM & 76.45 & 78.17 & 67.10 & 79.56 & - & - & 62.87 & 68.80 \\
\hline
GRU & 72.84 & 73.49 & 73.12 & 77.65 & - & - & 59.86 & 65.53 \\
\hline
obousmìrná CTC LSTM & 92.97 & 93.34 & 93.03 & 93.70 & 81.22 & 82.11 & 80.42 & 83.23 \\
\hline
dávková CTC LSTM & 63.04 & 68.58 & 65.28 & 70.80 & 58.32 & 60.87 & 60.05 & 63.81 \\
\hline
\makecell{obousmìrná \\ dávková CTC LSTM} & 72.31 & 72.74 & 72.19 & 75.19 & 62.42 & 66.81 & 63.42 & 67.87 \\
\hline
\end{tabular}}
\caption{Porovnání pøesnosti dekódování (phoneAcc [\%]) mezi datovou sadou ŠkodaAuto a~SpeechDat-E.}
\label{tab:sa_se}
\end{table}

Porovnáním pøesností rozpoznání se ukázalo (shrnuto v tabulce \ref{tab:sa_se}), e nejpøesnìjší architekturou je sí vyuívající obousmìrné LSTM vrstvy a metodu CTC, která dosáhla pøesnosti pøes 90\% na jednodušší datové sadì ŠkodaAuto a pøes 80\% na komplexnìjší datové sadì SpeechDat-E. Tato sí ovšem k rozpoznání vyuívala informaci z celé nahrávky a pro vyuití v reálném èase není vhodná.  Pomìrnì dobrıch vısledkù ovšem také dosahovala dopøedná sí s pyramidovou strukturou a rekurentní sí obsahující dvì LSTM vrstvy. Obousmìrná verze této rekurentní sítì, která vyuívala metodu CTC dosahovala obdobnıch vısledkù a pro rozpoznávání v reálném èase má pomìrnì velkı potenciál omezenı pouze vıkonem zaøízení.

Pro další zlepšení dosaenıch vısledkù by bylo vhodné zamìøit se na sít vyuívající LSTM vrstvy a na její obousmìrnou obdobu vyuívající metodu CTC a optimalizovat topologii tìchto architektur. Na základì podkladù danıch touto prací a vızkumem Alexe Gravese \cite{graves}, kterı ji øešil vliv poètu vrstev na pøesnost rozpoznání fonémù pro sí vyuívající celou sekvenci, se jako další postup jeví otestování vlivu délky vstupní sekvence spolu s hloubku sítì a šíøkou jednotlivıch vrstev na pøesnost rozpoznání. Zároveò je tøeba najít jistı kompromis mezi velikostí sítì a délkou vstupní sekvence tak, aby byla dosaena dostateèná pøesnost rozpoznání a zároveò byla zaruèena odezva sítì v reálném èase.

% SEZNAM OBRÁZKÙ A TABULEK
\newpage
\listoffigures
\listoftables

% LITERATURA
\newpage
\bibliographystyle{unsrt}
\bibliography{literatura}


\end{document}