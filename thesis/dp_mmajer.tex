\documentclass[12pt]{article}
\usepackage[czech]{babel}
\usepackage[cp1250]{inputenc}
\usepackage[a4paper,left=30mm,right=20mm,top=25mm,bottom=25mm]{geometry}

% pseudocodes
\usepackage[tworuled]{algorithm2e}

% tables and cline
\usepackage{multirow}
\usepackage{regexpatch}
\makeatletter
% Change the `-` delimiter to an active character
\xpatchparametertext\@@@cmidrule{-}{\cA-}{}{}
\xpatchparametertext\@cline{-}{\cA-}{}{}
\makeatother

% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{lmodern}
\usepackage{interval}

% equation numbering
\numberwithin{equation}{section}

% line-spacing 1.5
\usepackage{setspace}
\renewcommand{\baselinestretch}{1.5}

% indentation
\usepackage{indentfirst}

% citations
\usepackage{cite}
\usepackage[nottoc]{tocbibind}
\usepackage{url}

% images
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{float}
\graphicspath{{imgs/}}

% enumerate
\usepackage{enumitem}

\begin{document}

% TITULNÍ STRÁNKA
\begin{titlepage}
\thispagestyle{empty}
\begin{center}
\vspace*{0.5cm}
\Large Západoèeská univerzita v Plzni

\Large Fakulta aplikovanıch vìd

\Large Katedra kybernetiky

\vspace{8cm}
\LARGE DIPLOMOVÁ PRÁCE
\end{center}
\vfill
\large Plzeò, 2018
\hfill \large Martin Majer
\vspace*{1cm}
\end{titlepage}

% PROHLÁŠENÍ
\section*{Prohlášení}
\noindent Pøedkládám tímto k posouzení a obhajobì diplomovou práci zpracovanou na závìr studia na Fakultì aplikovanıch vìd Západoèeské univerzity v Plzni. \\[12pt]
\noindent Prohlašuji, e jsem diplomovou práci vypracoval samostatnì a vıhradnì s pouitím odborné literatury a pramenù, jejich úplnı seznam je její souèástí. \\[24pt]
\noindent V Plzni dne 20. dubna 2018
\hfill
$ \dots \dots \dots \dots \dots \dots \dots $
\thispagestyle{empty}
\newpage

% PODÌKOVÁNÍ
\section*{Podìkování}
Tímto bych rád podìkoval vedoucímu diplomové práce, Ing. Luboši Šmídlovi, Ph.D., za cenné rady a pøipomínky.
\thispagestyle{empty}
\newpage

\section*{Anotace}
\noindent Tato práce se zabıvá klasifikací izolovanıch slov pomocí neuronovıch sítí, klasifikátoru SVM a klasifikátoru zaloeném na algoritmu Dynamic Time Warping s ohledem na nízkou vıpoèetní nároènost. V první èásti jsou pøedstaveny pøíznaky v èasové a frekvenèní oblasti a odvozeny vyuité klasifikaèní algoritmy. Ve druhé èásti jsou uvedeny zvolené parametrizace testovanıch pøíznakù a struktura navrenıch klasifikaèních algoritmù. V závìru práce je pak vyhodnocena pøesnost klasifikace jednotlivıch metod pro zvolené parametrizace pøíznakù. \\[12pt]
\noindent \textbf{Klíèová slova:} zpracování akustického signálu, extrakce pøíznakù, detekce klíèovıch frází, dynamic time warping, support vector machine, neuronová sí

\section*{Abstract}
\noindent This thesis focuses on low computational cost isolated word recognition using neural networks, SVM classifier and Dynamic Time Warping based classifier. First part of the thesis introduces features in time and frequency domain and used classification techniques are derived. Parameterizations of tested features and structure of proposed classification algorithms are described in the second part of the thesis. Classification accuracy results of proposed methods for feature parameterizations are presented at the end of the thesis. \\[12pt]
\noindent \textbf{Keywords:} acoustic signal processing, feature extraction, keyword spotting, dynamic time warping, support vector machine, neural network
\thispagestyle{empty}
\newpage

% OBSAH
\tableofcontents
\thispagestyle{empty}
\newpage

% TEORETICKÁ ÈÁST
\pagenumbering{arabic}
\section{Úvod}
Neuronové sítì byly vyvinuty ji v polovinì minulého století, ale kvùli nedostateèné vıpoèetní kapacitì nemohly bıt plnì vyuity pro øešení reálnıch problémù. A v posledních letech, kdy došlo k vıznamnému vıvoji v oblasti hardwaru jak pro poèítaèe, tak i pro mobilní a vloená zaøízení, zaèali bıt plnì vyuívány a to zejména pro komplexní úlohy v oblasti zpracování obrazu a hlasu, kde stabilnì pøekonávají ostatní algoritmy strojového uèení. Vıvoj vıkonnıch grafickıch karet a optimalizovanıch knihoven pak umonil rychlé trénování tìchto modelù na velkém mnoství dat a díky vıkonnım èipùm lze provádìt predikci v reálném èase i na mobilních zaøízeních.

Tato práce se vìnuje vyuití neuronovıch sítí pro zpracování hlasu a to zejména úloze klasifikace fonémù s vyuitím základních metod zpracování akustického signálu pøedstavenıch v \cite{bp}. Ve zpracování hlasu jsou bìnì vyuívány jak sítì dopøedné, tak i rekurentní, které byly vytvoøeny pro klasifikaci èasovıch øad èi sekvencí a jsou schopny vyuívat kontextu. Pro trénování neuronovıch sítí je potøeba velké mnoství dat, aby byla zajištìna robustnost a schopnost generalizace s tím, e v úloze klasifikace fonémù jsou tato data ve formì zvukovıch nahrávek a odpovídajících pøepisù neboli transkripcí. 

Cílem práce je porovnat rùzné architektury neuronovıch sítí, které by mohly bıt vyuity pro pøepis mluvené øeèi do textové podoby (speech-to-text) v reálném èase. Za tímto úèelem byly voleny pøedevším jednodušší sítì s nízkım poètem parametrù. 

V první èásti práce je uvedena obecná teorie optimalizaèních algoritmù a neuronovıch sítí. Druhá èást se pak vìnuje metodì trénování rekurentních neuronovıch sítí, která nevyaduje pøedsegmentovaná trénovací data (více o segmentaci v \cite{bp}). Nakonec jsou porovnány rùzné architektury jak dopøednıch, tak i rekurentních neuronovıch sítí, na nìkolika datovıch sadách, které vznikly vyuitím rùznım typù pøíznakù.

\newpage
\section{Optimalizaèní algoritmy}
Vìtšina uèících se algoritmù vyuívá jistou formu optimalizace. Optimalizací rozumíme úlohu, kdy minimalizujeme èi maximalizujeme pøedem danou funkci $ f(\boldsymbol{x}) $ zmìnou parametru $ \boldsymbol{x} $. Hledáme tedy hodnotu $ x $ takovou, aby funkce funkce $ f(\boldsymbol{x}) $ nabıvala minimální èi maximální hodnoty. Vìtšina optimalizaèních metod uvauje minimalizace funkce $ f(\boldsymbol{x}) $ a její pøípadná maximalizace se provádí minimalizací funkce $ -f(\boldsymbol{x}) $.

Funkce, kterou optimalizujeme, nazıváme kritérium (v terminologii strojového uèení se také èasto objevují názvy cenová, ztrátová èi chybová funkce). Tato práce se zabıvá pøedevším optimalizací pro neuronové sítì, kde jsou nejèastìji vyuívány optimalizaèní metody zaloené na gradientu.

Základní metodou zaloenou na gradientu je tzv. gradientní sestup. Pøedpokládejme cenovou funkci $ J(\boldsymbol{\theta}) $ parametrizovanou souborem parametrù $ \boldsymbol{\theta} $. Gradientní sestup hledá optimální soubor parametrù $ \boldsymbol{\theta}^{*} = \text{argmin } J(\boldsymbol{\theta}) $ pomocí iterativního pravidla pro zmìnu parametrù
\begin{equation}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}),
\end{equation}
kde $ \epsilon $ je tzv. konstanta uèení, která udává velikost kroku v opaèném smìru nejvìtšího gradientu. Gradientní sestup pro konvexní cenové funkce vdy nalezne globální minimum a pro nekonvexní cenové funkce lokální minimum \cite{dl,karpathy,ruder}.

\subsection{Stochastickı gradientní sestup}
Aèkoliv je gradientní sestup efektivní optimalizaèní metoda, pøi optimalizaci nad velkım objemem dat mùe bıt velice pomalá a vıpoèetnì nároèná, jeliko pro jednu zmìnu parametrù je tøeba spoèítat gradient nad celou datovou sadou. Jednou z nejpouívanìjších modifikací gradientního sestupu, která tyto problémy øeší, je stochastickı gradientní sestup.

Pøedpokládejme cenovou funkci ve tvaru záporného logaritmu podmínìné pravdìpodobnosti
\begin{equation}
J(\boldsymbol{\theta}) =  \mathbb{E}_{\boldsymbol{x},  \boldsymbol{y} \sim p_{data}} L(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{\theta}) = \dfrac{1}{m} \sum_{i=1}^{m} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}),
\end{equation}
kde $ p_{data} $ je mnoina trénovacích dat o velikost $ m $ a $ L(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{\theta}) = -\log p(\boldsymbol{y} \mid \boldsymbol{x}; \boldsymbol{\theta}) $ je cena pro jedno pozorování v závislosti na souboru parametrù $ \boldsymbol{\theta} $. Pro takto definovanou cenovou funkci by gradientní sestup musel spoèítat gradient
\begin{equation}
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \dfrac{1}{m} \sum_{i=1}^{m} \nabla_{\boldsymbol{\theta}} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}),
\end{equation}
jeho komplexita je $ O(m) $. Stochastickı gradientní sestup nahlíí na gradient jako na støední hodnotu a tudí se pøedpokládá, e mùe bıt aproximován menšími soubory pozorování náhodnì vybíranımi z datové sady, tzv. dávky. V optimalizaèním kroku je tedy náhodnì vybrána dávka pozorování $ \boldsymbol{b} = \{ x^{1}, \ldots, x^{m'} \} $, kde $ m' $ se volí jako malé èíslo v závislosti na vıpoèetní kapacitì a velikosti $ m $ úplné datové sady. Pøi trénování modelu na grafické kartì (GPU) je vhodné volit velikost dávky jako mocninu dvou a to v rozmezí 8 a 256, aby mohly bıt plnì vyuity vektorizované operace GPU. Menší dávky mohou mít zároveò regularizaèní efekt díky šumu, kterı vnášejí do uèícího se procesu. Odhad gradientu je pak vypoèten s vyuitím dávky $ \boldsymbol{b} $
\begin{equation}
g = \dfrac{1}{m'} \nabla_{\boldsymbol{\theta}} \sum_{i=1}^{m'} L(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}, \boldsymbol{\theta}).
\end{equation}
Zmìna parametrù je pak dána dle pravidla
\begin{equation}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon g.
\end{equation}
Pro pevnì danou velikost modelu tedy vıpoèetní cena nezávisí na velikosti datové sady $ m $  \cite{dl,ruder}.

\subsection{Další modifikace gradientního sestupu}
Jedním z nejvìtších problémù pøi vyuití gradientního sestupu a gradientního stochastického sestupu je vıbìr konstanty uèení $ \epsilon $. Vysoká hodnota $ \epsilon $ mùe zpùsobit fluktuaci okolo lokálního minima nebo dokonce divergenci optimalizaèního procesu a nízká hodnota $ \epsilon $ mùe mít za následek vırazné zpomalení uèení. Konstanta uèení se tedy v praxi dynamicky mìní v závislosti na poètu epoch $ k $, tedy $ \epsilon_{k} $. Dalším bìnım problémem je uvíznutí v mìlkém lokálním minimu èi sedlovém bodì, co znemoní další uèení. Bylo tedy vytvoøeno nìkolik modifikací základních gradientních algoritmù, které tyto problémy do jisté míry øeší \cite{dl, ruder}.

\subsubsection{Moment}
Moment byl navren za úèelem zrychlení trénování a to pøedevším pøi optimalizace ztrátovıch funkcí, které
mají mnoho mìlkıch lokálních minim nebo v pøípadech, kdy jsou gradienty znaènì zašumìné. Algoritmus momentu vyuívá plovoucí prùmìr minulıch gradientù s exponenciálním zapomínáním a pokraèuje v jejich smìru.

Tento algoritmus zavádí parametr $ \alpha $, kterı udává, jakou rychlostí mají bıt minulé gradienty zapomínány. Pravidlo pro zmìnu parametrù pak vypadá následovnì
\begin{align}
v &\leftarrow \alpha v - \epsilon \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) \right), \\
\boldsymbol{\theta} &\leftarrow \boldsymbol{\theta} + v.
\end{align}
Promìnná $ v $ akumuluje prvky gradientu $ \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) \right) $ s tím, e èím vìtší je parametr $ \alpha $ vùèi konstantì uèení $ \epsilon $, tím více minulé gradienty ovlivòují aktuální smìr kroku. Stejnì jako u konstanty uèení, i parametr $ \alpha $ mùe bıt mìnen v závislosti na èase. Vìtšinou je jako poèáteèní hodnota zvolena 0.5 a je navyšována a na 0.99 \cite{dl, karpathy, ruder}.

\subsubsection{Nesterovùv moment}
Dalším variantou je Nesterovùv moment, kterı dále rozšiøuje algoritmus momentu. Pravidlo pro zmìnu parametrù se zmìní na
\begin{align}
v &\leftarrow \alpha v - \epsilon \nabla_{\boldsymbol{\theta}} \left( \dfrac{1}{m} \sum_{i=1}^{m} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta} + \alpha v), \boldsymbol{y}^{(i)}) \right), \\
\boldsymbol{\theta} &\leftarrow \boldsymbol{\theta} + v,
\end{align}
kde parametry $ \alpha $ a $ \epsilon $ odpovídají stejnım parametrùm jako u algoritmu momentu. Hlavním rozdílem oproti algoritmu momentu je, e gradient je vyhodnocen a po aplikaci minulıch gradientù. Nejprve je tedy proveden krok ve smìru akumulovanıch minulıch gradientù a následnì je provedena korekce \cite{dl, karpathy, ruder, nesterov}.

\subsection{ADAM}
Cenová funkce bıvá èasto citlivá na urèité smìry v prostoru parametrù a naopak necitlivá na smìry jiné. Moment tento problém do urèité míry øeší, ale za cenu zavedení nového parametru, kterı je tøeba správnì nastavit. ADAM je adaptivní metoda, která tento problém øeší tím, e zavede vlastní konstantu uèení pro kadı parametr a tyto konstanty pak automaticky v prùbìhu uèení adaptuje.

ADAM pouívá ke zmìnì parametrù plovoucí prùmìr (první centrální moment) gradientù s exponenciálním zapomínáním $ r $ a druhı necentrální moment $ v $. Poèáteèní hodnota obou momentù je nastavena na nulu a zejména v prvních krocích algoritmu je potøeba momenty opravit korekèním faktorem. Parametry $ \beta_{1} $ a $ \beta_{2} $ udávají rychlost zapomínání a $ \delta $ je malá konstanta  \cite{dl, karpathy, ruder, adam}.
\\[12pt]
\begin{algorithm}[H]
\setstretch{1.25}
inicializace \\
$ \boldsymbol{r} = 0 $, $ \boldsymbol{v} = 0 $, $ \boldsymbol{t} = 0 $  \\
\While{zastavovací podmínka není splnìna}{
 $ t \leftarrow t + 1 $  \\
 vıpoèet gradientu \\
$ g = \dfrac{1}{m} \nabla_{\boldsymbol{\theta}} \sum_{i} L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)}) $ \\ 
 zmìna odhadu prvního centrálního momentu \\
 $ \boldsymbol{r} \leftarrow \beta_{1}\boldsymbol{r} + (1 - \beta_{1})g $ \\
 zmìna odhadu druhého necentrálního momentu \\
 $ \boldsymbol{v} \leftarrow \beta_{2}\boldsymbol{v} + (1 - \beta_{2})g \odot g $ \\
 korekce odhadu prvního centrálního momentu \\
 $ \hat{\boldsymbol{r}} \leftarrow \dfrac{\boldsymbol{r}}{1- \beta_{1}^{t}} $ \\
 korekce odhadu druhého necentrálního momentu \\
 $ \hat{\boldsymbol{v}} \leftarrow \dfrac{\boldsymbol{v}}{1- \beta_{2}^{t}} $ \\
 zmìna parametrù \\
 $ \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + -\epsilon \dfrac{\hat{\boldsymbol{r}}}{\sqrt{\hat{\boldsymbol{v}} + \delta}} $
}
\end{algorithm}
\begin{center}
Algoritmus 1: ADAM.
\end{center}

\newpage
\section{Neuronové sítì}
Neuronová sí je algoritmus, kterı je schopen aproximovat i silnì nelineární funkce a zároveò je schopen dosáhnout vysoké míry statistické generalizace. Tento parametrickı model bıvá zpravidla sloen z nìkolika vrstev reprezentovanımi vektory, jejich dimenze udává šíøku modelu. Prvky tìchto vektorù, jednotky èi perceptrony, pracují paralelnì a reprezentují funkce zobrazující vstupní vektor na skalár. Cílem je natrénovat parametry tohoto modelu tak, aby dokázal splnit zadanou úlohu s minimální chybou. Jedná se tedy o optimalizaèní úlohu.

Základním rozdílem mezi bìnou optimalizací a optimalizací v neuronovıch sítí je, e optimalizace v neuronovıch sítích a ve strojovém uèení obecnì probíhá nepøímo. To znamená, e aèkoliv optimalizujeme zvolenou metriku $ P $, která v jistém smyslu kvantifikuje vıkon algoritmu na dané úloze, minimalizujeme jinou cenovou funkci $ J(\boldsymbol{\theta}) $ za úèelem minimalizace metriky $ P $. V bìném optimalizaèním problému bychom minimalizovali pøímo cenovou funkci $ J(\boldsymbol{\theta}) $ za úèelem její minimalizace \cite{dl, karpathy, bishop}.

\subsection{Cenová funkce}
Jedním ze zásadních aspektù pøi návrhu neuronovıch sítí je právì vıbìr cenové funkce. Vıbìr cenové funkce závisí na dané úloze a poadovaném vıstupu. Cenovou funkci definujeme stejnım zpùsobem jako u ostatních parametrickıch modelù, které generují hustotu pravdìpodobnosti $ p(\boldsymbol{y} \mid \boldsymbol{x} ; \boldsymbol{\theta}) $ a pøi trénování vyuívají principu maximální vìrohodnosti. Vìtšinou tedy cenovou funkci definujeme jako vzájemnou entropii mezi trénovacími daty a predikcemi modelu
\begin{equation}
J(\boldsymbol{\theta}) = -\mathbb{E}_{\boldsymbol{x}, \boldsymbol{y} \sim \hat{p}_{data}} \log p_{model} (\boldsymbol{y} \mid \boldsymbol{x}).
\end{equation}
Pøedpis cenové funkce se liší model od modelu v závislosti na tvaru $ \log p_{model} $ a kromì definice ceny také mùe obsahovat regularizaèní prvky. Vıhodou vyuití principu maximální vìrohodnosti je, e specifikací modelu $ p(\boldsymbol{y} \mid \boldsymbol{x}) $ zároveò definujeme cenovou funkci $ \log p(\boldsymbol{y} \mid \boldsymbol{x}) $ \cite{dl, bishop}.

\subsection{Dopøedné neuronové sítì}
Dopøedné neuronové sítì, obèas také nazıvané vícevrstvé perceptrony, jsou základním typem neuronovıch sítí. Jejich cílem je aproximovat urèitou funkci $ f^{*} $, napø. klasifikátor $ \boldsymbol{y} = f^{*}(\boldsymbol{x}) $ zobrazuje vstup $ \boldsymbol{y} $ na vıstupní tøídu $ \boldsymbol{x} $. Dopøedná neuronová sí tedy definuje zobrazení $ \boldsymbol{y} = f^{*}(\boldsymbol{x} ; \boldsymbol{\theta}) $ a uèí se optimální hodnoty parametrù $ \boldsymbol{\theta} $ pøi kterıch by mìla bıt dosaena nejlepší aproximace funkce.

Jak název napovídá, tok informací smìøuje od vstupu $ \boldsymbol{x} $ pøes nìkolik po sobì jdoucích vıpoèetních vrstev definujících $ f $ a k vıstupu $ \boldsymbol{y} $. Jedná se tedy o sloení nìkolika rùznıch funkcí do øetìzové struktury. Takto definovanı model lze také popsat jako orientovanı acyklickı graf, kterı urèuje závislosti mezi funkcemi. Mìjme napøíklad sí sloenou ze tøí funkcí $ f^{(1)} $, $ f^{(2)} $ a $ f^{(3)} $ spojenıch do øady $ f( \boldsymbol{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\boldsymbol{x}))) $. V tomto pøípadì nazıváme  $ f^{(1)} $ první skrytou vrstvou, $ f^{(2)} $ druhou skrytou vrstvou a $ f^{(3)} $ vıstupní vrstvou. Celková délka tohoto øetìzce pak udává hloubku modelu \cite{dl}.

Cílem trénování sítì je, aby se nauèila odpovídající zobrazení mezi $ f(\boldsymbol{x}) $ a $ f^{*}(\boldsymbol{x}) $. Uèení probíhá na základì trénovacích dat, která poskytují zašumìná aproximovaná pozorování $ f^{*}(\boldsymbol{x}) $ vyhodnocena v rùznıch bodech. Ke kadému pozorování $ \boldsymbol{x} $ je k dispozici také skuteèná hodnota $ \boldsymbol{y} \approx f^{*}(\boldsymbol{x}) $. Vıstupní vrstva sítì se tedy pro kadou trénovací hodnotu $ \boldsymbol{x} $ snaí vyprodukovat hodnotu blízkou $ \boldsymbol{y} $. Chování skrytıch vrstev není pøímo dáno trénovacími daty a sí se je musí nauèit vyuívat k získání poadovaného vısledku, tedy k aproximaci $ f^{*}(\boldsymbol{x}) $ \cite{dl, bishop}.

\subsubsection{Architektura}
Klíèovım prvkem pøi návrhu neuronovıch sítí je jejich architektura. Architekturou sítì èi modlu rozumíme celkovou strukturu sítì - kolik má mít jednotek a jak mají bıt tyto jednotky mezi sebou propojeny.

Jak ji bylo uvedeno, neuronové sítì jsou tvoøeny skupinami jednotek, které jsou uspoøádány do jednotlivıch vrstev. Vìtšina architektur tyto vrstvy uspoøádává do øetìzové struktury, kde kadá vrstva je funkcí vrstvy, která ji pøedchází. V této struktuøe je pak první vrstva definována jako
\begin{equation}
\boldsymbol{h}^{(1)} = g^{(1)}\left( \boldsymbol{W}^{(1)\top} \boldsymbol{x} + \boldsymbol{b}^{(1)} \right),
\end{equation}
druhá vrstva jako 
\begin{equation}
\boldsymbol{h}^{(2)} = g^{(2)}\left( \boldsymbol{W}^{(2)\top} \boldsymbol{x} + \boldsymbol{b}^{(2)} \right),
\end{equation}
a tak dále, kde $ g^{(i)} $ je aktivaèní funkce vrstvy $ i $, $ \boldsymbol{W}^{(i)\top} $ je váhová matice vrstvy $ i $ a $ \boldsymbol{b}^{(i)} $ je prahovı vektor vrstvy $ i $. 

Pro takto definovanou øetìzovou architekturu je pak hlavním problémem urèení hloubky sítì a šíøky kadé vrstvy. Vhodnou architekturu pro danou úlohu je tøeba nalézt pomocí experimentù zaloenıch na sledování chyby na validaèní datové sadì a apriorní znalosti o úloze a datech \cite{dl, karpathy, bishop, hastie}.

\subsubsection{Vıstupní jednotky}
Reprezentace vıstupu je úzce spojena s danou úlohou a tím i s vıbìrem cenové funkce. Pøedpokládejme, e dopøedná neuronová sí poskytuje vıstupní vrstvì soubor skrytıch pøíznakù  $ \boldsymbol{h} = f(\boldsymbol{x} ; \boldsymbol{\theta}) $, tj. vıstup poslední skryté vrstvy. Cílem vıstupní vrstvy je pak provést urèitou transformaci tìchto pøíznakù, aby sí plnila úlohu, pro kterou byla navrena. Tato transformace je provedena pouitím aktivaèní funkce $ g(\boldsymbol{h}) $. Aèkoliv existuje mnoho aktivaèních funkcí, které lze ve vıstupní vrstvì vyuít, zde se zamìøíme pouze na aktivaèní funkci softmax, která je vyuívána v experimentech provedenıch v rámci této práce.

Aktivaèní funkce softmax je vyuívána pro úlohy, kde je tøeba reprezentovat vıstup jako hustotu pravdìpodobnosti diskrétní promìnné s $ n $ monımi hodnotami (vıstupními tøídami). K odvození aktivaèní funkce softmax vyuijeme znalosti o úloze binární klasifikaci, pøi které predikujeme hodnotu
\begin{equation}
\hat{\boldsymbol{y}} = P(\boldsymbol{y} = 1 \mid \boldsymbol{x}),
\end{equation}
kde $ \hat{\boldsymbol{y}} \in \interval{0}{1} $. Aby byla zajištìna numerická stabilita pøi optimalizaci, budeme radìji hodnotu
\begin{equation}
\boldsymbol{z} = \log \tilde{P} (\boldsymbol{y} = 1 \mid \boldsymbol{x}).
\end{equation}
Aplikací exponenciální funkce a následnou normalizací bychom pak dostali Bernoulliho rozloení pravdìpodobnosti øízeného sigmoidální funkcí.

Pro generalizaci tohoto postupu pro diskrétní promìnnou s $ n $ hodnotami je tedy potøeba získat vektor $ \hat{\boldsymbol{y}} $, kde $ \hat{\boldsymbol{y}}_{i} = P(\boldsymbol{y} = i \mid \boldsymbol{x}) $. Pro kadı prvek $ \hat{\boldsymbol{y}}_{i}  $ musí platit $ \hat{\boldsymbol{y}}_{i} \in \interval{0}{1} $ a zároveò $ \sum_{i} \hat{\boldsymbol{y}}_{i}  = 1 $, aby bylo moné tento vektor interpretovat za hustotu pravdìpodobnosti. Nejprve je potøeba provést predikci vrstvou s lineární aktivaèní funkcí, která predikuje nenormalizované logaritmované pravdìpodobnosti
\begin{equation}
\boldsymbol{z} = \boldsymbol{W}^{\top} \boldsymbol{h} + \boldsymbol{b},
\end{equation}
kde $ \boldsymbol{z}_{i} = \log \tilde{P}(\boldsymbol{y} = i \mid \boldsymbol{x}) $. Softmax funkce pak aplikuje exponenciální funkci a normalizuje $ \boldsymbol{z} $, èím získáme poadované $ \hat{\boldsymbol{y}} $.
\begin{equation}
softmax(\boldsymbol{z}_{i}) = \dfrac{\exp(\boldsymbol{z}_{i})}{\sum_{j} \exp(\boldsymbol{z}_{j})}
\end{equation}
Natrénováním parametrù modelu pak bude vıstupní vrstva s aktivaèní funkcí softmax predikovat podíly poètù všech pozorovanıch vısledkù v trénovací datové sadì \cite{dl}.
\begin{equation}
softmax(\boldsymbol{z}(\boldsymbol{x} ; \boldsymbol{\theta}))_{i} \approx \dfrac{\sum_{j=1}^{m} 1_{\boldsymbol{y}^{(j)} = i, \boldsymbol{x}^{(j)} = \boldsymbol{x}}}{\sum_{j=1}^{m} 1_{\boldsymbol{x}^{(j)} = \boldsymbol{x}}}
\end{equation}

\subsubsection{Skryté jednotky}
Jak název napovídá, skryté jednotky jsou jednotky skryté vrstvy, jejich vstupem je vektor $ \boldsymbol{x} $, kterı je transformován na $ \boldsymbol{z} = \boldsymbol{W}^{\top} \boldsymbol{x} + \boldsymbol{b} $. Na takto transformovanı vstup je pak po prvcích aplikována nelineární aktivaèní funkce  $ g(\boldsymbol{z}) $. Volba aktivaèních funkcí skrytıch vrstev vyaduje mnoho experimentù a vyhodnocení pøesnosti modelu na validaèní datové sadì. V dnešní dobì se pro dopøedné neuronové sítì vìtšinou volí jednotky s aktivaèní funkcí RELU (z anglického "rectified linear unit") èi její modifikace, pro sítì rekurentní jsou pak voleny funkce hyperbolickı tangens a hard sigmoid.
\begin{itemize}
\item \textbf{RELU} - tyto jednotky vyuívají aktivaèní funkcí $ g(\boldsymbol{z}) = \max\{0, \boldsymbol{z}\} $. Jedná se tedy o lineární jednotky s prahem v bodì nula - levá polovina jejich definièního je rovna nule. To zaruèuje rychlı vıpoèet a vysokou hodnotu gradientu, kdykoliv je jednotka aktivní. Zásadním nedostatkem je, e tyto jednotky se pomocí gradientních metod nemohou uèit z pozorování, které mají aktivaèní hodnotu rovnou nule. Existuje proto nìkolik modifikací, které zajistí, e jednotky budou mít gradient všude (napø. Leaky RELU, PRELU, Maxout).
\item \textbf{sigmoid} a \textbf{tanh} - tyto jednotky vyuívají logistickou funkci (sigmoid) $ g(\boldsymbol{z}) = \sigma (\boldsymbol{z}) $, resp. hyperbolickı tangens $ \tanh (\boldsymbol{z}) = 2\sigma (2\boldsymbol{z}) - 1 $. Hlavním nedostatkem sigmoidální funkce je její citlivost a náchylnost k saturaci, kdy mùe dojít k tzv. explozi èi vymizení gradientu a sí nebude schopná se uèit.
\item \textbf{hard sigmoid} - tyto jednotky vyuívají aktivaèní funkci $ g(\boldsymbol{z}) = \max(0, \min(1, \frac{\boldsymbol{z} + 1}{2}) $. Jedná se o lineární aproximaci funkce sigmoid a díky své vıpoèetní nenároènosti jsou vyuívány v sítích typu LSTM \cite{dl, karpathy}.
\end{itemize}

\subsubsection{Algoritmus zpìtného šíøení}




..

..

..

..

..


Návrh a trénování neuronové sítì není nijak rozdílnı od ostatní algoritmù strojového uèení zaloenıch na gradientním sestupu. Nejvìtší rozdíl mezi takovımi algoritmy a neuronovımi sítìmi je, e nelinearita neuronové sítì mùe zpùsobit, e ztrátová funkce se stane nekonvexní. To znamenám, e neuronové sítì jsou vìtšinou trénovány pomocí iterativních optimalizaèních technik, které cost function dostanou na nízkou hodnotu místo toho, aby zajistili globální konvergenci jako napø. u logistické regrese nebo SVM. KOnvexní optimalizace konverguje z libovolnıch poèáteèních parametrù. Stochastickı gradientní sestup aplikovanı na nekonvexní loss function negarantuje konvergenci a je citlivı na hodnoty poèáteèní parametrù. Pro dopøedné neuronové sítì je dùleité incializovat všechny váhy nmalımi náhodnımi èísly. Biasy mohou bıt inicviálzovany nulou èi velmi malou klasdnou hodnotu. V následující kapitole si odvodíme algoritmus, kterım lze spoèítat gradient pro neuronové sítì - backpropagation. Srtejnì jako u ostatních modelù zaloenıch na gradientu i zde musíme definovat cost function a vhodnì reprezentovat vıstup modelu.

\newpage
\section{Kapacita modelu}

\newpage
\section{Connectionist temporal classification}

% PRAKTICKÁ ÈÁST
\newpage
\section{Klasifikace fonémù}

% VYHODNOCENÍ
\newpage
\section{Vyhodnocení}

% ZÁVÌR
\newpage
\section{Závìr}















\newpage
\newpage



















\section{Neuronové sítì}


\subsubsection{Backpropagation}
Kdy vyuíváme dopøednou neuronovou sí, která na vstupu pøijme $ x $ a na vıstupu vyprodukuje  $ \hat{y} $, informace/informaèní tok proudí skrz sí dopøedu (proto dopøedná sí). Vstup $ x $ dává síti poèáteèní informaci, která je poté propagována do skrytıch jednotek ve všech vrstvách a k vıstupní vrstvì, která vyprodukuje $ \hat{y} $. Tomuto procesu se øíká dopøedná propagace. Bìhem trénování probíhá dopøedná proapgace dokud nevyprodukuje skalární cost $ J(\theta) $. Algoritmus zpìtného šíøení (backpropagation) pak umoní zpìtnı tok informace od cost/ceny skrz sí za úèelem vıpoètu gradientu.

Analytickı vıpoèet gradientu je pomìrnì pøímoèarı, nicménì numericky mùe bıt vıpoèetnì velmi nároènı. Algoritmus zpìtného šíøení vıpoèet gradientu provádí pomocí jednoduché a vıpoèetnì nenároèené procedury.

Velmi èasto je algoritmus zpìtného šíøení povaován za celı uèící algoritmus vícevrstvıch neuronovıch sítí. Jedná se ovšem pouze o efektivní metodu vıpoètu gradientu a samotné uèení obstarává jinı optimalziaèní gradient (napø. SGD), kterı vyuívá gradient spoèítanı metodou zpìtného šíøení. Nyní se budeme zabıvat tím, jak pomocí tohoto algoritmu spoèítat gradient $ \nabla_{x}f(x,y) $ pro libovolnou funkci, kde $ x $ je soubor promìnnıch, jejich gradient chceme spoèítat a $ y $ je soubor promìnnıch, je jsou vstupem funkce, ale pro nì nevyadujeme vıpoèet gradientu. V uèících algoritmech jako je neuronová sí vìtšinou poèítáme gradient cenové funkce (cost function) with regard to parameters, $ \nabla_{\theta} J(\theta) $.

\textbf{øetìzové pravidlo}

Øetìzové pravidlo se bìnì vyuívá k vıpoètù derivací sloenıch funkcí, které jsou sloeny z funkcí, jejich derivace jsou známé. Algoritmus zpìtného šíøení je algoritmus, tkerı poèítá øetìzové pravidlo v pøesnì daném sledu operací tak, aby byl co nejvıkonnìjší.

Mìjmì reálné èíslo $ x $ a funkce $ f $ a $ g $, které zobrazují reálné èíslo na reálné èíslo. Dáke pøedpokládejme, e $ y = g(x) $ a $ z = f(g(x)) = f(y) $. Pak øetìzové pravidlo øíká, e 
$$
\dfrac{dz}{dx} = \dfrac{dz}{dy}\dfrac{dy}{dx}.
$$
Toto pravidlo mùeme rozšíøit ze skalárního pøípadu. pøedpokládejme, e $ x \in \mathbb{R}^{n}, y \in \mathbb{R}^{n}  $, $ g $ zobrazuje $ \mathbb{R}^{m} $ na $ \mathbb{R}^{n} $ a $ f $ je zobrazení z $ \mathbb{R}^{n} $ do $ \mathbb{R} $. Pokud  $ y = g(x) $ a $ z =  f(y) $, pak
$$
\dfrac{\partial z}{\partial x_{i}} = \sum_{j} \dfrac{\partial z}{\partial y_{j}} \dfrac{\partial y_{j}}{\partial x_{i}}
$$
Zapíšeme-li rovnici (odkaz) vektorovì, získáme tvar
$$
\nabla_{x}z = \left( \dfrac{\partial y}{\partial x} \right)^{T} \nabla_{y} z.
$$,
kde $ \dfrac{\partial y}{\partial x} $ je $ n \times m $ je Jakobián (Jacobian matrix) $ x $. V tomto tvaru øetìzové pravidla vidíme, e gradient promìnné $ x $ lze snadno získat vynásobením Jakobiánu $ \dfrac{\partial y}{\partial x} $ gradientem $ \nabla_{y} z $.

Toto pravidlo lze dále snadno rozšíøit i na tensory, jen jsou v neuronovıch sítích velice èasto pouívány. 

\textbf{dopøedné a zpìtné šíøení pro MLP}

Uveïmì si nyní pøíklad dopøedného a zpìtného šíøení v dopøedné neuronové síti. Pøedpokládejme sí o hloubce $ l $, kde kadé vrstvì náleí váhová matice $ \textbf{W}^{(i)}, i \in \{ 1, \ldots, l \} $ a parametr bias $ b^{(i)}, i \in \{ 1, \ldots, l \} $. Loss function $ L(y, \hat{y}) $ závisí na targetu $ y $ a vıstupu sítì $ \hat{y} $, která sí vyprodukuje, pokud dostane na vstupu $ x $. Celková cena $ J $ pro zjednodušení neobsahu ádnou regularizaèní sloku a odpovídá zrtrátové funcki $ L $.

\begin{algorithm}[H]
\setstretch{1.25}
$ h^{(0)} = x $ \\
\For{$ k = 1,\ldots,l $}{
	a .. aktivaèní hodnota (TODO) \\
	$ a^{(k)} = b^{(k)} + W^{(k)}h^{(k-1)} $ \\
	$ h^{(k)} = f(a^{(k)}) $
}
$ \hat{y} = h^{(l)} $ \\
$ J = L(y, \hat{y}) $
\end{algorithm}
\begin{center}
Algoritmus 1: Dopøedné šíøení.
\end{center}

Pøi zpìtném prùchodu jsou poèítány gradienty aktivací $ a^{(k)} $ pro kadou vrstvu $ k $ poèínaje vıstupní vrstvou a k první skryté vrstvì. Tyto gradienty mohou bıt interpretovány jako indikátor, jak by se mìl vıstup vrstvy zmìnit, aby sníil chybu. Gradienty vah a biasù mohou bıt rovnou vyuity pro optimalizace jako souèást stochastic grad. updatu (provedení zmìny parametrù hned po vıpoètu gradinetù) popø. mohou bıt vyuity pozdìji pro jinou optimalizaèní metodu zalouenou na gradientu.

\begin{algorithm}[H]
\setstretch{1.25}
vıpoèet gradientu vıstupní vrstvy (po dopøedné prùchodu sítì) \\
$ g \leftarrow \nabla_{\hat{y}} J = \nabla_{\hat{y}} L (y, \hat{y}) $ \\
\For{$ k = l, l-1, \ldots, 1 $}{
	zkonvertování gradientu na vıstupu vrstvy do gradientu pøed aplikací nelinární aktivace \\
	$ g \leftarrow \nabla_{a^{(k)}} J = g \odot f'(a^{(k)}) $ \\
	vıpoèet gradientù vah a biasu \\
	$ \nabla_{b^{(k)}} J = g $ \\
	$ \nabla_{W^{(k)}} J = g h^{(k-1)\top} $ \\
	propagace/šíøení gradientu wrt aktivaci skryté vrstvy o úroveò níe \\
	$ g \leftarrow \nabla_{h^{(k-1)}} J = W^{(k)\top} g $
	
}
\end{algorithm}
\begin{center}
Algoritmus 2: Zpìtné šíøení.
\end{center}

TODO: ztuènit $ \boldsymbol{x}^\top $, y,W,b,g hat(y)?

DEEP LEARNING

\subsection{Rekurentní neuronové sítì}
Rekurentní neuronové sítì (dále RNN) jsou typem neuronovıch sítí, která umí zpracovávat sekvenèní data, tj. sekevenci hodnot $ x^{(1)}, \ldots , x^{(\tau)} $. Abychom mohli od vícevrstvıch sítí pøejít k rekurentním, musíme vyuít principu z ranıch let strojového uèení a statistického modelování z 80. let: sdílení parametrù mezi rùznımi èástmi modelu. Sdílení parametrù umoní rozšíøit a aplikovat model na rùzné tvary pozorování (forms, v tomto pøípadì rùzné délky) a generalizovat mezi nimi. Kdybych mìli samostatné parametry pro kadou hodnotu v kadém èasové bodì (time index), nemohli bychom generalizovat na délky sekvencí, které nebyly vidìny bìhem trénovácí fázi a také bychom nemohli sdílet statistickou sílu (statistical strength) napøíè rùznımi délkami sekvencí a napøíè rùznımi pozicemi v èase. Toto sdílení je zejména dùleité v pøípadech, kdy specifická informace mùe nastat na více pozicích v sekvenci (napø. stejné datum na rùznıch pozicích ve vìtì).

Kadı prvek vıstupu je funkcí prvkù minulıch prvkù vıstupu. Kadı prvek vıstupu je vytvoøen stejnım updatovacím pravidlem jako minulé vıstupy. -> sharing through very deep computitaional graph

Pro jednoduchost pøedpokládejme, e RNN budu pracovat se sekvencí obsahující vektory $ x^{(t)} $ s èasovımi indexy $ t $ v rozsahu $ 1 $ a $ \tau $. Ve skuteènosti RNN vìtšinou pracují s minibatchemi takovıch sekvencí, s tím,  e kadı èlen minibatche mùe mít rùzné $ \tau $.

Abychom zajistili sdílení parametrù, musíme rekurentní vıpoèet "rozvinout" (unfolding) do vıpoèetní grafu s opakující se strukturou. Pøedpokládejme model v klasickém tvaru
$$
s^{(t)} = f(s^{(t-1)}; \theta),
$$
kde $ s^{(t)} $ nazıváme stavem systému. Tato rovnice (odkaz) je rekurzivní, jeliko definice $ s $ v èase $ t $ závisí na té samé definici v èase $ t-1 $. Pro koneènı poèet èasovıch krokù $ \tau $ mùeme rozvinout graf aplikací vzorce (definition) $ \tau - 1 $-krát. Napø. pro rovnici vıše (odkaz) pro rozvinutí $ \tau = 3 $ èasovıch krokù, získáme
$$
s^{(3)} = f(s^{(2)};\theta) = f(f(s^{(1)};\theta);\theta)
$$
Tímto opakovanım rozvinutím zajistíme, e rovnice ji neobsahjuja ádnı rekurenci a mle bıt reprezentována tradièním acyklickım directed comp. grafem.

OBRAZEK? DL 370

Pøidáním závislosti na externím vstupu $ x^{(t)} $ mùeme zadefinovat hodnoty skrytıch jednotek. Základní rovnice pro RNN je pak
$$
h^{(t)} = f(h^{(t-1)}, x^{(t)};\theta),
$$
kde $ h $ je stav. Typická RNN pak rozšíøí tuto rovnici a další architectural feasturas jako napø. vıstupní vrstvu která ète informaci ze skrytıch stavù $ h $ za úèelem predikce.

Pøi trénování sítì na danou úlohu, která vyaduje predikci budoucnosti z minulosti, sí se bìnì uèí jak pouívat $ h^{(t)} $ jako ztrátovou summary relevantních aspektù úlohy z minulosti sekvence od vstupu a po $ t $. Tato summary musí bıt z logiky ztrátová, jeliko mapuje sekvence o promìnné délce $ (x^{(t)}, x^{(t-1)}, x^{(t-2)}, \ldots, x^{(2)}, x^{(1)}) $ do vektoru pevnì dané délky $ h^{(t)} $. V závisloti na trénovacím kritériu, toto summary si mùe ponechat jisté aspekty z minulosti s jinou pøesností ne jiné.

DEEP LEARNING

Nyní mùeme zadefinovat rovnice pro dopøedné šíøení pro RNN zobrazenou v obrázku níe (odkaz). Aèkoliv obrázek pøesnì neurèuje aktivaèní funkci skrytıch jednotek, zde budeme pøedpokládat hyperbolickı tangens, kterı je v RNN vyuíván nejèastìji. Pøirozenou cestou, jak reprezentovat diskrétní promìnné je vracet ve vıstupu $ o $ nenormalizované log pravdìpodobnosti pro kadou monou diskrétní promìnnou. Aplikací operace softmax pak získáme vektor $ \hat{y} $ normalizovanıch pravdìpodobností nad vıstupem. Dopøedné šíøení zaèíná specifikací poèáteèního stavu $ h^{(0)} $. Poté jsou pro kadı èasovı krok $ t = 1 $ a $ t = \tau $ aplikovány následující rovnice updatu:
$$
a^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}
$$
$$
h^{(t)} = \tanh(a^{(t)})
$$
$$
o^{(t)} = c + Vh^{(t)}
$$
$$
\hat{y}^{(t)} = \text{softmax}(o^{(t)}),
$$
kde parametry jsou bias vektory $ b $ a $ c $ spolus s váhovımi maticemi $ U $, $ V $ a $ W $, které popoøadì odpovídají skrytım spojením (hidden connection) mezi input-to-hidden, hidden-to-output a hidden-to-hidden. Pøedpokládejme naoøíklad RNN, která mapuje vstupní sekvenci na vıstupní sekvenci o stejné délce. Celková ztráta pro danou sekvenci hodnot $ x $ paired with sekvence hodnot $ y $ by byla pouze suma ztrát ve všech èasovıch okamících. Napø. pokud $ L^{(t)} $ je záporní log-likelyhood $ y^{(t)} $ pro $ x^{(1)}, \ldots, x^{(t)} $, pak
$$
L\left( \{ x^{(1)}, \ldots, x^{(\tau)} \}, \{ y^{(1)}, \ldots, y^{(\tau)} \}  \right) = \sum_{t}L^{(t)} = - \sum_{t} \log p_{model} \left( y^{(t)} \mid \{ x^{(1)}, \ldots, x^{(t)} \}  \right),
$$
kde $ p_{model} \left( y^{(t)} \mid \{ x^{(1)}, \ldots, x^{(t)} \}  \right) $ je dán ètením vstupu $ y^{(t)} $ z vıstupního vektoru modelu $ \hat{y}^{(t)} $. Vıpoèet gradientu této ztrátové funkce vzhledem k parametrùm je drahá operace. Vıpoèet gradientu zahrnuje vıpoèet dopøedné propagace v našem obrázku zleva doprava  rozvinutého grafu následovanı zpìtnou propagací zprava doleva skrz uvedenı graf. Èasová nároènost je $ O(\tau) $ a není moné ji sníit paralelizací, jeliko dopøednı prùchod je sekvenèní a závislı na pøedchozích hodnotách. Algoritmus zpìtného šíøení aplikovanı na rozvinutı graf s cenou $ O(\tau) $ se nazıvá zpìtné šíøení èasem (back-propagation through time, BPPT).


OBRAZEK, DL str 373

DEEP LEARNIN|G


\subsubsection{Backpropagation through time}
Vıpoèet gradientu skrze RNN je pøímoèarı. Jedná se pouze o generalizovanou aplikaci algoritmu zpìtného šíøení nad rozvinutım vıpoèetním grafem. Gradienty získané pomocí algoritmu zpìtné šíøení pak mohou bıt vyuity s libovolnou technikou zaloenou na gradientu pro trénování RNN.

Uzly rozvinutého grafu obsahují parametry $ U $, $ V $, $ W $, $ b $ a $ c $ a sekvenci uzlu indexovanıch èasem $ t $ pro $ x^{(t)} $, $ h^{(t)} $, $ o^{(t)} $ a $ L^{(t)} $. Pro kadı uzel grafu $ N $ pak potøebujeme rekurzivnì spoèítat gradient $ \nabla_{N} L $ v závislosti na uzlech grafu, které tento uzel následují. Zaèneme tedy rekurzi v uzlu, kterı bezprostøednì pøedchází vıslednou ztrátu:
$$
\dfrac{\partial L}{\partial^{(t)}} = 1.
$$
V této derivaci pøedpokládáme, e vıstupy $ o^{(t)} $ jsou pouity jako arguemnt softmax funkce k tomu, abychom získali vektor pravdìpodobností vıstupu $ \hat{y} $. Dále pøedpokládáme, e ztráta je definována jako negative log-likelyhood skuteènéıch targetu $ y^{(t)} $ pro dosud dodanı vstup.  Gradient $ \nabla_{o^{(t)}} L $ nad všemi vıstupy v èase $ t $ pro všechna $ i, t $ vypadá následovnì:
$$
(\nabla_{o^{(t)}} L)_{i} = \dfrac{\partial L}{\partial o_{i}^{(t)}} = \dfrac{\partial L}{\partial L^{(t)}} \dfrac{\partial L^{(t)}}{\partial o_{i}^{(t)}} = \hat{y}_{i}^{(t)} - \boldsymbol{1}_{i,y^{(t)}}.
$$
Pokraèujeme dále s vıpoètem gradientu pøes další uzly poèínaje koncem sekvence. Ve finálním èasovém kroku $ \tau $, $ \boldsymbol{h}^{(\tau)} $ má jako následníka pouze $ o_{(\tau)} $, tudí vıpoèet gradientu je jednoduchı:
$$
\nabla_{h^{(\tau)}} L = V^{\top}\nabla_{o^{(\tau)}}L.
$$
Nyní ji mùeme iterovat zpìt v èase a zpìtnì šíøit gradienty od $ t = \tau - 1 $ a k $ t = 1 $ s tím, e $ h^{(t)} $ (pro $ t < \tau $) má jako své pøedchùdce $ o^{(t)} $ a $ h^{(t+1)} $. Gradient skryté vrstvy je pak
$$
\nabla_{h^{(t)}} L = \left( \dfrac{\partial h^{(t+1)}}{\partial h^{(t)}}  \right)^{\top} (\nabla_{h^{(t+1)}} L) + \left( \dfrac{\partial o^{(t)}}{\partial h^{(t)}}  \right)^{top} ( \nabla_{o^{(t)}} L ) = W^{\top} (\nabla_{h^{(t+1)}} L) \text{diag} \left(  1 - (h^{(t+1)})^{2} \right) + V^{\top} (\nabla_{o}^{(t)} L),
$$
kde $  \text{diag} \left(  1 - (h^{(t+1)})^{2} \right) $ je Jakobián hyperbolického tangensu pro skryté jednotky $ i $ v èase $ t+1 $.

Jakmile máme spoètené gradienty stavù, mùeme dopoèítat gradienty parametrù. Vzhledem k tomu, e parametry jsou sdílené mezi jednotlivımi èasovımi kroky, zavedeme nové promìnné $ W^{(t)} $, resp. $ U^{(t)} $, které jsou kopiemi váhovıch matic $ W $, resp. $ U $ a znaèí, jakou mírou tyto váhy pøíspívají ke gradientu v èase $ t $. Gradienty parametrù tedy mùeme urèit jako
$$
\nabla_{c} L = \sum_{t} \left( \dfrac{\partial o^{(t)}}{\partial c} \right)^{\top} \nabla_{o^{(t)}} L = \sum_{t} \nabla_{o^{(t)}} L 
$$

$$
\nabla_{b} L = \sum_{t} \left( \dfrac{\partial h^{(t)}}{\partial b^{(t)}} \right)^{\top} \nabla_{h^{(t)}} L = \sum_{t} \text{diag} \left( 1 - (h^{(t)})^{2} \right)  \nabla_{h^{(t)}} L
$$

$$
\nabla_{V} L = \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial o_{i}^{(t)}} \right) \nabla_{V} o_{i}^{(t)} = \sum_{t} (\nabla_{o^{(t)}} L) h^{(t)\top}
$$

$$
\nabla_{W} L = \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial h_{i}^{(t)}} \right) \nabla_{W^{(t)}} h_{i}^{(t)} = \sum_{t} \text{diag} \left( 1 - (h^{(t)})^{2} \right) (\nabla_{h}^{(t)} L) h^{(t-1)\top}
$$

$$
\nabla_{U} L = \sum_{t} \sum_{t} \left( \dfrac{\partial L}{\partial h_{i}^{(t)}} \right) \nabla_{u^{(t)}} h_{i}^{(t)} = \sum_{t} \text{diag} \left( 1 - (h^{(t)})^{2} \right) (\nabla_{h^{(t)}} L) x^{t\top} 
$$
ZKONTROLOVAT VZORCE a ZAROVNAT VLEVO

DEEP LEARNING

\subsection{Bidirectional RNN}
Zatím jsme se vìnovali pouze rekurentním sítím, které zachycovaly do stavu  v èase $ t $ pouze informaci z minulosti, tedy informace pro vstupy $ x^{(1)}, \ldots, x^{(t-1)} $ a aktuální vstupu $ x^{(t)} $.  Nicménì v mnoha aplikacích vıstupní predikce $ y^{(t)} $ mùe záviset na celé sekvenci. Napøíklad pøi zpracování øeèi je bìné, e správná interpretace fonémù závisí kvùli jevu koartikulace na minuláıch fonémech i na budoucích a na svém blízkém kontextu. Tento problém øeší obousmìrné rekuretní neuronové sítì.

Obousmìrná RNN kombinuje dvì RNN, kdy jedna se pohybuje v èase kupøedu, od zaèátku sekvence, zatímco druhá RNN se pohybuje v èase zpìt, od konce sekvence.
Tyto sítì mezi sebou sdílí skryté stavy $ h^{(t)} $ (sí dopøedná) a $ g^{(t)} $ (sí zpìtná), co uméní, e vıstupní jednotky $ o^{(t)} $ závisí jak na minulosti, tak na budoucnosti, ale zároveò jsou nejvíce citlivé na vstupní hodnoty okolo èasu $ t $.

OBRAZEK z DL str. 389 plus popisek

DEEP LEARNING

\subsubsection{Long short-term memory network}
Rekuretní sítì jsou velice náchylné na problém vanishing a exploding gradientu. Hlavnì v pøípadech dlouhodobıch závislostí dochází k velmi malım hodnotám vah vah, které jsou poté násobeny mnoha Jakobiány. Násobíme-li váhu $ w $ nìkolikrát samu s sebou, dojde buï k jejímu témìø vynulování/vymizení èi k explozi v závislosti na øádu $ w$. Obvzál¡ìt pro dlouhodobé vzdálenosti je øád tìchto vah a gtadientu exponencielnì menší ne u krátkodobích vzdáleností. Jednou z moností, jak øešit problém obtíného trénování sítí s dlouhodobımi závilosti je vyuití architektury dlouhıch sítí s krátkodobou pamìtí (long short-term memory network, dále jen LSTM) nebo rekurerní jednotku s branami (gated reccurent unit, dále jen GRU)

OBRAZKY OD COLAHAc NEBO z DL str 405?

Základním principem LSTM a zároveò jejím nejvìtším pøínosem je zavedení smyèek (self-loop), které vytvoøí cesty, kudy mùe gradient po dlouho dobu proudit (flow). Váha tìchto smyèek je pak trénována a mìnìna (conditioned) v závislosti na kontextu. Díky tomu, e je tato váha opatøena tzv. bránou (gated), tedy øízena jinou skrytou jednotkou, mùe bıt èasová integrace této informace mìnìna dynamicky. Díky této vlastnosti se LSTM stala velice úspìšnou pro úlohy jako napøíklad rozpoznání psaného písma, strojového pøekladu, rozpoznávání øeèi èi k popisu obrázkù. 

Rovnice pro dopøedné šíøení pro "mìlkou sí" jsou uvedeny níe. Nicménì byly testovány i hluboké sítì (Graves), které ukázali velkou reprezentativní schopnost.

POPIS diagramu ze strany 405: Jednotlivé jednotky v LSTM jsou nazıvány buòkami a jsou mezi sebou rekurentnì propojeny. Vstupní pøíznak/hodnota  je spoèten(a) obyèejnım perceptronem. Tato hodnotam ùe bıt akumulována ve stavu, pokud to sigmoidální vstupní brána umoní. Stavová jednotka má lineární smyèku, její váha je øízena zapomínací bránou. Vıstup buòky mùe bıt vypnut pomocí vıstupní brány. Všechny jednotky s bránou mají sigmoidální nelinearitu zatímco vstupní jednotka mùe mít libovolnı typ nelineariy. Stavová jednotka mùe bıt také vyuita jako vstup navíc pro ostatní jednotky s bránou.

Místo jednotek, které jednoduše aplikují nelinearity po prvcích k afinní tranformaci vstupu a rekurentních jednotek, LSTM rekurentní sítì mají tzv. "LSTM buòky", které mají kromì vnìjší rekurence RNN i interní rekurenci - smyèku (selfloop). Kadá buòka má stejné vstupy a vıstupy jako obyèejná rekurentní sí ale také má více parametrù a systém jednotek opatøenıch bránou (gating units), které øídí tok informací. Nejdùleitìjší komponent je stavová jednotka (state unit) $ s_{i}^{(t)} $, která je opatøena lineární smyèkou, její váha je øízena jednotkou se zapomínací bránou (forget gate unit) $ f_{i}^{(t)} $ (pro èas $ t $ a buòku $ i $), která nastavuje hodnoty této váhy na hodnoty mezi 0 a 1 pomocí jednotky se sigmoid aktivaèní funkcí (sigmoid unit):
$$
f_{i}^{(t)} = \sigma \left( b_{i}^{(t)} + \sum_{j}U_{i,j}^{f} x_{j}^{(t)} + \sum_{j} W_{i,j} h_{j}^{(t-1)}  \right),
$$
kde $ x^{(t)} $ je aktuální vstupní vektor a $ h^{(t)} $ je aktuální vektor skryté vrstvy obsahující vıstupy všech LSTM buòek a $ b^{f} $, $ U^{f} $, $ W^{f} $ jsou biasy, váhy vstupu (input weights) a rekurentní váhy (recurrent weights) pro zapomínací brány. Interní/vnitøní stav LSTM buòky je pak updatován následovnì, ale s conditional smyèkou váhou $ f_{i}^{(t)} $:
$$
s_{i}^{(t)} = f_{i}^{(t)} s_{i}^{(t-1)} + g_{i}^{(t)} \sigma \left( b_{i} + \sum_{j}U_{i,j} x_{j}^{(t)} + sum_{j} W_{i,j} h_{j}^{(t-1)}  \right),
$$
kde $ b $, $ U $ a $ W $ znaèí biasy, vstupní váhy a rekurentní váhy v LSTM buòce. Jednotka s vnìjší vstupní bránou (external input gate unit) $ g_{i}^{(t)} $ je poèítána podobnì jako zapomínací brána (se sigmoidální jednotkou k zisku hodnoty brány (gating value) mezi 0 a 1), ale s vlastními parametry:
$$
g_{i}^{(t)} = \sigma \left( b_{i}^{g} + \sum_{j} U_{i,j}^{g} x_{j}^{(t)} + \sum_{j} W_{i,j}^{g} h_{j}^{(t-1)}  \right).
$$
Vıstup LSTM buòky $ h_{i}^{(t)} $ mùe bıt vypnut pomocí vıstupní brány $ q_{i}^{(t)} $, která také vyuívá sigmoidální jednotku pro nastavení hodnoty brány (gating.. propustnost?):
$$
h_{i}^{(t)} = \tanh (s_{i}^{(t)}) q_{i}^{(t)},
$$
$$
q_{i}^{(t)} = \sigma \left( b_{i}^{o} + \sum_{j} U_{i,j}^{o} x_{j}^{(t)} + \sum_{j} W_{i,j}^{o} h_{j}^{(t-1)}  \right),
$$
kde paramtery $ b^{o} $, $ U^{o} $, $ W^{o} $ jsou biasy, vstupní vıhy a rekurentní váhy. 

Ukázalo se, e LS|TM sítì se snadno dokáí nauèit (nebo lépe ne ostastní) dlouhodobé závislosti v datech.

DEEP LEARNING

\subsubsection{Gated Recurrent Units}
Aèkoliv architektura LSTM je vıraznım vylepšením bìnıch RNN, ne všechny její prvky jsou nezbytnì nutné. Proto byly vyvinuty tzv. rekurentní jednotky s branami (gated recurrent unit, dále jen GRU). Hlavním rozdíl oproti LSTM je, e jediná jednotka s branou (gating unit) souèasnì øídí jak zapomínací faktor, tak i rozhodnutí k update stavové jednotky. Updatovací rovnice vypadají následovnì:
$$
h_{i}^{(t)} = u_{i}^{(t-1)} h_{i}^{(t-1)} + (1 - u_{i}^{(t-1)}) \left( b_{i} + \sum_{j} U_{i,j} x_{j}^{(t-1)} + \sum_{j} W_{i,j} r_{j}^{(t-1)} h_{j}^{(t-1)}  \right)
$$
kde $ u $ znaèí updatovací bránu a $ r $ znaèí resetovací bránu. Jejich hodnoty jsou definovány jako:
$$
u_{i}^{(t)} = \sigma \left( b_{i}^{u} + \sum_{j} U_{i,j}^{u} x_{j}^{(t)} + \sum_{j} W_{i,j}^{u} h_{j}^{(t)} \right)
$$
a
$$
r_{i}^{(t)} = \sigma \left( b_{i}^{r} + \sum_{j} U_{i,j}^{r} x_{j}^{(t)} + \sum_{j} W_{i,j}^{r} h_{j}^{(t)} \right ).
$$

Resetovací a updatovací brány mohou samostatnì "ignorovat" èásti stavového vektoru. Updatovací brány se pak mohou vytvoøit kopii libovolné dimenze (jeden extrém sigmodiy) èi kompletnì ignorovat (druhı extrém) tím, e nahradí stav novım cílovım stavem. Resetovací brána pak øídí, které èásti stavu budou pouity pro vıpoèet pøíštího cílového stavu. T|ím zavádí novı nelineární efekt ve vztahu mezi minulım a budoucím stavem.


DEEP LEARNING




\subsection{ADAM}

\section{Kapacita modelu, underfitting a overfitting}
Základní vlastností kadého modelu ve strojovém uèení by mìla bıt generalizace. To znamená, e model musí správnì predikovat i vìtšinu novıch, døíve nevidìnıch pozorování a nejen ta pozorování, na kterıch byl natrénován. Pøi trénování modelu tedy rozdìlíme datovou sadu na tøi: trénovací, validaèní a testovací. Cílem je, abychom po natrénovaní modelu dostali obdobnou trénovací, validaèní i testovací chybu. Pøi rozdìlení datové sady pøedpokládáme, e jednotlivá pozorování jsou navzájem nezávislá a všechny tøi sady podléhají stejnému rozloení. Kvalitní model by tedy mìl splòovat tyto vlastnosti:
\begin{enumerate}
\item musí minimalizovat chybu na trénovací sadì
\item musí minimalizovat rozdíl mezí chybou na trénovací a testovací sadou
\end{enumerate}

Pøi optimalizaci parametrù ovšem èasto dochází ke dvìma neádoucím jevùm: underfitting a overfitting.
K underfittingu dochází, pokud model není schopnı dostateènì minimalizovat chybu na trénovací sadì, tj. není schopnı se nauèit informaci obsaenou v trénovacích datech. K overfittingu naopak dochází, pokud model není schopnı splnit druhou podmínku - minimalizaci rozdílu chyby mezi trénovací a testovací sadou, tj. model si spíše zapamatuje trénovací data místo toho, aby se nauèil souvislosti v datech a na novıch, dosud nevidìnıch pozorováních selhává.


OBRAZEK NA POLYNOMECH

Oba tyto jevy mùeme ovládat pomocí tzv. kapacity modelu. Modely s nízkou kapacitou jsou náchylné na underfitting a naopak modely s vysokou kapacitou jsou náchylné na overfitting. Jedna z moností, jak ovládat kapacitu modelu, je pomocí vhodného vıbìru prostoru hypotéz neboli omezením vıbìru monıch funkcí, které model mùe vyuít (napø. poèet stupòù volnosti u lineární regrese). Omezením vıbìru funkce zároveò dochází k omezení poètu vyuitelnıch pøíznakù a sníení poètu volnıch parametrù, které model musí optimalizovat.

OBRAZEK OPTIMALNI KAPACITY (DL, 112)

Je zøejmé, e neexistuje jedinı model, kterı dokáe øešit libovolnı problém - vdy je potøeba zvolit vhodnı model pro danou úlohu a odladit jeho parametry.

Deep Learning, 107-115

TODO: generalizace pomocí sníení poètu parametrù

\subsection{Regularizace}
Jedním ze zpùsobù, jak navıšit generalizaci modelu bez ovlivnìní kapacity modelu, je regularizace. Regularizace  je modifikace uèícího se algoritmu, která sniuje generalizaèní chybu, ale zároveò nesniuje chybu trénovací. Regularizaci lze implementovat mnoha zpùsoby a opìt je tøeba vybrat vhodnou metodu pro danı problém.

Deep Learning, 117


Hlavní problémem strojového uèení je, jak zaøídit, aby algoritmus fungoval dobøe nejen na novıch datech, ale také na novıch, dosud nevidìnıch vstupech. Mnoho strategiíí ve strojovém uèení jsou explicitnì navreny tak, aby sníovaly chybu na testovací sadì, ovšem i za ceny vyšší chyby na trénovací sadì.

Existuje mnoho metod regularizace - nìkteré jsou zaloeny na zavedení omezujícíh podmínek modelu/hodnot jeho paraemtrù, jiné zase zavádìjí nové èlney do objective function. Tato omezení a penalizace èasto vycházejí z aprioirní znalosti problému, ale kolikrát jen zavádìjí preferenci po jednodušší tøídì modelu za úèelem vyšší generalizace.

V deep learningu vìtšina regularizaèních strategií regularizuje estimátory pomocá trade-offu vyšší bias za niší variance. Cílem regularizace je sniení overfittingu tak, aby prùbìh nauèené funkce odpovídal skuteènému trendu dat. (tuhle sraèku dhodnì pøepsat).

\subsection{Penalizace normy parametrù}

Populární metodou pro regularizaci jsou penalizaèní metody a to zejména L1 a L2 penalizace. Tyto penalizaèní metody omezují kapacitu modelu pøidáním penalizace $ \Omega(\theta) $ k objective function $ J $, která odpovídá normì parametrù. Regularizovaná objective funtion pak vypadá následnovnì:
$$
\tilde{J}(\theta ; X, y) = J(\theta ; X, y) + \alpha \Omega(\theta),
$$
kde $ \alpha \in [0, \infty) $ je hyperparametr, kterı váí relativní kontribuce penalizaèního èlenu $ \Omega $ relativnì je standardní funkci $ J $. Pokud $ \alpha = 0 $, poté nedochází k ádné regularizaci a vyšší hodnout regularizace roste.

Kdy pak trénovací algoritmus minimalzucje regularizavnou objective function $ \tilde{J} $, sniuje hosnotu jak originální objec. func. $ J $ na trénovacích datech, tak i nìjakou metriku velikosti parametrù $ \theta $. Rùzné volby normy $ \Omega $ pak vedou na preferenci rùznıch rešení. U neuronovıch sítí regularizujeme pouze váhy a biasy zùstavají bez regularizace.

DEEP LEARNING

\subsection{Noise injection}

Další metoda regularizace, která neovlivòuje model samotnı a byla vyuita jako primární metoda regularizace v této práci, je augmentace trénovacích dat ve formì aditivního šumu. Aèkoliv neuronové sítì nejsou moc robustní vùèi šumu,  pro vìtšinu klasifikaèních i regresních úloh schopny øešit úlohy i po zatíení malım náhodnım šumem pøidanım ke vstupním datùm. Pro nìkteré modely je pøidání šumu ekvivalentní pøidání penalizace normy vah.

DEEP LEARNING

\subsection{Early Stopping}
Pøi trénování velkıch modelù (s vysokou hloubkou, šíøkou èi obojím) s dostaèující reprezentaèní kapacitou k pøetrénovaná nad úlohou mùeme èasto sledovat, e chyba na térnvoací sadì pomalu a jistì bìhem èasu klesá, zatímco chyba na validaeèní sadì zaène stoupat.

OBRAZEK

Z rtohoto tedy mùeme usoudit, e mùeme získat lepší model s lepší validaèní chybou (a doufejme i s lepší testovací chybou), pokud  se vrátíme k parametrùm v èase s nejniší validaèní chybou. Pokadé, kdy se chyba na validaèní sadì sníí, uloíme si kopii parametrù modelu. Jakmile trénování skonèí, vrátíme se k nejlepšímu souboru parametrù místo ponechání posledních. Algoritmus konèí, pokud se ádná zmìna parametrù nepøekoná dosud nejlepší validaèní chybu pro pøeddefinovanou poèet iterací.

Algoritmus pøedèasného zastavení je forma regularizaxce, která nevyaduje témìø ádné zmìny trénovací procesu, modifikaci ovbjective function nebo souboru monıch hodnot parametrù. Je tedy snadné tento alogritmus pouít bez poškození dynamiky uèení. Early stopping lze vyuít budï samosratnì nebo spolu s vıše zmínìnımi metodami regularizace.

DEEP LEARNING


\subsection{Dropout}
Dropout nabízí vıpoèetnì nenároèné ale vıkonnou metodu regularizace pro širokou škálu modelù. Trénování s dropoutem funguje nejlépe pro uèící algorimty zaloenıch na minibachtchi, kterı dìlá malé kroky, napø. SGD. Pokádé, kdy pozorování vstoupí do minibatche, náhodnım vıbìrem (randomly sample) je vytvoøená different binární maska, která je aplikována na všechny vstupy skrytéch jednotek v síti. Tato maska je pro kadou jednotku samplována nezávisle na ostatních. Pravdìpodobnost samplingu hodnoty jedna v masce (zpùsobijící, e jednotka bude zahrnuta do trénování) je hyperparametr,k terı se urèuje pøed trénováním sítì. Vìtšinou se volí pravdìpobnost 0.8, e vstupní jednotku bude pouita a ppst 0.5, e skrytá jednotka bude pouita. Poté trénování pokraèuje jak obvykle - dopøedné šíøení,  backprop, zpìtné šíøení.

Jednou z hlavních vıchod kromì vıpoèetní a pamìtové nenároènosti je, e nijak vıraznì neomezuej vıbìr modelu a trénovací proceduru, která mùe bıt pouita. Funguje dobøe témìø s kadım modelem,, kterı lze trénovat pomocí SGD (vèetnì rekurntních neuronovıch sítí, kde je tøeba dropout zobecnict na rekurentní spojení jednotek)-

Nejvìtší síla dropout vycházi z toho, e maskování vnáší jistou formu šumu do skrytıch jednotek. Lze na nìj nahlíet jako na adaptivní destrukci informaèního obsahu vstupu ne-li na destrukci raw values inputu. Tím zamezuje, aby se urèitá jednotka zamìøila na rozpoznávání jednoho signifikatního pøíznaku, zatímco jiná jednotka by byla témìø nevyuita. Nághodnou deaktivací jednotek tak donutí ostatní jednotky, aby se také nauèili tuto informaci.

Dropout opìt mùe bıt vyuit spolu s ostatními metodami regularizace.

DEEP LEARNING


\section{Connectionist temporal classification}
Mnoho úloh, kde je tøeba se nauèit sekvence, vyadují predikci labelù sekvencí ze zašumìnıch a nesegmentovanıch vstupních dat. Napøíklad ve zpracování øeèi je akustickı signál pøepsán na slova nebo na jednotky podúrovní slova, napø. fonémy. RNN, jakoto modely schopny se nauèit sekvence, se pro tento typ úloh zdají bıti vhodné. Nicménì vyadují pøedsegmentovaná trénovací data a zároveò je tøeba jistı post-processing jejich vıstupu k transformaci na sekvence labelù.

Labelování nepøedsegmentovanıch sekvenèích dat je dlouhodobı problém v úlohách, kde je tøeba se nauèit sekvenci. Je to zejména bìné v úlohách vnímání, jako je napøíklad rozpozánávní ruènì pssaného písma, rozpoznávání øeèi a rozpoznávání gest, kde vstupní data jsou reprezentována jako zašumìnı tok (stream) reálnıch èísel anotovanıch pomocí øetìzcù diskrétních labelù, jako jsou napøíklad písmena èi slova.

Na tyto úlohy labelování byly velmi èasto vyuívány modely jako napøíklad HMM, které ovšem vyadujíc jisté pøedpoklady k jejich vyuití: specifická znalost domény úlohy pro správnı návrh stavovıch modelù, pøedpoklad, e vstupní pozorování jsou nezávislá a hlavnì trénování HMM je generativní zatímco úloha olabelování sekvence je diskriminativní. Na druhou stranu RNN nevyadují ádné pøedešlé znalosti o datech, pouze je tøeba zvolit reprezentaci jejich vstupních a vıstupních dat. Zároveò mohou bıt trénovanı diskriminativnì a jejich vnitøní stavy pøedstavují velmi silnı mechaniskus pro modelování èasovıch øad. Zároveò jsou velice robustní vùèi temporal a spatial šumu.

Døíve ovšem nebylo moné RNN na úlohu olabelování sekvence napøímo vyuít, jeliko jejich objective functions jsou definovány samostatnì pro kadı bod v trénovací sekvenci - jinımi slovy, RNN mohou bıt natrénovíny pouze ke klasifikaci series nezávislıch labelù. To znamená, e trénovací data musejí bıt pøedsegmentována a vıstup sítì musí projít jistım post-processingem, abychom získali vıslednou sekvenci labelù.

V této kapitole si uvedeme metodu, která nám umoní vyuít RNN pøímo k predikci sekvence labelù bez toho, abych potøebovali olabelovaná pøedsegmentovaná data a postprocessingu vıstupu. Základním principem této metody je interpretovat vıstupy sítì jako pravdìpodobnostní rozloení (hustota?) nad všemi monımi sekvencemi labelù v závislosti na vstupní sekvenci. Na základì této hustoty mùeme odvodit objective function, která pøímo maximalizuje pravdìpodobnost správného olabelování. Vzhledem k tomu, e tarto obj. func. je diferencovatelná, mùeme pak vyuít standarní BPTT k natrénovaání sítì.

Úloze labelování nepøedsegmentované sekvence dat budeme øíkat temporal classification a pokud k této úlzoe vyuijeme RNN, budeme pouívat connectionist temporal classification (volnì pøeloeno XYZ, dále jen CTC). Oproti tomu, úloze nezávislého labelování kadého kroku (framu èi okénka) vstupní sekvence budeme nazıvat framewise classification (podrobnyı postup, jak je provedena framewise clf. naleznete v [moje bakaláøka jako zdroj]).))

Nejprve si pøipravíme matematickou formalizaci pro temporal classification, dále si popíšeme reprezentaci vıstupu, která nám umoní vyuít RNN jako temporal klasifikátor a nakonec si uvedeme, jak je taková sí potom trénována.

CTC PAPER

Connectionism is a set of approaches in the fields of artificial intelligence, cognitive science and philosophy of mind, that attempts to represents mental or behavioral phenomena as emergent processes of interconnected networks of simple units.

WIKI

Mìjmì úlohu rozpoznávání øeèi. Pro tuto úlohu máme typicky kdispozici zvukové narhrávjky/stopy a odpodvídající transkripce/pøepisy. Bohuel ale neznáme, které písmeno v pøepisu odpovídá jako èásti zvukové stopy. Proto je tøeba ruènì èi dle pøedtrénovanım modelem pøiøadit ke kadému èasovému okamiku foném, která se v tomto okamiku vyskytuje. Toto mùe bıt pro velké datasety pomìrnì pracné a ne vdy je moné pøesnì urèit hranici mezi jednotlivımi fonémy.

DISTILLPUB


\subsection{Temporální klasifikace}
Mìjme trénovací mnoinu $ S $ z pevnì daného rozloení $ \mathcal{D}_{\mathcal{X} \times \mathcal{Z}} $.  Prostor vstupních hodnot $ \mathcal{X} = (\mathcal{R})^{*} $ je mnoina všech sekvencí tvoøenıch $ m $ dimenzionálními reálnımi vektory. Prostor vıstupních hodnot $ \mathcal{Z} = L^{*} $ je mnoina všech sekvencí nad koneènou abecedou (alphabet) $ L $ labelù. Obecnì budeme nazıvat prvky $ L^{*} $ sekvencemi labelù èi labellings. Kadé pozorování v $ S $ sestává z páru $ (x, z) $. Cílová (target) sekvence $ z = (z_{1}, z_{2}, \ldots , z_{U}) $ je nejvıše tak dlouhá, jako vstupní sekvence $ x = (x_{1}, x_{2}, \ldots , x_{T}) $, tedy $ U \leq T $. Vzhledem k tomu, e vstupní a cílové sekvence nejsou obecnì stejnì dlouhé, není moné je a priori align them.

Cílem je za vyuití $ S $ natrénovat temporal klasifikátor $ h: \mathcal{X} \mapsto \mathcal{Z} $, kterı bude klasifikovat pøedem nevidìné vstupní sekvence tak, aby minimalizoval nìjakou error measure specifickou pro danou úlohu (pro klasifikaci foñému se mùe jednat napøíklad o LER, tj. label error rate, kde je cílem minimalizovat poèet transkripèních chyb).


CTC PAPER

Hledáme pøesné zobrazení z $ X $ na $ Y $ s tím, e $ X $ a $ Y $ se mohou lišit svoji délkou a pøesnì neznáme, které prvky $ X $ odpovídají kterım prvkùm $ Y$. CTC tento problém øeší a pro vstupní sekvenci $ X $ najde vıstupní rozloení (distribution) nad všemi monımi $ Y $.

DISTILLPUB

\subsection{CTC}
Dále se budeme vìnovat reprezentacu vıstupu, kterı nám umoní vyuít RNN spolu s CTC. Zásádním krokem je transformovat vıstupy sítì do podmínìné hustoty pravdìpodobnosti nad sekvencemi labelù. Sí pak mùe bıt pouita jako klasifikátor vıbìrem nejvíce pravdìpodobného labellingu pro danou vstupní sekvenci.

CTC sí vyaduje softmax vıstupní vrstvu, která obsahuje o jednu jetnoku více, ne je poèet labelù v $ L $. Aktivace prvních $ |L| $ je jednotek je interpretována jako pravdìpodobnost pozorování koespundíjících labelù v danıch èasech. Aktivace $ |L| + 1 $ jednotky (jednotky navíc) je pak interpretována jako pravdìpodobnost pozorování "blank"/"prázdného" èí ádného labelu. Dohromady tyto vıstupy definují pravdìpodobosti všech monıch zarovnání (alignment) sekvencí labelù vùèi vstupní sekvencu. Celková pravdìpodobnost libovolné sekvece labelù apk mùe bıt nalezena seètením pravdìpodobností jejich rùznıch zarovnání.

Formálnìjší, pro vstupní sekvence $ x $ délky $ T $ definujeme RNN s $ m $ vstupy, $ n $ vıstupy a váhovım vektorem $ w $, kterı slouí jako spojité zobrazení $ \mathcal{N}_{w}: (\mathbb{R}^{m})^{T} \mapsto (\mathbb{R}^{n})^{T} $. Necht $ y = \mathcal{N}_{w}(x) $ je sekvence vıstupu sítì a $ y_{k}^{(t)} $ je aktivace vıstupní jednotky $ k $ v èase $ t $. Potom mùeme $ y_{k}^{(t)} $ interpretovat jako pravdìpodobnost pozorování labelu $ k $ v èase $ t $, které definuje distribution nad mnoinou $ L^{'T} $ sekvence délky $ T $ nad abecedou $ L^{'} = L \cup \{blank\} $:
$$
p(\pi \mid x) = \prod _{t=1}^{T} y_{\pi_{t}}^{(t)}, \forall \pi \in L^{'T}.
$$
Prvky $ L^{'T} $ budeme nazıvat cestami a budeme je oznaèovat $ \pi $. Z vıše uvedené rovnice (odkaz) je implicitní pøedpoklad, e vıstup sítì v rùznıch èasovıch okamicích je conditionally nezávislı, given the interní stav sítì. Toto je zaruèení tím, e neexistuje ádná zpìtná vazba z vıstupní vsrtvy sítì zpìt do sebe èí kamkoliv jinak v síti.

Dalším krokem je definece many-to-one zobrazení $ \mathcal{B}: L^{'T} \mapsto L^{\leq T} $, kde $ L^{\leq T} $ je mnoina monıch labbelingu, tj. mnoina sekvenci o délce menší èi rovné $ T $ nad pùvodní abecenou labelù $ L $). Toto zobrazení získáme jednodušše odstranìním prázdnıch labelù a labelù, co se opakují ze získanıch cest, napø. $ \mathcal{B}(a-ab-) = \mathcal{B}(-aa--abb) = aab $. Nakonec vyuijeme $ \mathcal{B} $ k definici podmínìné pravdìpodobnosti daného labellingu $ l \in L^{\leq T} $ jako sumu pravdìpodobností všech odpovídajících cest:
$$
p(l \mid x) = \sum_{\pi \in \mathcal{B}^{-1}(l)} p(\pi \mid x)
$$

CTC PAPER

Vzhledem k tomu, e ne pro všechny prvky vstupní sekvence dává smysl mapovat je na vıstup, zavádíme novı prázdnı token. Díky nim mùeme napø. ignorovat ticho, které není obsaeno ve vıstupu èi povolit nìkolik opakujících se znakù v øadì (aby z neexistuje nevzniklo nexistuje). Tento token neodpovídá ádnému znaku a z vısledné sekvence je odstranìn.

DISTILL PUB

\subsection{Konstrukce klasifikátoru}
Vıstupem klasifkátoru by mìlo bıt nejvíce pravdìpodobné olabelování pro vstupní sekvenci
$$
h(x) = \text{argmax}_{l \in L^{\leq T}} p(l \mid x)
$$
 (FIXNOUT ARGMIN a UNDERSCRIPT)

V terminologii HMM bychom nazvali tento proces hledání labellingu dekódováním. V tomto pøípadì se pro dekódování nabízí dvì aproximativní metody.

První metoda, dekódování nejlepší cesty (best path decoding), je zaloeno na pøedpodkladu, e nejpravdìpodobnìjší cesta odpovídá nejpravdìpodobnìjšímu labellingu:
$$
h(x) \approx \mathcal{B}(\pi^{*})
$$
kde
$$
\pi^{*} = \text{argmax}_{\pi \in N^{t}} p(\pi \mid x).
$$

Dekódování nejlepší cesty lze snadno vypoèítat, jeliko $ \pi^{*} $ je pouze konkatenace nejvíc aktivních vıstupù v kadém èasovém kroce. Nicménì není zaruèeno, e tato metoda nalezne nejlepší labelling.

Druhá metoda, prefix search decoding, sice zaruèuje nalezení nejlepšího labellingu, alej e vıpoèetnì nároènìjší a v jistıch pøípadech mùe její heurestika selhat. Tato metoda vyaduje úpravu dopøedného a zpìtného algortimu uvedeného dále a maximální poèet prefixù, které musí bıt expandovány roste exponencielnì s rostoucí délkou vstupní sekvence. Aby tato metoda byla upoèitatelná, vìtšinou je kombinována s dalšími heuristikami (napø. BEAM proøezávání).

CTC PAPER


\subsection{Trénování sítì}
Zatím jsme si uvedli, jak reprezentovat vıstup sítì tak, abychom mohli pouít RNN s CTC. Nyní si odvodíme objective function, která nám uméní natrénovat CTC sí pomocí metod zaloenıch na gradientnm sestupu. 

Objective function odvodíme ze základního principu maximum likelyhood, tedy e minimalizací této funkce maximalizujeme log likelyhood cíloveho olabelování. Váhy této sítì pak mùeme snadno spoèítat pomocí obyèejného zpìtnného šíøení v èase (BPTT).

CTC PAPER

CTC modely jsou vìšinou trénovány s vyuitím RNN k odhadu pravdìpodobností v kadém kroce $ p_{t}(a_{t} \mid X) $, která vìtšinu funguje velice dobøe díky vzhledem k tomu, e bere ohled i na kontext ve vstupní sekvenci. Nicménì je moné vyuít libovolnı algortimus, kterı dokáe vygenerovat hustotu pravdìpodobnosti nad vıstupními tøídami pro vstup o pevnì dané velikosti.

DIS|TILL PUB

\subsubsection{CTC zpìtnı a dopøednı algoritmus}
Potøebujeme efficient zpùsob, jak spoèítast podmínìnou pravdìpodobnost $ p(l \mid x) $ jednotlivıch labellingù. To je znaènì problematické, jeliko je potøeba spoèítat sumu pøes všechny cesty odpovídající danému labbelingu, kterıch mùe bıt velmi mnoho. Proto tento problém vyøešíme pomocí algoritmu dynamického programování, kterı je podobonı dopøednému-zpìtnému algoritmu pro HMM. Hlavním idea, jak spoèítat sumu všech cest odpovídajících labellingu je, e tato suma mùe bıt rozloena na iterativní sumu pøes cesty odpovídající prefixu toho labellingu. Iterace pak mohou bıt eficciently spoèteny pomocí rekurzivních dopøednıch a zpìtnıch promìnnıch.

Pro nìjakou sekevenci $ q $ délky $ r $ kde $ q_{1:p} $ a $ q_{r-p:r} $ oznaèuí prvních a posledních $ p $ symbolù. Potom pro labelling $ l $ zavedeme dopøednou promìnnou $ \alpha_{t}(s) $, která odpovídá celkové pravdìpodobnosti $ l_{1:s} $ v èase $ t $, tedy
$$
\alpha_{t}(s) = \sum_{\pi \in N^{T}: \mathcal{B}(\pi_{1:t})=l_{1:s}} \prod_{t^{'}=1}^{t} y_{\pi_{t^{'}}}^{t^{'}}
$$
kde $ \alpha_{t}(s) $ mùe bıt rekurzivnì spoètena z $ \alpha_{t-1}(s) $ a $ \alpha_{t-1}(s-1) $.

Abychom povolili vyuití prázdnıch labelù ve vıstupní cestì, pøedpokládejme modifikovanou sekvenci labelù $ l^{'} $, která vloí prázdnı label na zaèátek a na konec sekvence a mezi kadé dva labely. Délka sekvence $ l^{'} $ je tedy $ 2|l| + 1 $. Pøi vıpoètu pravdìpodobností prefixù $ l^{'} $ umoníme pøechody mezi prázdnımi a neprázdnımi labely a také mezi kadım párem unikátních neprázdnıch labelù. Dále umoníme (allow), aby všechny prefixy zaèínali buï prázdnıch symbolem $ b $ nebo prvním symbolem v $ l $. Tím získáváme inicializaèní hodnoty algoritmu
$$
\alpha_{1}(1) = y_{b}^{(1)}
$$
$$
\alpha_{1}(2) = y_{l_{1}}^{(1)}
$$
$$
\alpha_{1}(s) = 0, \forall s > 2
$$
a rekurzi
\[ 
\alpha_{t}(s) = \left\{
\begin{array}{ll}
     \bar{\alpha}_{t}(s) y_{l_{s}^{'}}^{t} & \text{pokud } l_{s}^{'} = b \text{ nebo } l_{s-2}^{'} = l_{s}^{'} \\
      (\bar{\alpha}_{t}(s) + \alpha_{t-1}(s-2)) y_{l_{s}^{'}}^{t} & \text{jinak}
\end{array} 
\right. 
\]
kde
$$
\bar{\alpha}_{t}(s) = \alpha_{t-1}(s) + \alpha_{t-1}(s-1)
$$.

Pravdìpodobnost $ l $ je potom suma vıslednıch pravdìpodobností $ l^{'} $ s a bez finální prázdného labelu v èase $ T $
$$
p(l \mid x) = \alpha_{T}(|l^{'}|) + \alpha_{T}(|l^{'}| - 1)
$$

Obdobnì zadefinujeme zpìtnou promìnnou $ \beta_{t}(s) $ jako vıslednou pravdìpodobnsot $ l_{s:|l|} $ v èase $ t $:
$$
\beta_{t}(s) = \sum_{\pi \in N^{T}: \mathcal{B}(\pi_{t:T})=l_{s:|l|}} \prod_{t^{'}=t}^{T} y_{\pi_{t^{'}}}^{t^{'}}
$$
$$
\beta_{T}(|l^{'}|) = y_{b}^{(T)}
$$
$$
\beta_{T}(|l^{'}| - 1) = y_{l_{l}}^{(T)}
$$
$$
\beta_{T}(s) = 0, \forall s < |l^{'}| - 1
$$
\[ 
\beta_{t}(s) = \left\{
\begin{array}{ll}
     \bar{\beta}_{t}(s) y_{l_{s}^{'}}^{t} & \text{pokud } l_{s}^{'} = b \text{ nebo } l_{s+2}^{'} = l_{s}^{'} \\
      (\bar{\beta}_{t}(s) + \beta_{t+1}(s+2)) y_{l_{s}^{'}}^{t} & \text{jinak}
\end{array} 
\right. 
\]
kde
$$
\bar{\beta}_{t}(s) = \beta_{t+1}(s) + \beta_{t+1}(s+1)
$$.

OBRAZEK Z CTC PAPERU

Aby pøi vıpoèty nedošlo k podteèení a byla zajištìna numerická stabilita, zadefinujme si pøeškálované dopøedné a zpìtné promìnné
$$
\hat{\alpha}_{t}(s) = \dfrac{\alpha_{t}(s)}{\sum_{s} \alpha_{t}(s)}
$$
$$
\hat{\beta}_{t}(s) = \dfrac{\beta_{t}(s)}{\sum_{s} \beta_{t}(s)}
$$
a nahraïmì $ \alpha $ za $ \hat{\alpha} $ v rovnicích (odkaz na tu s vidlièkou a tu pod ní) a $ \beta $ za $ \hat{\beta} $ v rovnicích (odkaz na tu s vidlièkou a tu pod ní).

CTC PAPER


Dosazením vzorcù a úpravou vzorcù  (podrobnìjí v [odkaz na CTC paper]) pak mùeme  sí natrénovat na základì maximum likelihood, kde zpìtné šíøení gradientu skrz softmax vrstvu je definování následovnì
$$
\dfrac{\partial O^{ML} (\{x,z\},\mathcal{N}_{w})}{\partial u_{k}^{(t)}} = y_{k}^{(t)} - \dfrac{1}{_{k}^{(t)} Z_{(t)}} \sum_{s \in lab(z,k)} \hat{\alpha}_{t}(s) \hat{\beta}_{t}(s)
$$
(POROVNAT ZAVORKY u t)

, kde $ u_{k}^{(t)} $ jsou nenormalizované vıstupy a $ lab(z,k) = lab(l, k) = \{ s : l_{s}^{'} = k \} $  je mnoina pozic  a kterı se v labellingu $ l $ vyskytuje label $ k $, a kde
$$
Z_{t} = \sum_{s=1}^{|l^{'}|} \dfrac{\hat{\alpha}_{t}(s) \hat{\beta}_{t}(s)}{y_{l_{s}^{'}}^{t} }
$$
CTC PAPER

Vıpoèet CTC mùe bıt vıpoèetnì velice nároènı. Naivním pøístupem pro vıpoèert by bylo seèíst skóre všech zarovnání, nicménì poèet zarovnání mùe bıt velmi velkı. Vıpoèet tedy vyøešíme algortimem dynamického programování. Hlavní ideou je, e dvì zarovnání, která dosáhla stejné vıstupu v daném èasovém okamice mohou bıt spojeny.

CTC funkce je diferencovatelná vzhledem k pravdìpodobnostem v kadém èasosvém kroku a lze pouít gradientní metody.

Lze vyuít BEAM search  (trade-off mezi accuracy a runtime) a language model modifikací klasifikátoru
$$
Y^{*} = \text{argmax } p(y\mid X) \cdot p(Y)^{\gamma} \cdot L(Y)^{\beta}
$$,
kde  $ p(y\mid X) $ je podmínìná CTC pravdìpodobnost, $ p(Y)^{\gamma}  $ je pravdìpodobnost daná jazykovıım modelem  a $  L(Y)^{\zeta} $ a je penalizace za vloení slova. Parametry $ \gamma $ a $ \zeta $ jsou vìtšinou získány pomocí crossvalidace.

DISTILL PUB

PREPSAT DO TVARU $ \alpha_{s}^{(t)} $?

\section{Klasifikace fonémù}
\section{Vyhodnocení}
\section{Závìr}














































\section{Úvod}
V posledních letech došlo k vıznamnému vıvoji v oblasti mobilních zaøízení a s tím i k nárùstu popularity hlasového ovládání. Aplikace spoleènosti Google nabízejí monost vyhledávání hlasem, Apple a Microsoft vytvoøili inteligentní osobní asistentky Siri, resp. Cortana a stále èastìji se hlasové ovládání zaèíná objevovat i v automobilovém prùmyslu.

Jedním z øešenıch problémù hlasového ovládání je detekce klíèovıch frází  v proudu øeèi (KWS - Keyword spotting). Na pozadí zaøízení nepøetritì bìí KWS systém, kterı naslouchá mluvenému slovu a pøi zaznìní klíèové fráze provede urèitou akci. V dnešní dobì se nejèastìji pouívají dva typy KWS systémù - systémy s pøeddefinovanımi klíèovımi frázemi a systémy vyuívající obsáhlı slovník øeèi v textové podobì (LVCSR - Large vocabulary continuous speech recognition). LVCSR systémy jsou vıpoèetnì velmi nároèné, jeliko je potøeba nejprve pøevést mluvenou øeè do textové podoby a následnì vyhledat klíèovou frázi v databázi. Vzhledem k tomu, e tyto systémy neumoòují vytvoøení vlastní bezpeèné klíèové fráze, propojují se vìtšinou se systémem pro identifikaci øeèníka\cite{lvcsr}.

Bylo by tedy vhodné vytvoøit KWS systém, kterı by nebyl závislı na pøedem definovanıch frázích a umonil by uivateli volbu vlastních klíèovıch frází v libovolném jazyce. Také by mìl bıt vıpoèetnì nenároènı, aby byla zajištìna odezva v reálném èase na mobilních zaøízeních. Tyto poadavky splòuje metoda Query-by-Example, kdy si uivatel vytvoøí vzorovou klíèovou frázi a všechny budoucí promluvy jsou porovnávány vùèi ní\cite{chen,hazen}.

Práce se vìnuje zpracování akustického signálu a pøedevším problematice rozpoznání izolovanıch slov. Uvedeme si nejèastìji vyuívané algoritmy strojového uèení a porovnáme jejich rozpoznávací schopnosti a jejich vhodnost potenciálního vyuití v KWS systému.

\newpage
\section{Klasifikace}
Klasifikace neboli rozpoznávání je úloha, kde je cílem správnì zaøadit objekt do jedné z~$ k $ tøíd. Aby bylo moné objekt klasifikovat, je tøeba ho nejprve vhodnì popsat pomocí tzv. pøíznakového vektoru (obrazu). Pøíznakovı vektor se skládá z pøíznakù, které reprezentují jednotlivé mìøitelné èi pozorovatelné vlastnosti objektu. Pøíznaky je tøeba volit v závislosti na øešeném problému tak, aby dostateènì popisovaly pozorovanı objekt. Mùe se zdát, e vytvoøením pøíznakového vektoru ze všech mìøitelnıch velièin objektu získáme dokonalı popis objektu, kterı pøispìje k pøesnosti klasifikace. Opak je však pravdou, jeliko velké mnoství pøíznakù s nedostateènou informativní hodnotou mùe mít negativní vliv na pøesnost klasifikace a vést k pøetrénování modelu\cite{psutka1}.

Algoritmus, kterı provádí klasifikaci, se nazıvá klasifikátor. Tato práce se zabıvá pøedevším klasifikátory zaloenımi na uèení s uèitelem a klasifikátoru podle minimální vzdálenosti k vzorovım obrazùm.

Uèící se klasifikátor pracuje ve dvou fázích - trénovací fáze a klasifikaèní fáze:
\begin{enumerate}
	\item \textbf{trénovací fáze} - v trénovací fází jsou klasifikátoru pøedkládány dvojice $ \lbrace x_{i}, y_{i}\rbrace $, $ i = 1, 2, \dots, l $, kde $ x_{i} $ jsou pøíznakové vektory a $ y_{i} $ jsou cílové tøídy jednotlivıch obrazù z trénovací mnoiny. Klasifikátor vypoète svùj vıstup pro pøíznakovı vektor $ x_{i} $ (odhadovaná tøída) a porovná jej s cílovou tøídou. V pøípadì neshody odhadované a cílové tøídy upraví své parametry tak, aby minimalizoval danou chybovou funkci (jedná se tedy o optimalizaèní úlohu). Trénovací dvojice jsou klasifikátoru pøedkládány tak dlouho, dokud nedosáhne optimálního nastavení.
	\item \textbf{klasifikaèní fáze} - klasifikátoru pøedkládáme neznámé obrazy a na základì \newline natrénovanıch parametrù klasifikujeme do jedné z $ k $ tøíd\cite{alpaydin,duda}.
\end{enumerate}

Pøi trénování dochází velmi èasto k tzv. pøetrénování modelu (overfitting). Klasifikátor se nauèí dokonale rozpoznávat pozorování z trénovací mnoiny, ale ztrácí schopnost generalizace a nová, neznámá pozorování klasifikuje chybnì. Overfitting je zpùsoben volbou pøíliš komplexního modelu, nedostatkem trénovacích dat nebo vysokou dimenzí obrazù. Komplexitu modelu je tedy tøeba volit s ohledem na povahu klasifikovanıch dat, popø. lze vyuít jednu z metod, která overfitting omezí (regularizace, cross-validation, dropout vrstvy u neuronovıch sítí, atd.)\cite{abu-mostafa}. Opaènım pøípadem overfittingu je tzv. underfitting, kdy model na úkor generalizace ztrácí klasifikaèní schopnosti. Vliv volby modelu s rùznım poètem stupòù volnosti mùeme vidìt na obrázku \ref{fig:overfitting}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{overfitting.eps}
    \caption{Pøíklad overfittingu a underfittingu.}
    \label{fig:overfitting}
\end{figure}

\newpage
\section{Zpracování akustického signálu}
Základním krokem pro klasifikaci øeèi je samotné zpracování akustickıch signálù a jejich transformace na pøíznakové vektory. Pøi zpracování akustického signálu pøedpokládáme, e se jeho vlastnosti mìní pomalu a na krátkıch úsecích je témìø stacionární (v èasové i~frekvenèní oblasti). Signál je tedy rozdìlen na kratší úseky neboli mikrosegmenty, které jsou zpracovány samostatnì, jako by se jednalo o rùzné signály. Pro kadı mikrosegment pak vypoèteme èíslo (pøíznak), popøípadì vektor èísel na základì zvolené metriky. Délka mikrosegmentù se nejèastìji volí mezi 10 a 25ms a jednotlivé mikrosegmenty na sebe mohou navazovat nebo se i pøekrıvat. Díky tomu lze celı signál popsat posloupností èísel (pøíznakovı vektor), její délka je pøímo úmìrná délce slova a poètu mikrosegmentù\cite{psutka1}.

\subsection{Okénková funkce}
Jednotlivé mikrosegmenty jsou ze signálu vybírány pomocí okénka o urèité délce, které se posouvá o danı poèet vzorkù signálu. Okénko také slouí k pøidìlení vah zpracovávanım vzorkùm signálu. Mezi nejèastìji pouívané okénkové funkce pøi zpracování øeèi patøí pravoúhlé a Hammingovo okénko:
\begin{itemize}
	\item \textbf{pravoúhlé okénko} - pravoúhlé okénko pøidìlí všem vzorkùm mikrosegmentu stejnou váhu. Lze jej definovat vztahem
\begin{equation}
w(n) =
\begin{cases}
1 & \quad \text{pro } 0 \leq n \leq N-1 \\
0 & \quad \text{jinak,}
\end{cases}
\end{equation}
kde $ N $ je poèet vzorkù mikrosegmentu.
	\item \textbf{Hammingovo okénko} - Hammingovo okénko je vhodné pouít v pøípadì, kdy je tøeba potlaèit vzorky na krajích zpracovávaného mikrosegmentu\cite{psutka1, scipy}. Lze jej definovat vztahem
\begin{equation}
w(n) =
\begin{cases}
0.54 - 0.46\cos\left( \dfrac{2\pi n}{N-1} \right) & \quad \text{pro } 0 \leq n \leq N-1 \\
0 & \quad \text{jinak.}
\end{cases}
\end{equation}
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{window.eps}
    \caption{Aplikace Hammingova okénka na funkci $ f(x) = \sin (10\pi x) $.}
    \label{fig:window}
\end{figure}

\subsection{Pøíznaky v èasové a frekvenèní oblasti}
\subsubsection{Krátkodobá energie signálu}
Funkce krátkodobé energie signálu je definována vztahem
\begin{equation}
E_{n} = \sum_{k=-\infty}^{\infty} \left[ s(k)w(n-k)\right]^2,
\end{equation}
kde $ s(k) $ je vzorek signálu v èase $ k $ a $ w(n) $ je danı typ okénka. 
Hodnoty této funkce poskytují informaci o prùmìrné hodnotì energie v kadém mikrosegmentu signálu. Hlavním nedostatkem funkce krátkodobé energie signálu je její vysoká citlivost na velké vıkyvy v~amplitudì signálu.

\subsubsection{Krátkodobá intenzita signálu}
Funkce krátkodobé intenzity signálu je definována vztahem
\begin{equation}
I_{n} = \sum_{k=-\infty}^{\infty} |s(k)|w(n-k).
\end{equation}
Na rozdíl od funkce krátkodobé energie signálu není tato funkce citlivá na velké zmìny amplitudy signálu. 

\subsubsection{Krátkodobé prùchody nulou}
Funkce krátkodobıch prùchodù nulou je definována vztahem
\begin{equation}
Z_{n} = \dfrac{1}{2} \sum_{k=-\infty}^{\infty} |sign\left[ s(k) \right] - sign\left[ s(k-1) \right]|w(n-k).
\end{equation}
Funkce krátkodobıch prùchodù nulou nese informaci o frekvenci signálu - èím více prùchodù nulou, tím vyšší je v daném úseku frekvence signálu a naopak. Frekvenci prùchodù signálu nulovou úrovní mùeme tedy vyuít jako jednoduchou charakteristiku, která popisuje spektrální vlastnosti signálu. Hodnoty této funkce se nejèastìji vyuívají k detekci øeèi v~akustickém signálu (urèení zaèátku a konce promluvy)\cite{psutka1, psutka2}.

\subsubsection{Mel-frekvenèní kepstrální koeficienty}
Nejèastìji pouívanım typem pøíznakù v rozpoznávání øeèi jsou mel-frekvenèní kepstrální koeficienty (MFCC). Jedná se o velice robustní typ pøíznakù ve frekvenèní oblasti, kterı respektuje nelineární vlastnosti vnímání zvukù lidskım uchem. Lineární frekvence $ f $[Hz] je pøevedena na frekvenci $ f_{m} $[mel] v nelineární melovské frekvenèní škále
\begin{equation}
f_{m} = 2595 \log_{10} \left( \dfrac{f}{700} \right).
\end{equation}

Pøi vıpoètu MFCC se nejèastìji volí segmentace signálu na mikrosegmenty o délce 10 a 30ms, na které se aplikuje Hammingovo okénko s posunem o 10ms. Segmentovanı signál je následovnì zpracován rychlou Fourierovou transformací (FFT), díky které získáme amplitudové spektrum $ |S(f)| $ analyzovaného signálu. Vzhledem k pouití FFT je vhodné volit poèet vzorkù okénka pøi dané frekvenci roven mocninì 2.

Dalším krokem vıpoètu je melovská filtrace, pøi ní vyuijeme banku trojúhelníkovıch pásmovıch filtrù. Jednotlivé trojúhelníkové filtry jsou rozloeny pøes celé frekvenèní pásmo on nuly a do Nyquistovy frekvence a jejich støední frekvence jsou rovnomìrnì rozloeny podél frekvenèní osy v melovské škále. Støední hodnoty filtrù $ b_{m}$ lze vyjádøit vztahem
\begin{equation}
b_{m,i} = b_{m,i-1} + \Delta_{m},
\end{equation}
kde $ i = 1, 2,\dots,M^{*} $ je poèet trojúhelníkovıch filtrù v bance, $ b_{m,0} = 0 $ mel a $ \Delta_{m} = B_{m,w} / (M^{*} + 1)$.

Dále je tøeba vypoèítat odezvy všech filtrù, èeho dosáhneme jejich vyjádøením ve frekvenèní škále s mìøítkem v herzích za vyuití pùvodních koeficientù získanıch FFT. Pøepoèteme tedy všechny støední frekvence v melovské škále $ b_{m,i}, i=1,\dots ,M^{*}+1 $ pomocí inverzního vztahu $ f = 700[exp(0.887\cdot 10^{-3}f_{m})-1] $ na støední frekvence $ b_{i}, i=1,\dots ,M^{*}+1 $. Nyní mùeme trojúhelníkové filtry vyjádøit vztahem
\begin{equation}
u(f, i) =
\begin{cases}
\dfrac{1}{b_{i}-b_{i-1}}(f-b_{i-1}) & \quad \text{pro } b_{i-1} \leq f < b_{i} \\
\dfrac{1}{b_{i}-b_{i+1}}(f-b_{i+1}) & \quad \text{pro } b_{i} \leq f < b_{i+1} \\
0 & \quad \text{jinak}
\end{cases}
\end{equation}
a odezvy jednotlivıch filtrù $ y_{m}(i) $ vztahem
\begin{equation}
y_{m}(i) = \sum_{f=b_{i-1}}^{b_{i+1}} |S(f)|u(f,i),
\end{equation}
kde $ i = 1,2,\dots ,M^{*} $ a $ f $ jsou frekvence vyuité pøi vıpoètu FFT. Pøi prùchodu signálu filtrem je kadı koeficient FFT násoben odpovídajícím ziskem filtru a vısledky pro jednotlivé filtry jsou akumulovány a následnì zlogaritmovány. Tím dojde k dekorelaci energií filtrù a získané hodnoty lze pouít jako plnohodnotné pøíznaky.

Nakonec provedeme zpìtnou diskrétní kosinovou transformaci (DCT). Znaènou vıhodou DCT je vysoká nekorelovanost vzniklıch koeficientù. Koeficienty jsou dány vztahem
\begin{equation}
c_{m}(j) = \sum_{i=1}^{M^{*}}\log y_{m}(i) \cos \left( \dfrac{\pi j}{M^{*}} (i-0.5) \right) \quad \text{pro } j=0,1,\dots,M,
\end{equation}
kde $ M $ je poèet mel-frekvenèních kepstrálních koeficientù\cite{psutka2,mfcc}.

\subsection{Delta a delta-delta koeficienty}
Pøi vıpoètu pøíznakovıch vektorù pøedpokládáme, e signál je na krátkém úseku stacionární. Je tedy zøejmé, e tyto pøíznakové vektory budou popisovat pouze statické vlastnosti signálu a dynamické vlastnosti se ztrácí. Tento nedostatek mùeme vyøešit rozšíøením pøíznakovıch vektorù o dynamické koeficienty.

Vyuívají se delta (diferenèní) koeficienty a delta-delta (akceleraèní) koeficienty, které odpovídají první, respektive druhé derivaci pøíznakového vektoru. Delta koeficienty lze definovat vztahem
\begin{equation}
d_{t} = \dfrac{\sum_{n=1}^{N} n(c_{t+n} - c_{t-n})}{2\sum_{n=1}^{N}n^{2}},
\end{equation}
kde $ d_{t} $ je delta koeficient v èase $ t $ a $ N $ je volitelnı parametr. Tento parametr se vìtšinou volí $ N=1 $ nebo $ N=2 $ a urèuje, z kolika sousedních mikrosegmentù budou vypoèítány delta koeficienty (jeden, resp. dva leví a praví sousedé mikrosegmentu). Delta-delta koeficienty lze spoèítat vyuitím stejného vztahu s tím rozdílem, e se nepoèítají ze statickıch koeficientù, ale z delta koeficientù\cite{psutka2,mfcc}.

\subsection{Normalizace pøíznakù}
Jednotlivé pøíznaky se mohou pohybovat na rùznıch definièních oborech hodnot a~pøi klasifikaèních úlohách se mohou projevit s rùznou váhou. Proto je vhodné pøíznaky transformovat na stejnı definièní obor hodnot, popø. do rozdìlení o stejnıch parametrech, a tím zaruèit, e všechny pøíznaky budou mít stejnı vliv. Tuto transformaci nazıváme normalizací dat a v praxi se nejèastìji pouívají následující dvì metody:
\begin{itemize}
 \item \textbf{Z-score normalizace} - tato metoda se pouívá v pøípadì, kdy data odpovídají normálnímu rozloení. Z-score normalizací pøetransformujeme vstupní data $ x$, na data $ z $, které odpovídají normálnímu rozdìlení o støední hodnotì $ \mu = 0 $ s rozptylem $ \sigma = 1 $.
\begin{equation}
z = \dfrac{x-\mu}{\sigma}
\end{equation}
 \item \textbf{Min-Max normalizace} - Min-Max normalizace pøeškáluje data do definovaného intervalu (nejèastìji $ [0, 1] $ nebo $ [-1, 1] $, popø. $ [0, 255] $ pøi zpracování obrazu). Tím dosáhneme menšího rozptylu a eliminujeme vliv odlehlıch bodù (tzv. outliers).
\begin{equation}
x_{norm} = \dfrac{x - x_{min}}{x_{max} - x_{min}}
\end{equation}
\end{itemize}

Nìkteré klasifikaèní algoritmy pøímo vyadují, aby data byla normalizována. Pøíkladem mohou bıt algoritmy zaloené na gradientních metodách (logistická regrese, SVM, neuronové sítì, atd.), kdy jsou zmìny parametrù modelu pøi trénování pøímo závislé na vstupním pøíznakovém vektoru. Pøi velkıch rozdílech definièních oborù jednotlivıch pøíznakù se tedy nìkteré parametry mohou mìnit vıraznì rychleji ne ostatní. Normalizací dat zajistíme rychlejší a stabilnìjší konvergenci parametrù. Dalším pøíkladem mohou bıt shlukovací algoritmy pracující s Euklidovou vzdáleností, kde je vhodné, aby všechny pøíznaky mìly na shlukování stejnı vliv\cite{raschka}.


\newpage
\section{Support Vector Machine}
Pøedpokládejme, e máme trénovací mnoinu $ \lbrace x_{i}, y_{i}\rbrace $, $ i = 1, 2, \dots, l $, $ y_{i} \in \lbrace -1, 1 \rbrace $, která je sloena z pøíznakovıch vektorù $ x_{i} $ a odpovídajících cílovıch tøíd
$ y_{i} $ (binární klasifikace). Pro tuto mnoinu si odvodíme lineární klasifikátor zaloenı na podpùrnıch vektorech pro separabilní a neseparabilní pøípad a nelineární klasifikátor. Nakonec si uvedeme metody, jak lze klasifikovat do více tøíd\cite{burges,svec}.

\subsection{Nadrovina}
Pro odvození klasifikátoru SVM (a pozdìji i pro odvození neuronové sítì) je vhodné nejprve zavést pojem nadrovina. Pøedpokládejme tedy, e máme $ p $-dimenzionální prostor. Nadrovinou pak rozumíme libovolnı afinní podprostor o dimenzi $ p-1 $. Napøíklad ve dvoudimenzionálním prostoru je nadrovinou jednodimenzionální podprostor - pøímka, ve tøídimenzionálním prostoru je nadrovinou plocha. Nadrovinu o dimenzi $ p $ lze definovat následovnì
\begin{equation} \label{eq:hyperplane}
b + w_{1}x_{1} + w_{2}x_{2} + \dots w_{p}x_{p} = 0,
\end{equation}
kde $ x = (x_{1}, x_{2}, \dots, x_{p})^{T} $ je bod v $ p $-dimenzionálním prostoru vyhovující rovnici. Bod $ x $ tedy leí na nadrovinì.

V pøípadì, e bod $ x $ nevyhovuje rovnici \ref{eq:hyperplane}, tedy
\begin{equation}
b + w_{1}x_{1} + w_{2}x_{2} + \dots w_{p}x_{p} > 0,
\end{equation}
bod leí na jedné stranì poloroviny, resp. pokud
\begin{equation}
b + w_{1}x_{1} + w_{2}x_{2} + \dots w_{p}x_{p} < 0,
\end{equation}
bod leí na druhé stranì poloroviny. Nadrovina tedy dìlí $ p $-dimenzionální prostor na dvì poloviny, tzv. poloprostory. Náleitost bodu do tìchto poloprostorù pak mùeme zjistit jednoduchım vıpoètem znaménka levé strany rovnice \ref{eq:hyperplane}\cite{james}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{hyperplane.eps}
    \caption{Nadrovina oddìlující body ve dvoudimenzionálním prostoru\cite{sklearn}.}
    \label{fig:hyperplane}
\end{figure}

\subsection{Lineárnì separabilní pøípad}
Pøedpokládejme, e existuje nadrovina, která oddìlí body trénovací mnoiny jednotlivıch tøíd od sebe (oddìlující nadrovina). Body $ x $, které leí na této rovinì splòují rovnici $ w\cdot x + b = 0 $, kde $ w $ je normála nadroviny. Dále si definujme kolmou vzdálenost oddìlující nadroviny k poèátku jako $ \frac{|b|}{||w||} $, kde $ ||w|| $ je Euklidovská norma $ w $.

Dále si zaveïme nejkratší vzdálenost $ d_{+} $ ($ d_{-} $) mezi oddìlující rovinou a pozitivním (negativním) pøíkladem a tzv. margin (odstup) jako $ d_{+} + d_{-} $. Pro lineárnì separabilní pøípad hledá klasifikátor takovou nadrovinu, pro kterou nabıvá margin nejvyšší hodnoty. Všechny body trénovací mnoiny tedy splòují tyto podmínky
\begin{align}
x_{i} \cdot w + b &\geq +1 \quad \text{pro } y_{i}=+1, \label{eq:h1}\\
x_{i} \cdot w + b &\leq -1 \quad \text{pro } y_{i}=-1, \label{eq:h2}
\end{align}
které lze slouèit do jedné mnoiny nerovností
\begin{equation}
y_{i}(x_{i} \cdot w + b) - 1 \geq 0 \quad \forall i. \label{eq:h_united}
\end{equation}

Body, pro které v nerovnici \ref{eq:h1} platí rovnost, leí na nadrovinì $ H_{1}: x_{i} \cdot w + b = +1 $ a jejich kolmou vzdálenost vùèi poèátku lze vyjádøit jako $ \frac{|1-b|}{||w||} $. Obdobnì body, pro které v nerovnici \ref{eq:h2} platí rovnost, leí na nadrovinì $ H_{2}: x_{i} \cdot w + b = -1 $ v kolmé vzdálenosti od poèátku $ \frac{|-1-b|}{||w||} $. Z toho plyne, e $ d_{+} = d_{-} = \frac{1}{||w||} $ a margin je roven $ \frac{2}{||w||} $. Nadroviny $ H_{1} $ a $ H_{2} $ mají stejnou normálu $ w $ (jsou rovnobìné) a nenachází se mezi nimi ádnı bod z trénovací mnoiny. Nalezení takovıch nadrovin, které maximalizují margin, provedeme minimalizací $ ||w||^{2} $ za respektování omezujících podmínek.

Body leící na nadrovinách $ H_{1} $ a $ H_{2} $ se nazıvají podpùrné vektory (support vectors, zvıraznìné body na obrázku \ref{fig:svm_hyperplane}) a oddìlující rovina je na nich pøímo závislá. Pokud dojde k jejich pohybu nebo odstranìní, zmìní se i vısledné øešení.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{svm_hyperplane.eps}
    \caption{Vizualizace podpùrnıch vektorù\cite{sklearn}.}
    \label{fig:svm_hyperplane}
\end{figure}

Formulujme si nyní tento problém pomocí Lagrangeovıch multiplikátorù $ \alpha_{i}$, $ i=1, 2, \dots ,l $ - jeden pro kadou nerovnost \ref{eq:h_united}, neboli jeden pro kadı prvek trénovací mnoiny $  \lbrace x_{i}, y_{i}\rbrace $. Dostáváme Lagrangeovu funkci
\begin{equation}
L_{P} \equiv \dfrac{1}{2}||w||^{2} - \sum_{i=1}^{l}\alpha_{i}y_{i}(x_{i}\cdot w + b) + \sum_{i=1}^{l}\alpha_{i}. \label{eq:lagrangian1}
\end{equation}
Nyní staèí minimalizovat $ L_{P} $ podle $ w $ a $ b $ a zároveò poadujeme, aby $ \frac{\partial L_{P}}{\partial \alpha_{i}} = 0 $, $ \alpha_{i} \geq 0 $ (oznaème si tuto mnoinu omezujících podmínek $ \mathcal{C}_{1} $). Vzhledem k tomu, e se jedná o úlohu konvexního kvadratického programování, mùeme ekvivalentnì øešit duální problém: maximalizace $ L_{P} $ za omezujících podmínek $ \frac{\partial L_{P}}{\partial w} = 0 $ a $ \frac{\partial L_{P}}{\partial b} = 0 $ a zároveò $ \alpha_{i} \geq 0 $ (oznaème tuto mnoinu omezujících podmínek jako $ \mathcal{C}_{2} $). Tato duální formulace problému má tu vlastnost, e maximum $ L_{P} $ za podmínek $ \mathcal{C}_{2} $ nastává pøi stejnıch hodnotách  $ w $, $ b $ a $ \alpha $ jako minimum $ L_{P} $ za podmínek $ \mathcal{C}_{1} $.

Omezeními gradientu $ \frac{\partial L_{P}}{\partial w} = 0 $ a $ \frac{\partial L_{P}}{\partial b} = 0 $ získáme rovnice
\begin{align}
&w = \sum_{i} \alpha_{i} y_{i} x_{i}, \label{eq:sum1}\\
&\sum_{i} \alpha_{i} y_{i} = 0. \label{eq:sum2}
\end{align}

Dosazením rovnic \ref{eq:sum1} a \ref{eq:sum2} do \ref{eq:lagrangian1} dostaneme
\begin{equation}
L_{D} = \sum_{i} \alpha_{i} - \dfrac{1}{2} \sum_{i,j} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i} \cdot x_{j}.
\end{equation}

Po vyøešení optimalizaèní úlohy (trénovací fáze) mùeme klasifikovat libovolnı vektor $ x $ na základì toho, na které stranì oddìlující nadroviny leí. Tøída vektoru $ x $ tedy bude $ \text{sgn}(w\cdot x + b) $\cite{burges}.

\subsection{Lineárnì neseparabilní pøípad}
Pokud pouijeme vıše zmínìnı algoritmus na lineárnì neseparabilní data, nenalezneme ádné pøijatelné øešení, jeliko duální Langrangeova funkce $ L_{D} $ poroste nade všechny meze. Chceme-li algoritmus rozšíøit i pro neseparabilní data, musíme uvolnit podmínky \ref{eq:h1} a \ref{eq:h2}. Toho dosáhneme zavedením volnıch (slack) promìnnıch $ \varepsilon_{i} $, $ i=1,2,\dots,l $
\begin{align}
&x_{i} \cdot w + b \geq +1 - \varepsilon_{i} \quad \text{pro } y_{i}=+1 \\
&x_{i} \cdot w + b \leq -1 + \varepsilon_{i} \quad \text{pro } y_{i}=-1 \\
&\varepsilon_{i} \geq 0 \quad \forall i.
\end{align}
Chyba klasifikace tedy nastane v pøípadì, e $ \varepsilon > 1 $. Horní mez poètu chyb klasifikace prvkù trénovací mnoiny je tedy $ \sum_{i} \varepsilon_{i} $. Vhodnım zpùsobem, jak navıšit hodnotu kriteriální funkce za kadou chybu, je minimalizovat $ \frac{||w||^{2}}{2} + C(\sum_{i}\varepsilon_{i})^{k} $ místo pùvodního kritéria $ \frac{||w||^{2}}{2} $. Parametr $ C $ se volí a odpovídá penalizaci za chybnou klasifikaci - èím vyšší hodnota $ C $, tím vìtší penalizace. Pro $ k > 0 $, $ k \in \mathbb{Z} $ se jedná o úlohu konvexního programování, pro $ k = 2 $ a $ k = 1 $ se jedná pøímo o úlohu kvadratického programování. Volbou $ k = 1 $ navíc zajistíme, e $ \varepsilon_{i} $ ani jejich multiplikátory se neobjeví v definici duálního problému
\begin{equation}
L_{D} \equiv \sum_{i} \alpha_{i} - \dfrac{1}{2}\sum_{i,j}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}\cdot x_{j} \label{eq:lagrangian2}
\end{equation}
za podmínek
\begin{align}
0 \leq \alpha_{i} \leq C, \label{eq:cond1}\\
\sum_{i}\alpha_{i}y_{i} = 0. \label{eq:cond2}
\end{align}
Øešením je pak
\begin{equation}
w = \sum_{i=1}^{N_{S}}\alpha_{i}y_{i}x_{i},
\end{equation}
kde $ N_{S} $ je poèet podpùrnıch vektorù.

Primární úloha je dána Lagrangeovou funkcí
\begin{equation}
L_{P} = \dfrac{1}{2}||w||^{2} + C\sum_{i}\varepsilon_{i} - \sum_{i}\alpha_{i}\lbrace y_{i}(x_{i}\cdot w + b) - 1 + \varepsilon_{i} \rbrace - \sum_{i}\mu_{i}\epsilon_{i}, \label{eq:lagrangian3}
\end{equation}
kde $ \mu_{i} $ jsou Lagrangeovské multiplikátory zajištující kladnost $ \varepsilon_{i} $. Øešitelnost \ref{eq:lagrangian3} je dána Karush-Kuhn-Tucker podmínkami (více v \cite{burges}). Pro vıpoèet prahu $ b $ postaèí pouze dvì z~Karush-Kuhn-Tucker podmínek
\begin{align}
\alpha_{i}\lbrace y_{i}(x_{i}\cdot w + b) - 1 + \varepsilon_{i} \rbrace = 0,\\
\mu_{i}\epsilon_{i} = 0.
\end{align}

Aèkoliv k vıpoètu prahu $ b $ staèí znát pouze jeden prvek trénovací mnoiny splòující podmínku $ 0 < \alpha_{i} < C $ a zároveò $ \varepsilon_{i} = 0 $, z numerického hlediska je rozumnìjší urèit vıslednı práh jako prùmìr prahù pøes všechny prvky trénovací mnoiny\cite{burges}.

\subsection{Nelineárnì separabilní pøípad}
Uvaujme nyní pøípad, kdy oddìlující nadrovina není lineární funkcí trénovacích dat - potøebujeme tedy zobecnit rovnice \ref{eq:lagrangian2}, \ref{eq:cond1} a \ref{eq:cond2} pro nelineární oddìlující nadrovinu. Všimnìme si, e v tìchto rovnicích se data trénovací mnoiny vyskytují vdy jako skalární souèin $ x_{i} \cdot x_{j} $. Pøedpokládejme, e existuje zobrazení $ \Phi $ z $ n $-dimenzionálního prostoru do jiného Euklidovského prostoru $ \mathcal{H} $ o vyšší dimenzi (a nekoneènìdimenzionálního)
\begin{equation}
\Phi: \mathbb{R}^{n} \mapsto \mathcal{H}.
\end{equation}
Potom by trénovací algoritmus závisel pouze na skalárních souèinech $ \Phi (x_{i}) \cdot \Phi (x_{j}) $ v~prostoru $ \mathcal{H} $. Definujme si tedy jádrovou funkci (kernel function) $ K(x_{i},x_{j}) = \Phi (x_{i}) \cdot \Phi (x_{j}) $, kterou mùeme nahradit skalární souèin $ x_{i} \cdot x_{j} $ v trénovacím algoritmu \ref{eq:lagrangian2} a opìt dopoèítat normálu $ w $ oddìlující nadroviny. Nemusíme tedy explicitnì znát zobrazení $ \Phi $, pouze jádrovou funkci.

Klasifikaci libovolného bodu $ x $ provedeme vıpoètem znaménka funkce $ f(x) $
\begin{equation}
f(x) = \sum_{i=1}^{N_{S}}\alpha_{i}y_{i}\Phi (s_{i})\cdot\Phi (x) + b = \sum_{i=1}^{N_{S}}\alpha_{i}y_{i}K(s_{i},x) + b, \label{eq:svm}
\end{equation}
kde $ s_{i} $ jsou podpùrné vektory\cite{burges,friedman}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{svm_kernels.eps}
    \caption{SVM s lineární a polynomiální jádrovou funkcí\cite{sklearn}.}
    \label{fig:svm_kernels}
\end{figure}

\subsection{Klasifikace do více tøíd}
Zatím jsme se zabıvali pouze klasifikaci do dvou tøíd neboli binární klasifikací. Nyní uvaujme trénovací mnoinu  $ \lbrace x_{i}, y_{i}\rbrace $, $ i = 1, 2, \dots, l $, $ y_{i} \in \lbrace 1,2,\dots , k \rbrace $, kde $ k > 2 $ je poèet cílovıch tøíd. Uveïme si dva základní pøístupy, jak do tìchto tøíd klasifikovat:
\begin{itemize}
	\item \textbf{One-Versus-One} - natrénujeme $ \frac{k(k-1)}{2} $ binárních klasifikátorù, kde kadı z nich porovnává dvì rùzné tøídy navzájem. Klasifikace testovacího vektoru je pak zaloena na hlasování, kdy kadı klasifikátor hlasuje pro jednu tøídu a testovací vektor je zaøazen do té tøídy, která dostala nejvíce hlasù.
	\item \textbf{One-Versus-All} - natrénujeme $ k $ binárních klasifikátorù, kde kadı z nich porovnává jednu z $ k $ tøíd oproti zbylım $ k-1 $ tøídám. Testovací vektor je zaklasifikován do té tøídy, pro kterou nabıvá oddìlující nadrovina \ref{eq:svm} nejvyšší hodnoty (tj. bod leí nejdále od oddìlující nadroviny a byl zaklasifikován s nejvìtší "jistotou")\cite{friedman,svec}.
\end{itemize}

\newpage
\section{Neuronové sítì}
Neuronová sí je algoritmus inspirovanı funkcí neuronù a jejich propojením v lidském mozku. Skládá se z mnoha vıpoèetních jednotek (neurony) propojenıch pomocí numerickıch parametrù (synapse) a úpravou tìchto parametrù je schopna se uèit. Tato práce je omezena pouze na dopøedné neuronové sítì a jejich trénování pomocí uèení s uèitelem.

\subsection{Perceptron a aktivaèní funkce}
Základní vıpoèetní jednotkou neuronovıch sítí je tzv. perceptron. Vıstupní hodnota perceptronu je dána vztahem
\begin{equation}
y = f\left( \sum_{i=1}^{n} w_{i}x_{i} + w_{0} \right) = f(w^{T}x + w_{0}),
\end{equation}
kde $ w = [w_{1}, w_{2}, \dots ,w_{n}]^{T} $ je váhovı vektor, $ x = [x_{1}, x_{2}, \dots ,x_{n}]^{T} $ je vstupní vektor, $ w_{0} $ je práh a $ f(\cdot) $ je aktivaèní funkce. Práh reprezentuje váhu vedoucí od jednotkového vstupního bodu, kterı se zavádí z dùvodu generalizace sítì.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\textwidth]{perceptron.eps}
    \caption{Perceptron.}
    \label{fig:perceptron}
\end{figure}

Pro zjednodušení následujících odvození si rozšiøme váhovı vektor $ w $ o práh 
$ w_{0} $ a~vstupní vektor o jednotkovı vstupní bod
\begin{align}
w &= [w_{0}, w_{1}, \dots ,w_{n}]^{T}, \\
x &= [1, x_{1}, \dots ,x_{n}]^{T}.
\end{align}

Uveïme si také pøíklady nejznámìjších aktivaèních funkcí:
\begin{itemize}
 \item \textbf{Sigmoidální aktivaèní funkce} - sigmoidální aktivaèní funkce transformuje reálné èíslo do intervalu $ (0,1) $. Nevıhodou tohoto rozsahu je, e velmi malá èísla jsou transformována na hodnoty blízké nule, co má za následek i velice nízkou hodnotu lokálního gradientu a neuronem tak projde minimum signálu (více u algoritmu back\-propagation). Pøi inicializaci vah vysokımi hodnotami naopak mùe dojít k~saturaci signálu a sí nebude schopná se uèit. Vıstupy neuronu s touto aktivaèní funkcí zároveò nemají støední hodnotu v nule, co má za následek, e pro kladnı vstup budou mít všechny váhy vedoucí k neuronu stejné znaménko.
\begin{equation}
f(\xi) = \dfrac{1}{1 + e^{-\xi}}
\end{equation}
 \item \textbf{Tanh} - aktivaèní funkce tanh transformuje reálnì èíslo do intervalu $ (-1,1) $. Vıstupní interval má støední hodnotu v nule a øeší nedostatky sigmoidální aktivaèní funkce.
\begin{equation}
f(\xi) = \tanh (\xi) = \dfrac{2}{1 + e^{-2\xi}} - 1
\end{equation}
 \item \textbf{ReLU (Rectified Linear Unit)} - aktivaèní funkce ReLU je lineární aktivaèní funkce s prahem v hodnotì nula. Oproti vıše zmínìnım aktivaèním funkcím vıraznì urychluje konvergenci gradientu a není tak vıpoèetnì nároèná. Pøi nevhodnì zvolené konstantì uèení však mue dojít k "deaktivaci" neuronù, kdy jejich gradient poklesne na nulu a tyto neurony ji nikdy nebudou aktivovány.
\begin{equation}
f(\xi) = \max (0, \xi)
\end{equation}
 \item \textbf{Maxout} - Maxout je generalizací aktivaèní funkce ReLU. Na rozdíl od vıše zmínìnıch aktivaèních funkcí nemá stejnı funkcionální tvar $ f(w^{T}x+b) $, ale poèítá hodnotu funkce $ \max(w_{1}^{T}x+b_{1},w_{2}^{T}x+b_{2}) $. Maxout øeší nedostatky ReLU, ovšem za cenu zdvojnásobení poètu parametrù pro kadı neuron\cite{karpathy}.
\end{itemize}

Mùeme si všimnout, e argument aktivaèní funkce definuje nadrovinu v $ n $-dimenzionálním prostoru. Volbou aktivaèní funkce
\begin{equation}
f(\xi) = \text{sign}(\xi) = 
\begin{cases}
1 & \quad \text{pro } \xi \geq 0 \\
0 & \quad \text{jinak}\\
\end{cases}
\end{equation}
získáme jednoduché rozhodovací pravidlo, které pøiøazuje body do poloroviny na základì znaménka argumentu aktivaèní funkce. Jedná se tedy o jednoduchı klasifikátor, kterı dokáe klasifikovat do dvou tøíd (jedná se o analogii lineárnì separabilního pøípadu u~klasifikátoru SVM).

Paralelním spojením více perceptronù získáme nejjednodušší typ dopøedné neuronové sítì, tzv. jednovrstvou neuronovou sí\cite{alpaydin, mitchell}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{single_layer.eps}
    \caption{Jednovrstvá neuronová sí s $ n $ vstupy, aktivaèní funkcí $ f(\cdot) $ a $ m $ vıstupy.}
    \label{fig:slp}
\end{figure}

\subsection{Trénování jednovrstvé neuronové sítì}
Jedním ze zpùsobù trénování jednovrstvé neuronové sítì je tzv. perceptronové pravidlo. Nejprve síti pøidìlíme náhodné váhy (vìtšinou se volí malá èísla v okolí nuly) a poté pøivádíme na vstup sítì jednotlivé pøíznakové vektory z trénovací mnoiny. V pøípadì, e sí zaklasifikuje bod chybnì, dojde k úpravì hodnot vah. Tento proces probíhá tak dlouho, dokud nejsou všechny body zaklasifikovány správnì.

Jednotlivé váhy $ w_{i} $ vedoucí od $ i $-tého vstupu $ x_{i} $ jsou modifikovány pomocí perceptronového pravidla
\begin{equation}
w_{i}(k+1) = w_{i}(k) + \Delta w_{i}(k) = w_{i}(k) + \alpha(t-y)x_{i},
\end{equation}
kde $ k $ je iterace uèícího algoritmu, $ t $ je oèekávanı vıstup neuronu, $ y $ je skuteènı vıstup neuronu a $ \alpha \in \mathbb{R}^{+} $ je konstanta uèení. V pøípadì, e je vhodnì zvolena konstanta uèení $ \alpha $ a data jsou lineárnì separabilní, algoritmus uèení dokonverguje k optimálnímu nastavení vah.

Druhım zpùsobem uèení je tzv. delta pravidlo, které zajišuje konvergenci i pro lineárnì neseparabilní data. Pro zavedení delta pravidla si nejprve definujme trénovací chybu
\begin{equation}
E(w) = \dfrac{1}{2}\sum_{i=0}^{n} (t_{i}-y_{i})^{2},
\end{equation}
kde $ n $ je poèet prvkù trénovací mnoiny. 

Cílem uèení je tuto chybu minimalizovat, èeho dosáhneme pomocí gradientního sestupu (gradient descent). Jedná se o iterativní algoritmus, kterı opìt zaèíná s náhodnì inicializovanımi hodnotami vah a v kadém kroku je upraví ve smìru nejvìtšího sestupu gradientu. Tento proces probíhá tak dlouho, dokud není nalezeno globální minimum chybové funkce. Modifikace vah je tedy závislá na gradientu $ E(w) $ podle $ w $
\begin{equation}
\nabla E = \left[ \dfrac{\partial E}{\partial w_{0}}, \dfrac{\partial E}{\partial w_{1}}, \dots, \dfrac{\partial E}{\partial w_{n}} \right].
\end{equation}
Gradient $ \nabla E(w) $ udává smìr nejvìtšího rùstu chybové funkce - jeho zápornou hodnotou tedy získáme smìr nejvìtšího poklesu a trénovací pravidlo v maticovém tvaru bude
\begin{equation}
w(k+1) = w(k) + \Delta w(k) = w(k) - \alpha \nabla E(w) = w(k) - \alpha(t-y)x^{T}.
\end{equation}

Vzhledem k tomu, e daná chybová funkce má pouze jedno globální minimum, gradientní sestup pøi vhodnì zvolené konstantì uèení vdy dokonverguje k takovému váhovému vektoru, kterı zajišuje minimální chybu\cite{mitchell}.

\subsection{Vícevrstvá dopøedná neuronová sí}
Jak ji bylo zmínìno, jednovrstvé neuronové sítì umoòují vyjádøit pouze lineární rozhodovací hranici. Nelineární hranici mùeme vyjádøit pomocí vícevrstvé sítì, která se skládá z jedné vstupní vrstvy, jedné nebo více skrytıch vrstev a vıstupní vrstvy. Kadá skrytá vrstva se skládá z libovolného poètu neuronù a prahové jednotky a jednotlivé skryté vrstvy mohou mít rùzné aktivaèní funkce.

Pro zjednodušení následujících odvození pøedpokládejme sí se vstupní vrstvou s $ I $ vstupními jednotkami, skrytou vrstvu s $ J $ neurony a vıstupní vrstvu s $ K $ vıstupními jednotkami. Neurony ve skryté vrstvì mají aktivaèní funkci $ f(\cdot) $ a vıstupní vrstva má aktivaèní funkci $ g(\cdot) $ (znázornìno na obrázku \ref{fig:mlp}). Opìt rozšíøíme váhovou matici $ w $ o~práh $ w_{0} $ vedoucí k jednotkovému vstupnímu bodu $ x_{0} $, kterım rozšíøíme vstupní vektor $ x $. Vıstup neuronu $ j $ ve skryté vrstvì tedy mùeme zapsat jako
\begin{align}
net_{j}  &= \sum_{i=1}^{I}x_{i}w_{ji} + x_{0}w_{j0} = \sum_{i=0}^{I}x_{i}w_{ji} = w^{T}_{j}x, \\
z_{j} &= f(net_{j}),
\end{align}
kde $ w_{ji} $ znaèí váhu mezi $ j $-tım neuronem skryté vrstvy a $ i $-tou vstupní jednotkou. Obdobnì lze vypoèítat vıstup $ k $-té vıstupní jednotky
\begin{align}
net_{k} &= \sum_{j=1}^{J}z_{j}w_{kj} + z_{0}w_{k0} = \sum_{j=0}^{J}z_{j}w_{kj} = w^{T}_{k}z, \\
y_{k}  &= g\left(net_{k}\right).
\end{align}
Úpravou lze $ k $-tı vıstup zapsat jako
\begin{equation}
y_{k} = g\left(\sum_{j=1}^{J}w_{kj}f\left( \sum_{i=1}^{I}x_{i}w_{ji} + x_{0}w_{j0} \right)  + w_{k0} \right).
\end{equation}

Obdobnım zpùsobem lze vyjádøit vıstup pro neuronovou sí s libovolnım poètem skrytıch vrstev.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{mlp.eps}
    \caption{Dvouvrstvá neuronová sí.}
    \label{fig:mlp}
\end{figure}

\subsection{Trénování vícevrstvé neuronové sítì}
Pro trénování vícevrstvıch neuronovıch sítí se vyuívá algoritmus backpropagation neboli algoritmus zpìtného šíøení chyby, kterı je stejnì jako delta pravidlo zaloen na minimalizaci chybové funkce pomocí gradientu. Definujme si tedy trénovací chybu $ E $ jako kvadrát sumy rozdílù mezi skuteènım vıstupem $ k $-té vıstupní jednotky $ y_{k} $ a oèekávanım vıstupem $ t_{k} $
\begin{equation}
E(w) = \dfrac{1}{2}\sum_{k=1}^{K}(t_{k} - y_{t})^{2} = \dfrac{1}{2}(t - y)^{2}. \label{eq:error}
\end{equation}

Algoritmus backpropagation, stejnì jako delta pravidlo, vychází z algoritmu gradientního sestupu. Jednotlivé váhy jsou inicializovány náhodnımi malımi èísly a jsou modifikovány ve smìru nejvìtšího poklesu chybové funkce
\begin{equation}
\Delta w = -\alpha\dfrac{\partial E}{\partial w},
\end{equation}
èím opìt získáme pravidlo pro modifikaci vah
\begin{equation}
w(k+1) = w(k) + \Delta w(k).
\end{equation}

Nejprve si odvoïme pravidlo pro modifikaci vah mezi skrytou a vıstupní vrstvou. Vzhledem k tomu, e chybová funkce není pøímo závislá na $ w_{jk} $, musíme pouít øetìzové pravidlo.
\begin{equation}
\dfrac{\partial E}{\partial w_{jk}} = \dfrac{\partial E}{\partial net_{k}} \dfrac{\partial net_{k}}{\partial w_{jk}} = \delta_{k} \dfrac{\partial net_{k}}{\partial w_{jk}} \quad\Rightarrow\quad \delta_{k} = -\dfrac{\partial E}{\partial net_{k}}
\end{equation}
Hodnotu $ \delta_{k} $ nazıváme citlivost neuronu a popisuje, jak se zmìní celková chyba pøi jeho aktivaci. Derivací rovnice \ref{eq:error} získáme hledané pravidlo
\begin{equation}
\Delta w_{jk} = \alpha \delta_{k}z_{j} = \alpha (t_{k}-y_{k})f'(net_{k})z_{j}.
\end{equation}
Obdobnım zpùsobem vyjádøíme pravidlo pro modifikaci vah mezi vstupní a skrytou vrstvou
\begin{align}
\dfrac{\partial E}{\partial w_{ji}} &= \dfrac{\partial E}{\partial z_{j}}\dfrac{\partial z_{j}}{\partial net_{j}}\dfrac{\partial net_{j}}{\partial w_{ji}} \quad\Rightarrow\quad \delta_{j}=f'(net_{j})\sum_{k=1}^{K}w_{kj}\delta_{k}, \\
\Delta w_{ji} &= \alpha x_{i}\delta_{j} = \alpha x_{i} f'(net_{j} \sum_{k=1}^{K}w_{kj}\delta_{k}).
\end{align}
Podrobnìjší odvození algoritmu backpropagation lze nalézt v \cite{duda,mitchell}.

\subsubsection{Momentum}
Chybová funkce vícevrstvé sítì mùe mít více lokálních minim a na rozdíl od chybové funkce jednovrstvé sítì (parabolická funkce s jedním globálním minimem) gradientní sestup nezaruèuje nalezení globálního minima, ale pouze lokálního minima. Z tohoto dùvodu se zavádí tzv. momentum (setrvaènost), které zabraòuje uváznutí uèícího algoritmu v~mìlkém lokálním minimu a urychluje konvergenci na plochıch èástech povrchu chybové funkce. Pøidáním momentového èlenu získáme následující pravidlo pro úpravu vah
\begin{equation}
\Delta w_{ji}(k) = \alpha x_{i}\delta_{j} + \mu \Delta w_{ji}(k-1),
\end{equation}
kde $ 0 < \mu < 1 $ je momentum a $ k $ je iterace uèícího algoritmu. Zmìna vah tedy závisí i na zmìnì vah v minulé iteraci\cite{mitchell,duda}.

\subsubsection{Sekvenèní a dávkové uèení}
Jak ji bylo zmínìno, pøi trénování s uèitelem jsou neuronové síti pøedkládány obrazy z trénovací mnoiny, která se pak snaí minimalizovat celkovou chybu mezi vıstupy sítì a oèekávanımi hodnotami. V kadém trénovacím cyklu algoritmu jsou síti pøedloeny všechny obrazy z trénovací mnoiny. V praxi se nejèastìji pouívají dva typy trénování neuronovıch sítí:
\begin{itemize}
	\item \textbf{sekvenèní uèení} - neuronové síti jsou postupnì pøedkládány jednotlivé obrazy z~trénovací mnoiny (vìtšinou v náhodném poøadí), pro kadı obraz je spoètena chyba klasifikace a následnì jsou upraveny parametry sítì.
	\item \textbf{dávkové uèení} - neuronové síti jsou postupnì pøedkládány jednotlivé obrazy z~trénovací mnoiny, jednotlivé chyby klasifikace jsou akumulovány a k úpravì parametrù sítì dojde a na konci trénovacího cyklu s ohledem na celkovou chybu klasifikace\cite{duda}.
\end{itemize}

\newpage
\section{Dynamic Time Warping}
Rozpoznávání øeèi je velmi obtíná úloha, jeliko ádné dvì promluvy nejsou stejné. Hlasy rùznıch osob se liší a stejnì tak se liší i jejich artikulace nebo tempo a barva øeèi. Ani promluvy jedné osoby nejsou stejné - jedna promluva mùe bıt pronesena potichu, druhá nahlas nebo šeptem, mohou bıt proneseny rùznì rychle nebo napø. pod vlivem nachlazení. Na akustickém signálu se dále projevuje pøítomnost šumu a rušení na pozadí\cite{psutka2}.

Jedním z øešení tohoto problému je vyuití klasifikátoru podle minimální vzdálenosti k vzorovım obrazùm. Pøi klasifikaci se slovo zpracovává jako celek a je zaøazeno do té tøídy, k jejímu vzorovému obrazu má nejmenší vzdálenost.
Základním problémem je urèení této vzdálenosti, jeliko obrazy mají rùzné délky v závislosti na délce signálu. Odlišnosti mezi podobnımi signály tedy nejsou ve spektrální oblasti, ale v èasové oblasti. K urèení vzdálenosti mezi dvìma signály se tedy vyuívá algoritmus Dynamic Time Warping (DTW), neboli nelineární "borcení" èasové osy, kterı je zaloen na metodì dynamického programování. "Borcením" èasové osy obrazu jedné z nahrávek dojde k maximalizaci shody mezi nahrávkami.

\subsection{Základní algoritmus}
Pøedpokládejme, e máme dvì nahrávky, které jsou reprezentovány svımi obrazy. Oznaème obraz testovaného slova
\begin{equation}
A = \left\lbrace a_{1}, a_{2}, \dots, a_{n}, \dots, a_{I} \right\rbrace
\end{equation}
a obraz referenèního slova
\begin{equation}
B = \left\lbrace b_{1}, b_{2}, \dots, b_{m}, \dots, b_{J} \right\rbrace,
\end{equation}
kde $ a_n $ je $ n $-tı pøíznak testovaného slova a $ b_m $ je $ m $-tı pøíznak referenèního slova. Algoritmus DTW pak hledá v rovinì ($ n $,$ m $) optimální cestu $ m = \Psi(n) $, která minimalizuje vzdálenost mezi obrazy $ A $ a $ B $
\begin{equation}
D(A,B) = \sum_{n=1}^{I} \hat{d} \left( a_{n}, b_{\Psi(n)} \right),
\end{equation}
kde $ \hat{d} ( a_{n}, b_{\Psi(n)} ) $ je vzdálenost mezi $ n $-tım prvkem testovaného obrazu a $ m $-tım prvkem referenèního obrazu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{dtw_threeway.eps}
    \caption{Prùbìh funkce DTW pro $ sin(x) $ se šumem a $ cos(x) $.}
    \label{fig:dtw_threeway}
\end{figure}

\subsection{Omezení pohybu funkce}
Optimální cesta by mìla zachovávat základní vlastnosti èasovıch os obou obrazù (souvislost, monotónnost, atd.). Z toho dùvodu se zavádí omezení na pohyb funkce DTW. Zavedeme obecnou èasovou promìnnou $ k $ a èasové promìnné $ m $ a $ n $ vyjádøíme jako funkce $ k $
\begin{align}
n &= i(k), \\
m &= j(k),
\end{align}
kde $ k = 1,2,\dots,K $ a $ K $ je délka obecné èasové osy.

\subsubsection{Omezení na hranièní body}
Hranièní body funkce DTW jsou dány podmínkami
\begin{align}
i(1) = 1 \quad i(K)=I, \\
j(1) = 1 \quad j(K)=J.
\end{align}

\subsubsection{Omezení na lokální souvislost}
Pøi prùchodu funkce DTW mùe dojít k nadmìrné expanzi èi kompresi èasové osy. Proto je vhodné omezit lokální monotónnost a souvislost DTW funkce
\begin{align}
0 \leq i(k) - i(k-1) \leq \bar{I}, \\
0 \leq j(k) - j(k-1) \leq \bar{J},
\end{align}
pøièem $ \bar{I} $ a $ \bar{J} $ jsou volitelné konstanty. Nejèastìji volíme hodnoty $ \bar{I}, \bar{J} = 1,2,3 $ s tím, e pøi hodnotì vìtší ne 1 mùe funkce DTW pøi porovnávání nìkteré mikrosegmenty pøeskoèit.

\subsubsection{Omezení na lokální strmost}
Pro funkci není vhodnı pøíliš velkı, ani pøíliš malı pøírùstek, a tak se zavádí omezení na lokální strmost. Pokud se zastupující bod $ \left[ i(k),j(k)\right] $ pohybuje ve smìru jedné osy $ \bar{n} $-krát po sobì pøi rostoucím $ k $, pak se v tomto smìru nesmí nadále pohybovat, dokud neudìlá $ \bar{m} $ krokù v jiném smìru.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{dtw_local.eps}
    \caption{Nejèastìji pouívaná lokální omezení (více v \cite{psutka1,sakoe}).}
    \label{fig:dtw_local}
\end{figure}

\subsubsection{Globální vymezení oblasti pohybu funkce}
Splnìním podmínek pro omezení na hranièní body a zobecnìním podmínek omezení na lokální strmost na celou rovinu $ (n,m) $ lze vymezit pøípustnou oblast prùchodu funkce DTW
\begin{align}
1 + \alpha\left[i(k)-1 \right] &\leq j(k) \leq 1 + \beta\left[ i(k)-1\right], \\
J + \beta\left[ i(k)-I\right] &\leq j(k) \leq J + \alpha\left[i(k)-I \right],
\end{align}
kde $ \alpha $ je minimální smìrnice a $ \beta $ maximální smìrnice pøímky vymezující pøípustnou oblast.

Pøedpokládejme, e pøi porovnání testovaného a referenèního obrazu, které reprezentují stejné slovo, nemùe dojít k zásadním èasovım rozdílùm mezi pøíslušnımi úseky stejnıch obrazù zapøíèinìnıch kolísáním tempa øeèi. S ohledem na tento pøedpoklad lze tedy stanovit podmínku pro druhé globální vymezení oblasti pohybu funkce DTW
\begin{equation}
|i(k) - j(k)| \leq w,
\end{equation}
kde $ w $ je celé èíslo, které urèuje šíøku okénka. Šíøka okénka musí bıt menší ne $ |J-I| $, aby do pøípustné oblasti funkce DTW bylo moné zahrnout i koncovı bod $ (I,J) $.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{dtw_global.eps}
    \caption{Globální vymezení oblasti pohybu funkce.}
    \label{fig:dtw_global}
\end{figure}

\subsection{Minimální vzdálenost}
Celkovou minimální vzdálenost mezi testovacím obrazem $ A $ a referenèním obrazem $ B $ lze vyjádøit vztahem
\begin{equation}
D(A,B) = \min_{\lbrace i(k), j(k), K \rbrace} \left[ \dfrac{\sum_{k=1}^{K}d\left[ i(k),j(k) \right] \hat{W}(k)}{N(\hat{W})} \right],\label{eq:dist1}
\end{equation}
kde $ d[i(k),j(k)] $ je lokální vzdálenost mezi $ n $-tım úsekem testovaného obrazu $ A $ a $ m $-tım úsekem referenèního obrazu $ B $, $ \hat{W}(k) $ je hodnota váhové funkce pro $ k $-tı úsek a $ N(\hat{W}) $ je normalizaèní faktor, jen je funkcí váhové funkce. Váhová funkce je závislá pouze na lokální cestì funkce DTW.

Implementace váhové funkce se volí na základì zvolenıch lokálních omezení funkce DTW. Nejèastìji se vyuívá jeden z tìchto ètyø typù váhovıch funkcí:
\begin{enumerate}[label=\alph*)]
	\item symetrická váhová funkce
\begin{equation}
\hat{W}(k) = \left[ i(k)-i(k-1)\right] + \left[ j(k)-j(k-1)\right],\label{eq:sym}
\end{equation}
	\item asymetrická váhová funkce
	\begin{enumerate}[label=b\arabic*)]
		\item
		\begin{equation}
		\hat{W}(k) = i(k) - i(k-1),\label{eq:asym1}
		\end{equation}
		\item
		\begin{equation}
		\hat{W}(k) = j(k) - j(k-1),\label{eq:asym2}
		\end{equation}
	\end{enumerate}
	\item 
\begin{equation}
\hat{W}(k) = \min \left[ i(k)-i(k-1), j(k)-j(k-1) \right],
\end{equation}
	\item 
\begin{equation}
\hat{W}(k) = \max \left[ i(k)-i(k-1), j(k)-j(k-1) \right],
\end{equation}
\end{enumerate}
pøièem $ i(0)=j(0)=0 $.

\subsection{Normalizaèní faktor}
Normalizaèní faktor $ N(\hat{W}) $ kompenzuje poèet krokù funkce DTW, kterı se pro rùznì dlouhé testovací a referenèní sekvence mùe vıraznì lišit. Lze jej definovat jako
\begin{equation}
N(\hat{W}) = \sum_{k=1}^{K}\hat{W}(k).
\end{equation}
Dosazením vztahù \ref{eq:sym}, \ref{eq:asym1}, \ref{eq:asym2} pro váhové funkce typu a) a b) získáme normalizaèní faktory
\begin{align}
N(\hat{W}_{a}) &= \sum_{k=1}^{K} \left[ i(k)-i(k-1) + j(k)-j(k-1)\right] = \\
&= i(K)-i(0)+j(K)-j(0) = I + J, \\
N(\hat{W}_{b1}) &= \sum_{k=1}^{K} \left[ i(k)-i(k-1)\right] = i(K)-i(0) = I, \\
N(\hat{W}_{b2}) &= \sum_{k=1}^{K} \left[ j(k)-j(k-1)\right] = j(K)-j(0) = J.
\end{align}

Ze vztahù je patrné, e pro váhové funkce typu a) a b) je hodnota normalizaèního faktoru nezávislá na konkrétním prùbìhu funkce DTW. Pro váhové funkce typu c) a d) je hodnota normalizaèního faktoru silnì závislá na prùbìhu funkce DTW a nelze ji urèit pomocí metod dynamického programování. Pro tyto pøípady se hodnota normalizaèního faktoru volí nezávisle na prùbìhu funkce DTW, aby bylo moné pouít rekurzivní algoritmus.
\begin{equation}
N(\hat{W}_{c}) = N(\hat{W}_{d}) = I
\end{equation}

\subsection{Rekurzivní algoritmus}
Díky nezávislosti normalizaèního faktoru na prùbìhu funkce DTW lze vztah \ref{eq:dist1} pro vıpoèet celkové minimální vzdálenosti mezi dvìma obrazy $ A $ a $ B $ zjednodušit do tvaru
\begin{equation}
D(A,B) = \dfrac{1}{N(\hat{W})}\lbrace \min_{\lbrace i(k), j(k), K \rbrace}  \sum_{k=1}^{K}d\left[ i(k),j(k) \right] \hat{W}(k) \rbrace \label{eq:dist2}
\end{equation}
Vıslednou hodnotu vztahu \ref{eq:dist2} lze urèit rekurzivnì algoritmem dynamického programování, kdy zavedeme funkci $ g $ èásteèné akumulované vzdálenosti:
\begin{enumerate}
	\item Inicializace
\begin{equation}
g\left[ i(1), j(1) \right] = d\left[ i(1), j(1) \right]\hat{W}(1)
\end{equation}
	\item Rekurze
\begin{equation}
g\left[ i(k), j(k) \right] = \min_{\lbrace i(k),j(k)\rbrace} \lbrace g\left[ i(k-1), j(k-1) \right] +  d\left[ i(k), j(k) \right]\hat{W}(k) \rbrace
\end{equation}
	\item Koneèná normalizovaná vzdálenost
\begin{equation}
D(A,B) = \dfrac{1}{N(\hat{W})} g\left[ i(K), j(K) \right] = \dfrac{1}{N(\hat{W})} g\left[ I,J \right]
\end{equation}
\end{enumerate}
Rekurzivní vztahy pro rùzné typy lokálních omezení lze odvodit dosazením za $ \hat{W}(k) $\cite{psutka1,sakoe}.

% PRAKTICKÁ ÈÁST
\newpage
\section{Klasifikace izolovanıch slov}
Pro porovnání jednotlivıch metod byla vytvoøena datová sada obsahující 240 nahrávek od šesti øeèníkù. Mezi øeèníky byli ètyøi mui (v tabulkách s vısledky znaèeni èísly 1, 2, 3 a~6) a dvì eny (znaèeny èísly 4, 5). Kadı øeèník pronesl ètyøikrát po sobì èíslovky nula a devìt. První dvì nahrávání probìhla v tichém prostøedí, druhá dvì za mírného okolního šumu, díky èemu lze lépe porovnat robustnost zkoumanıch metod. Tato datová sada byla následnì rozdìlena na referenèní a testovací sadu. Jako referenèní nahrávky byly zvoleny èíslovky nula a devìt z prvního nahrávání kadého øeèníka, ostatní nahrávky tvoøí testovací sadu.

Nahrávky z referenèní sady pak byly vyuity jako trénovací data pro klasifikátor SVM a neuronovou sí. Pro pøíznaky generované neuronovou sítí byla trénovací sada rozšíøena o nahrávky vytvoøené pøi vıvoji hlasového rozhraní pro Škoda Auto, kdy kadı øeèník tøikrát pronesl povely hlasového ovládání navigace, rádia, telefonu a poté promluvy mìsta, ulice a èíslovek nula a devìt.

Porovnávané metody byly implementovány v programovacím jazyce Python s vyuitím knihoven pro vìdecké vıpoèty NumPy a SciPy\cite{scipy}, knihovny pro strojové uèení scikit-learn\cite{sklearn} a knihovny pro neuronové sítì Lasagne\cite{lasagne}.

\subsection{Zpracování akustického signálu}
Prvním krokem pro klasifikaci izolovanıch slov je extrakce pøíznakù z akustického signálu. Pro tyto úèely byl vytvoøen samostatnı modul obsahující metody pro zpracování akustického signálu a transformaci pøíznakovıch vektorù. Modul umoòuje segmentaci signálu s vyuitím pravoúhlého, Hammingova nebo Hanningova okénka s volitelnou délkou v milisekundách a volitelnım posunem. 

Z dùvodu úspornìjšího zápisu vısledkù si zaveïme znaèení pro jednotlivé typy pøíznakù, které modul implementuje:
\begin{table}[H]
\centering
\singlespacing
\def\arraystretch{1.5}
\begin{tabular}{ l|l }
znaèení & typ pøíznakù \\ \hline
ste & krátkodobá energie \\ \hline
sti & krátkodobá intenzita \\ \hline
stzcr & krátkodobé prùchody nulou \\ \hline
ste\_sti\_stzcr & \vtop{\hbox{\strut kombinace pøíznakù krátkodobé energie, intenzity a prùchodù nulou}\hbox{\strut (tvoøí matici, kde sloupce odpovídají jednotlivım typùm pøíznakù v èase)}} \\ \hline
log\_fb\_en & logaritmované energie banky filtrù \\ \hline
mfcc & \vtop{\hbox{\strut mel-frekvenèní kepstrální koeficienty}\hbox{\strut (implementace byla pøevzata z modulu \cite{pymfcc})}} \\
\end{tabular}
\caption{\label{tab:znaceni}Znaèení typù pøíznakù.}
\end{table}
Po vygenerování pøíznakového vektoru je moné aplikovat Z-score nebo Min-Max normalizaci a dopoèítat delta a delta-delta koeficienty (implementace byla pøevzata z modulu LibROSA\cite{librosa}).

Vygenerované parametrizace pøíznakù jsou uvedeny v tabulce \ref{tab:parametrizace}. První èíslo v názvu pøíznakù znaèí délku okénka v milisekundách, druhé posun okénka v milisekundách. Vyuití Hammingova okénka je znaèeno zkratkou \textit{ham} (pokud není uvedeno, pøíznak byl vygenerován za vyuití pravoúhlého okénka), delta a delta-delta koeficienty zkratkou \textit{deltas} a normalizované pøíznaky \textit{norm}, napø.:
\begin{itemize}
\item log\_fb\_en\_25\_10\_ham\_deltas - logaritmované energie banky filtrù, segmentováno pomocí Hammingova okénka o délce 25ms s posunem 10ms, vypoèteny delta a delta-delta koeficienty,
\item stzcr\_10\_10\_norm - krátkodobé prùchody nulou, segmentováno pomocí pravoúhlého okénka o délce 10ms s posunem 10ms, normalizovány
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{ c|c }
ste\_10\_10 & ste\_10\_10\_norm \\ \hline
sti\_10\_10 & sti\_10\_10\_norm \\ \hline
stzcr\_10\_10 & stzcr\_10\_10\_norm \\ \hline
ste\_sti\_stzcr\_10\_10 & ste\_sti\_stzcr\_10\_10\_norm \\ \hline
log\_fb\_en\_25\_10\_ham & log\_fb\_en\_25\_10\_ham\_norm \\ \hline
log\_fb\_en\_25\_10\_ham\_deltas & log\_fb\_en\_25\_10\_ham\_deltas\_norm \\ \hline
mfcc\_25\_10\_ham & mfcc\_25\_10\_ham\_norm \\ \hline
mfcc\_25\_10\_ham\_deltas & mfcc\_25\_10\_ham\_deltas\_norm \\
\end{tabular}
\caption{\label{tab:parametrizace}Vygenerované parametrizace pøíznakù.}
\end{table}

Z tabulky \ref{tab:parametrizace} je patrné, e všechny pøíznaky v èasové oblasti byly vygenerovány s~vyuitím pravoúhlého okénka o délce 10ms, které se posouvá o 10ms. Nedochází tedy k~pøesahu mezi jednotlivımi mikrosegmenty. Pøíznaky ve frekvenèní oblasti byly vygenerovány s~vyuitím Hammingova okénka o délce 25ms s posunem 10ms (jednotlivé mikrosegmenty se èásteènì pøekrıvají) a 512 bodové FFT. Pro vıpoèet logaritmovanıch energií banky filtrù bylo vyuito 40 filtrù, pro vıpoèet MFCC 26 filtrù.

\subsection{Dynamic Time Warping}
Pro vıpoèet DTW vzdálenosti byl vytvoøen modul, kterı obsahuje základní DTW algoritmus bez globálního vymezení pohybu funkce a vyuívá symetrické omezení na lokální strmost (první zleva na obrázku \ref{fig:dtw_local}). Jako vzdálenostní metrika pro všechny typy pøíznakù kromì \textit{ste\_sti\_stzcr} byla zvolena Euklidova vzdálenost.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{dtw_alignment.eps}
    \caption{Krátkodobé energie pro dvì rùzné nahrávky èíslovky 7.}
    \label{fig:dtw_alignment}
    \includegraphics[width=0.7\textwidth]{dtw_test.eps}
    \caption{Prùbìh funkce DTW pro dva rùzné obrazy reprezentující èíslovku 7.}
    \label{fig:dtw_test}
\end{figure}

\subsubsection{Optimalizace parametrù vzdálenostní metriky}
Pro kombinaci pøíznakù sloenou z krátkodobé energie, intenzity a prùchodù nulou byla vyuita vlastní vzdálenostní metrika
\begin{equation}
d(a_{i}, b_{j}) = \alpha |a_{ste,i} - b_{ste,j}| + \beta |a_{sti,i} - b_{sti,j}| + \gamma |a_{stzcr,i} - b_{stzcr,j}|,
\end{equation}
kde $ a_{i} $ je $ i $-tı pøíznak testovaného obrazu $ A $, $ b_{j} $ je $ j $-tı pøíznak referenèního obrazu $ B $ a~$ \alpha $, $ \beta $, $ \gamma $ jsou volitelné parametry.

Pro nenormalizovanou verzi byly zvoleny parametry $ \alpha = \beta = \gamma = 1 $, pro normalizovanou verzi byla provedena optimalizace parametrù, aby bylo moné urèit, kterı z~pøíznakù v~èasové oblasti má nejvìtší informativní hodnotu pro problém klasifikace izolovanıch slov. Pro optimalizaci byla zvolena brute-force metoda, kdy byl procházen seznam monıch parametrù s krokem 0.1 za podmínky $ \alpha + \beta + \gamma = 1 $. Pro kadou trojici parametrù pak byla vyhodnocena pøesnost klasifikace.

Nejvyšší pøesnosti klasifikace v rámci jednoho øeèníka bylo dosaeno s parametry $ \alpha = 0.3 $, $ \beta = 0.3 $, $ \gamma = 0.4 $ - kadı typ pøíznakù se tedy projeví pøiblinì stejnou mírou. Optimalizací mezi všemi øeèníky pak byly získány parametry $ \alpha = 0 $, $ \beta = 0.6 $, $ \gamma = 0.4 $ - je tedy zøejmé, e krátkodobá energie v kombinaci s krátkodobou intenzitou a prùchody nulou negativnì ovlivòuje pøesnost klasifikace (pravdìpodobnì z dùvodu závislosti na øeèníkovi, viz. kapitola Vyhodnocení).

Na tyto dvì parametrizace se dále budeme odkazovat jako \textit{ste\_sti\_stzcr\_10\_10\break\_norm\_single}, resp. \textit{ste\_sti\_stzcr\_10\_10\_norm\_all}.

\subsection{Support Vector Machine}
Ke klasifikaci normalizovanıch pøíznakù byl vyuit také SVM klasifikátor s pøedpoèítanou jádrovou funkcí ve tvaru
\begin{equation}
K(A,B) = 1 - DTW(A,B).
\end{equation}
Jádrová funkce byla vypoètena ze všech referenèních obrazù navzájem a funkcionálnì odpovídá Gramovì matici $ G = X^{T}X $.

\subsection{Neuronová sí}
Pro kadého øeèníka byla natrénována tøívrstvá neuronová sí (znázornìna na obrázku \ref{fig:nn}). Vstupem této sítì je DTW vzdálenost testované nahrávky vùèi referenèním nahrávkám, vıstupem pak vektor obsahující pravdìpodobnosti náleitosti do danıch tøíd (zajištìno aktivaèní funkcí Softmax). Po otestovaní nìkolika rùznıch parametrizací neuronové sítì byly zvoleny dvì skryté vrstvy o 200 neuronech s aktivaèní funkcí ReLU.

Sí byla trénována 1500 trénovacích epoch s vyuitím dávkového uèení s dávkami o~velikosti 10. Váhy sítì byly modifikovány stochastickım gradientním sestupem rozšíøenım o~Nesterovo momentum $ \mu = 0.9 $. Konstanta uèení byla zvolena $ \alpha = 0.01 $. Kvùli pøedpokladùm tohoto klasifikaèního algoritmu byla neuronová sí natrénována pouze pro normalizovaná data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{nn.eps}
    \caption{Tøívrstvá neuronová sí.}
    \label{fig:nn}
\end{figure}

\subsection{Bottleneck}
Pro generování pøíznakù pomocí neuronové sítì byl pouit tzv. bottleneck. Bottleneckem je nazıvána taková struktura neuronové sítì, kdy jedna ze skrytıch vrstev (bottleneck vrstva) obsahuje vıraznì niší poèet neuronù ne její sousední vrstvy. Po natrénování neuronové sítì je bottleneck vrstva vyuita jako vıstupní vrstva a všechny následující vrstvy jsou odstranìny. Tím dochází ke kompresi vstupní informace do pøíznakového vektoru o~konstantní délce.

Bottleneck byl vytvoøen natrénováním ètyøvrstvé neuronové sítì a odstranìním vıstupní a poslední skryté vrstvy (znázornìno na obrázku \ref{fig:bn}). První a tøetí skrytá vrstva se skládá z 1024 neuronù s aktivaèní funkcí Tanh, druhá (bottleneck) vrstva a vıstupní vrstva mají aktivaèní funkci Softmax.  


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{bn.eps}
    \caption{Ètyøvrstvá neuronová sí.}
    \label{fig:bn}
\end{figure}

Neuronová sí byla natrénována pro dvì rùzné datové sady - pùvodní datovou sadu šesti øeèníkù a pùvodní datovou sadu rozšíøenou o nahrávky pro Škoda Auto. Vstupem neuronové sítì je vektor o velikosti 440 sloenı z logaritmované energie banky filtru pro jeden foném (vektor o velikosti 40) a jeho kontextu zleva a zprava (vektory o velikosti $ 5 \cdot 40 = 200 $). 

Sí byla trénována pro klasifikaci 20 fonému pro pùvodní datovou sadu s 8 a 16 neurony v bottleneck vrstvì a pro klasifikaci 40 fonému s 8, 16 a 32 neurony pro rozšíøenou sadu. Pro pøehlednìjší vyhodnocení si opìt zavedeme znaèení pro vygenerované pøíznaky:
\begin{itemize}
\item \textit{bn\_X} - neuronová sí natrénovaná pro pùvodní datovou sadu,
\item \textit{bn\_SA\_X} - neuronová sí natrénovaná pro rozšíøenou datovou sadu,
\end{itemize}
kde \textit{X} znaèí poèet neuronù bottleneck vrstvy.


% VYHODNOCENÍ
\newpage
\section{Vyhodnocení}
Pøi vyhodnocení procentuální pøesnosti klasifikace jednotlivıch metod byly uvaovány tøi rùzné varianty:
\begin{enumerate}
 \item pøesnost klasifikace v rámci jednoho øeèníka (testovací nahrávky jednoho øeèníka vùèi svım referenèním nahrávkám)
 \item pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním (testovací nahrávky všech øeèníkù vùèi referenèním nahrávkám jednoho øeèníka)
 \item pøesnost klasifikace mezi všemi øeèníky (testovací nahrávky všech øeèníkù vùèi referenèním nahrávkám všech øeèníkù) 
\end{enumerate}

\begin{algorithm}[H]
\setstretch{1.25}
\For{speaker \textbf{\emph{in}} speakers}{
	reference\_features, reference\_targets = get\_references(speaker) \\
	test\_features, test\_targets = get\_test(speaker) \\
	model = train(reference\_features, reference\_targets) \\
	prediction = predict(test\_features) \\
	accuracy = calculate\_accuracy(prediction, test\_targets)	}
\end{algorithm}
\begin{center}
Algoritmus 1: Vyhodnocení pøesnosti klasifikace v rámci jednoho øeèníka.
\end{center}

\begin{algorithm}[H]
\setstretch{1.25}
\For{speaker \textbf{\emph{in}} speakers}{
	\tcc{combine test features/targets of all speakers}
	test\_features, test\_targets += get\_test(speaker) \\
}
\For{speaker \textbf{\emph{in}} speakers}{
	reference\_features, reference\_targets = get\_references(speaker) \\
	model = train(reference\_features, reference\_targets) \\
	prediction = predict(test\_features) \\
	accuracy = calculate\_accuracy(prediction, test\_targets)	
}
\end{algorithm}
\begin{center}
Algoritmus 2: Vyhodnocení pøesnosti klasifikace v rámci jednoho øeèníka vùèi ostatním.
\end{center}

\begin{algorithm}[H]
\setstretch{1.25}
\For{speaker \textbf{\emph{in}} speakers}{
	\tcc{combine reference and test features/targets of all speakers}
	reference\_features, reference\_targets += get\_references(speaker) \\
	test\_features, test\_targets += get\_test(speaker) \\
}
model = train(reference\_features, reference\_targets) \\
prediction = predict(test\_features) \\
accuracy = calculate\_accuracy(prediction, test\_targets)
\end{algorithm}
\begin{center}
Algoritmus 3: Vyhodnocení pøesnosti klasifikace mezi všemi øeèníky.
\end{center}

\subsection{Pøesnost klasifikace v rámci jednoho øeèníka}
Z tabulky \ref{tab:dtw_single_speaker} je zøejmé, e nejvyšší pøesnosti klasifikace v rámci jednoho øeèníka je dosaeno vyuitím pøíznakù generovanıch neuronovou sítí. Pro pøíznaky generované neuronovou sítí natrénovanou nad pùvodní sadou s bottleneck vrstvou o 8 neuronech je dokonce dosaeno nejvyšší pøesnosti ze všech zkoumanıch metod.

Velice vysoké pøesnosti také dosahují pøíznaky ve frekvenèní oblasti s tím, e Z-score normalizace jejich pøesnost mírnì sniuje. Aèkoliv by pøidáním delta a delta-delta koeficientù mìlo dojít k nárùstu pøesnosti klasifikace, z neznámıch dùvodù došlo k jejímu poklesu. V pøípadì normalizovanıch pøíznakù se tento pokles pohybuje dokonce mezi 25-30\%.

Jednotlivé pøíznaky v èasové oblasti dle oèekávání nedosahují vysokıch pøesností. Pro krátkodobou energii a intenzitu ovšem velmi pomáhá normalizace dat. Dále si mùeme povšimnout, e pøesnost nenormalizované krátkodobé energie je stejná jako pøesnost nenormalizované kombinace pøíznakù krátkodobé energie, intenzity a prùchodù nulou. To je zpùsobeno tím, e nenormalizované hodnoty krátkodobé energie se pohybují v øádech desetitisícù, zatímco hodnoty krátkodobé intenzity a prùchodù nulou v øádech desítek a oproti krátkodobé energii se projeví jen minimálnì. Normalizací a kombinací tìchto tøí typù pøíznakù pak dostáváme typ pøíznaku s pomìrnì vysokou informaèní hodnotou. Optimalizací parametrù vzdálenostní metriky skuteènì došlo k navıšení pøesnosti a to o~1.1\%.

\begin{table}[H]
\centering
\resizebox{0.9\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 90.0 & 93.3 & 96.7 & 96.7 & 93.3 & 100.0 & 95.0 \\
\hline
bn\_16 & 100.0 & 90.0 & 93.3 & 96.7 & 86.7 & 90.0 & 92.8 \\
\hline
bn\_SA\_8 & 90.0 & 83.3 & 86.7 & 83.3 & 83.3 & 93.3 & 86.7 \\
\hline
bn\_SA\_16 & 96.7 & 93.3 & 93.3 & 96.7 & 86.7 & 80.0 & 91.1 \\
\hline
bn\_SA\_32 & 93.3 & 93.3 & 96.7 & 93.3 & 76.7 & 93.3 & 91.1 \\
\hline
log\_fb\_en\_25\_10\_ham & 80.0 & 83.3 & 100.0 & 93.3 & 83.3 & 93.3 & 88.9 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 86.7 & 86.7 & 93.3 & 90.0 & 76.7 & 93.3 & 87.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 70.0 & 83.3 & 96.7 & 86.7 & 80.0 & 93.3 & 85.0 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 63.3 & 60.0 & 73.3 & 76.7 & 66.7 & 76.7 & 69.4 \\
\hline
mfcc\_25\_10\_ham & 86.7 & 83.3 & 100.0 & 93.3 & 90.0 & 90.0 & 90.6 \\
\hline
mfcc\_25\_10\_ham\_norm & 90.0 & 90.0 & 96.7 & 86.7 & 86.7 & 90.0 & 90.0 \\
\hline
mfcc\_25\_10\_ham\_deltas & 86.7 & 83.3 & 100.0 & 86.7 & 80.0 & 90.0 & 87.8 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 70.0 & 60.0 & 76.7 & 70.0 & 66.7 & 76.7 & 70.0 \\
\hline
ste\_10\_10 & 60.0 & 20.0 & 33.3 & 46.7 & 43.3 & 26.7 & 38.3 \\
\hline
ste\_10\_10\_norm & 56.7 & 40.0 & 43.3 & 50.0 & 46.7 & 56.7 & 48.9 \\
\hline
ste\_sti\_stzcr\_10\_10 & 60.0 & 20.0 & 33.3 & 46.7 & 43.3 & 26.7 & 38.3 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 96.7 & 86.7 & 80.0 & 80.0 & 66.7 & 76.7 & 81.1 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm\_single & 96.7 & 90.0 & 83.3 & 83.3 & 66.7 & 73.3 & 82.2 \\
\hline
sti\_10\_10 & 63.3 & 36.7 & 40.0 & 66.7 & 70.0 & 66.7 & 57.2 \\
\hline
sti\_10\_10\_norm & 76.7 & 43.3 & 56.7 & 70.0 & 56.7 & 70.0 & 62.2 \\
\hline
stzcr\_10\_10 & 56.7 & 70.0 & 60.0 & 63.3 & 60.0 & 60.0 & 61.7 \\
\hline
stzcr\_10\_10\_norm & 60.0 & 66.7 & 53.3 & 60.0 & 43.3 & 53.3 & 56.1 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka pro DTW.}
\label{tab:dtw_single_speaker}
\end{table}

Vısledky dosaené klasifikátorem SVM (tabulka \ref{tab:svm_single_speaker}) jak pro pøíznaky ve frekvenèní oblasti, tak pro bottleneck pøíznaky, jsou niší ne vısledky dosaené metodou DTW. Pøidáním dynamickıch koeficientù opìt došlo k vıraznému poklesu pøesnosti - 48.9\% pro logaritmované energie banky filtrù a 32.3\% pro MFCC. Pøesnosti dosaené u pøíznakù v~èasové oblasti jsou srovnatelné s pøesnostmi metody DTW.

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 83.3 & 86.7 & 93.3 & 90.0 & 90.0 & 93.3 & 89.4 \\
\hline
bn\_16 & 90.0 & 83.3 & 90.0 & 93.3 & 86.7 & 86.7 & 88.3 \\
\hline
bn\_SA\_8 & 86.7 & 73.3 & 80.0 & 70.0 & 80.0 & 90.0 & 80.0 \\
\hline
bn\_SA\_16 & 83.3 & 80.0 & 76.7 & 90.0 & 76.7 & 76.7 & 80.6 \\
\hline
bn\_SA\_32 & 80.0 & 70.0 & 86.7 & 80.0 & 70.0 & 80.0 & 77.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 80.0 & 90.0 & 80.0 & 83.3 & 76.7 & 86.7 & 82.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 60.0 & 46.7 & 20.0 & 20.0 & 26.7 & 30.0 & 33.9 \\
\hline
mfcc\_25\_10\_ham\_norm & 70.0 & 56.7 & 76.7 & 76.7 & 70.0 & 73.3 & 70.6 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 60.0 & 46.7 & 30.0 & 30.0 & 26.7 & 36.7 & 38.3 \\
\hline
ste\_10\_10\_norm & 60.0 & 33.3 & 46.7 & 43.3 & 50.0 & 53.3 & 47.8 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 90.0 & 76.7 & 73.3 & 60.0 & 66.7 & 73.3 & 73.3 \\
\hline
sti\_10\_10\_norm & 73.3 & 40.0 & 43.3 & 60.0 & 53.3 & 66.7 & 56.1 \\
\hline
stzcr\_10\_10\_norm & 60.0 & 63.3 & 53.3 & 50.0 & 40.0 & 50.0 & 52.8 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka pro SVM.}
\label{tab:svm_single_speaker}
\end{table}

Pøesnost klasifikace pomocí neuronové sítì (tabulka \ref{tab:nn_single_speaker}) pro pøíznaky ve frekvenèní oblasti je srovnatelná s pøesností pøíznakù generovanıch neuronovou sítí a klasifikovanıch pomocí DTW. Pøidání dynamickıch koeficientù tuto pøesnost opìt sniuje. Vysokıch pøesností také dosahují bottleneck pøíznaky. Pøesnost pøíznakù v èasové oblasti je mírnì horší ne u SVM.

\begin{table}[H]
\centering
\resizebox{0.9\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 83.3 & 90.0 & 93.3 & 96.7 & 90.0 & 96.7 & 91.7 \\
\hline
bn\_16 & 96.7 & 90.0 & 86.7 & 83.3 & 83.3 & 83.3 & 87.2 \\
\hline
bn\_SA\_8 & 86.7 & 90.0 & 93.3 & 80.0 & 93.3 & 83.3 & 87.8 \\
\hline
bn\_SA\_16 & 93.3 & 93.3 & 96.7 & 96.7 & 83.3 & 80.0 & 90.6 \\
\hline
bn\_SA\_32 & 90.0 & 86.7 & 100.0 & 96.7 & 83.3 & 76.7 & 88.9 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 86.7 & 93.3 & 93.3 & 100.0 & 93.3 & 90.0 & 92.8 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 86.7 & 73.3 & 80.0 & 66.7 & 66.7 & 76.7 & 75.0 \\
\hline
mfcc\_25\_10\_ham\_norm & 93.3 & 96.7 & 93.3 & 96.7 & 86.7 & 90.0 & 92.8 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 93.3 & 73.3 & 76.7 & 86.7 & 83.3 & 86.7 & 83.3 \\
\hline
ste\_10\_10\_norm & 53.3 & 20.0 & 50.0 & 36.7 & 36.7 & 53.3 & 41.7 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 96.7 & 66.7 & 76.7 & 86.7 & 66.7 & 73.3 & 77.8 \\
\hline
sti\_10\_10\_norm & 53.3 & 50.0 & 56.7 & 53.3 & 46.7 & 56.7 & 52.8 \\
\hline
stzcr\_10\_10\_norm & 53.3 & 50.0 & 50.0 & 46.7 & 30.0 & 56.7 & 47.8 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka pro neuronovou sí.}
\label{tab:nn_single_speaker}
\end{table}

\subsection{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním}
Stejnì jako pøi klasifikaci v rámci jednoho øeèníka dosahuje nejlepší vısledkù metoda DTW (tabulka \ref{tab:dtw_all_test_per_speaker}) s vyuitím pøíznakù vygenerovanıch neuronovou sítí. Nejvyšší pøesnosti dosahuje bottleneck o 8 a 16 neuronech vytvoøenı natrénováním neuronové sítì nad pùvodní datovou sadou. 

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 86.1 & 85.0 & 88.9 & 89.4 & 83.3 & 95.6 & 88.1 \\
\hline
bn\_16 & 90.6 & 87.8 & 86.1 & 90.0 & 86.1 & 92.2 & 88.8 \\
\hline
bn\_SA\_8 & 85.6 & 71.7 & 80.0 & 67.8 & 80.0 & 81.7 & 77.8 \\
\hline
bn\_SA\_16 & 86.7 & 81.1 & 85.6 & 80.0 & 87.2 & 81.1 & 83.6 \\
\hline
bn\_SA\_32 & 77.8 & 76.7 & 78.9 & 80.0 & 83.9 & 87.2 & 80.7 \\
\hline
log\_fb\_en\_25\_10\_ham & 68.9 & 70.6 & 82.8 & 70.6 & 77.8 & 82.8 & 75.6 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 67.8 & 70.6 & 74.4 & 72.2 & 76.1 & 78.9 & 73.3 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 61.1 & 66.7 & 77.8 & 65.0 & 71.1 & 72.2 & 69.0 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 49.4 & 52.8 & 58.3 & 53.9 & 57.2 & 55.0 & 54.4 \\
\hline
mfcc\_25\_10\_ham & 70.6 & 72.8 & 81.7 & 72.8 & 75.6 & 87.2 & 76.8 \\
\hline
mfcc\_25\_10\_ham\_norm & 59.4 & 63.3 & 66.7 & 59.4 & 56.1 & 73.9 & 63.1 \\
\hline
mfcc\_25\_10\_ham\_deltas & 67.8 & 68.9 & 80.0 & 70.0 & 74.4 & 83.3 & 74.1 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 47.8 & 46.7 & 52.2 & 42.8 & 48.3 & 47.2 & 47.5 \\
\hline
ste\_10\_10 & 18.3 & 16.1 & 22.8 & 22.8 & 18.9 & 23.3 & 20.4 \\
\hline
ste\_10\_10\_norm & 28.3 & 22.8 & 25.6 & 29.4 & 22.2 & 27.2 & 25.9 \\
\hline
ste\_sti\_stzcr\_10\_10 & 18.3 & 16.1 & 22.8 & 22.8 & 18.9 & 23.3 & 20.4 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 70.0 & 60.6 & 65.6 & 60.6 & 59.4 & 66.7 & 63.8 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm\_all & 70.0 & 60.6 & 68.3 & 65.6 & 61.7 & 64.4 & 65.1 \\
\hline
sti\_10\_10 & 20.6 & 30.6 & 26.1 & 38.9 & 30.0 & 39.4 & 30.9 \\
\hline
sti\_10\_10\_norm & 34.4 & 33.3 & 40.6 & 41.1 & 36.1 & 47.2 & 38.8 \\
\hline
stzcr\_10\_10 & 52.2 & 49.4 & 48.3 & 41.7 & 54.4 & 56.1 & 50.4 \\
\hline
stzcr\_10\_10\_norm & 50.6 & 47.8 & 49.4 & 44.4 & 47.8 & 51.1 & 48.5 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním pro DTW.}
\label{tab:dtw_all_test_per_speaker}
\end{table}

Porovnáním tabulek \ref{tab:dtw_all_test_per_speaker}, \ref{tab:svm_all_test_per_speaker} a \ref{tab:nn_all_test_per_speaker} je patrné, e pøíznaky ve frekvenèní oblasti nejhùøe klasifikuje SVM a pøíznaky v èasové oblasti neuronová sí. Normalizace a aplikace dynamickıch koeficientù má na pøesnost klasifikace obdobnı vliv jako v pøípadì klasifikace v~rámci jednoho øeèníka.

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 81.1 & 80.0 & 83.9 & 82.2 & 82.8 & 93.9 & 84.0 \\
\hline
bn\_16 & 86.1 & 78.3 & 81.1 & 82.2 & 82.2 & 89.9 & 83.2 \\
\hline
bn\_SA\_8 & 80.6 & 60.0 & 75.6 & 60.0 & 70.0 & 77.8 & 70.7 \\
\hline
bn\_SA\_16 & 75.0 & 60.6 & 71.1 & 73.3 & 75.0 & 77.8 & 72.1 \\
\hline
bn\_SA\_32 & 68.9 & 58.3 & 69.4 & 69.4 & 62.8 & 70.0 & 66.5 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 62.2 & 65.0 & 64.4 & 62.2 & 65.6 & 66.7 & 64.4 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 39.4 & 37.2 & 13.3 & 18.3 & 20.0 & 27.8 & 26.0 \\
\hline
mfcc\_25\_10\_ham\_norm & 50.6 & 46.7 & 43.3 & 41.1 & 35.0 & 53.3 & 45.0 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 37.2 & 35.0 & 15.0 & 21.1 & 18.9 & 24.4 & 25.3 \\
\hline
ste\_10\_10\_norm & 28.3 & 23.9 & 28.9 & 27.2 & 21.1 & 27.2 & 26.1 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 66.7 & 56.7 & 59.4 & 48.3 & 55.6 & 60.6 & 57.9 \\
\hline
sti\_10\_10\_norm & 36.7 & 33.3 & 35.6 & 37.2 & 33.3 & 43.9 & 36.7 \\
\hline
stzcr\_10\_10\_norm & 50.6 & 47.2 & 44.4 & 42.8 & 46.1 & 48.9 & 46.7 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním pro SVM.}
\label{tab:svm_all_test_per_speaker}
\end{table}

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|}
\cline{2-8}
\multicolumn{1}{c}{} & \multicolumn{7}{|c|}{Pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{6}{|c|}{Øeèník} & \multicolumn{1}{c|}{\multirow{2}{*}{Prùmìr}} \\
\cline{2-7}
 & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{6} & \\
\hline
bn\_8 & 77.2 & 80.6 & 83.9 & 88.3 & 78.9 & 88.3 & 82.9 \\
\hline
bn\_16 & 85.0  & 88.9 & 78.9 & 89.4 & 82.8 & 83.9 & 84.8 \\
\hline
bn\_SA\_8 & 76.1 & 70.6 & 81.7 & 72.2 & 82.2 & 77.8 & 76.8 \\
\hline
bn\_SA\_16 & 74.4 & 80.6 & 78.9 & 75.6 & 87.8 & 76.1 & 78.9 \\
\hline
bn\_SA\_32 & 70.0  & 77.2 & 78.3 & 87.8 & 82.8 & 73.9 & 78.3 \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 70.0 & 74.4 & 80.0 & 74.4 & 80.0 & 85.6 & 77.4 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 57.8 & 49.4 & 53.3 & 57.2 & 42.2 & 48.9 & 51.5 \\
\hline
mfcc\_25\_10\_ham\_norm & 68.3 & 72.8 & 68.3 & 66.7 & 68.9 & 78.3 & 70.6 \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 63.9 & 58.9 & 63.9 & 54.4 & 62.8 & 67.2 & 61.9 \\
\hline
ste\_10\_10\_norm & 23.3 & 19.4 & 26.1 & 28.3 & 22.2 & 25.6 & 24.2 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 56.7 & 46.7 & 64.4 & 58.3 & 58.3 & 66.1 & 58.4 \\
\hline
sti\_10\_10\_norm & 27.2 & 33.9 & 30.0 & 33.9 & 27.8 & 35.6 & 31.4 \\
\hline
stzcr\_10\_10\_norm & 45.6 & 40.6 & 45.6 & 45.6 & 40.6 & 44.4 & 43.7 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace v rámci jednoho øeèníka vùèi ostatním pro neuronovou sí.}
\label{tab:nn_all_test_per_speaker}
\end{table}

\subsection{Pøesnost klasifikace mezi všemi øeèníky}
Pøi klasifikaci mezi všemi øeèníky (tabulka \ref{tab:all_speakers}) navzájem dosahují opìt nejlepších vısledkù bottleneck pøíznaky a to zejména 8 neuronovı natrénovanı nad pùvodní datovou sadou a~16 neuronovı natrénovanı nad rozšíøenou sadou. Velmi vysokıch pøesností také dosahují pøíznaky ve frekvenèní oblasti a kombinace pøíznakù v èasové oblasti.

Jak ji bylo zmínìno v teoretické èásti, krátkodobá energie je silnì ovlivòována vıkyvy v amplitudì akustického signálu, zatímco krátkodobá intenzita tento problém nemá. Všimnìme si tedy, e krátkodobá intenzita dosahuje témìø dvakrát vìtší pøesnosti ne krátkodobá energie.

\begin{table}[H]
\centering
\resizebox{0.6\textwidth}{!}{\begin{tabular}{|l|r|r|r|}
\cline{2-4}
\multicolumn{1}{c}{} & \multicolumn{3}{|c|}{Prùmìrná pøesnost [\%]} \\
\hline
\multicolumn{1}{|c|}{Typ pøíznakù} & \multicolumn{1}{c|}{DTW} & \multicolumn{1}{c|}{SVM} & \multicolumn{1}{c|}{NN} \\
\hline
bn\_8 & 98.9 & 94.4 & 98.9 \\
\hline
bn\_16 & 96.7 & 92.2 & 95.6 \\
\hline
bn\_SA\_8 & 90.6 & 76.7 & 91.1 \\
\hline
bn\_SA\_16 & 98.9 & 73.9 & 92.8 \\
\hline
bn\_SA\_32 & 94.4 & 75.0 & 91.1 \\
\hline
log\_fb\_en\_25\_10\_ham & 90.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 92.8 & 74.4 & 91.1 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 86.1 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 73.3 & 30.0  & 10.0 \\
\hline
mfcc\_25\_10\_ham & 91.7 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_norm & 90.6 & 52.8 & 65.0 \\
\hline
mfcc\_25\_10\_ham\_deltas & 89.4 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 74.4 & 33.3 & 10.0 \\
\hline
ste\_10\_10 & 32.8 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_10\_10\_norm & 36.1 & 29.4 & 52.2 \\
\hline
ste\_sti\_stzcr\_10\_10 & 32.8 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 82.2 & 76.1 & 77.2 \\
\hline
ste\_sti\_stzcr\_10\_10\_norm\_all & 85.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
sti\_10\_10 & 60.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
sti\_10\_10\_norm & 62.2 & 46.1 & 57.2 \\
\hline
stzcr\_10\_10 & 63.9 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
stzcr\_10\_10\_norm & 56.7 & 53.3 & 58.9 \\
\hline
\end{tabular}}
\caption{Pøesnost klasifikace mezi všemi øeèníky.}
\label{tab:all_speakers}
\end{table}

Pøesnost klasifikátoru pro pøíznaky v èasové a frekvenèní oblasti SVM je vıraznì horší ne pøesnost DTW. Neuronová sí se zdá bıt velice robustní pro normalizované logaritmované energie banky filtrù, nicménì pro pøíznaky s dynamickımi koeficienty se nepodaøilo sí s danou strukturou úspìšnì natrénovat a pøesnost je pouhıch 10\%. Pøíznaky generované pomocí neuronové sítì s bottleneck vrstvou o 8 neuronech nad pùvodní datovou sadou se zdají bıt nezávislé na klasifikaèní metodì a dosahují velmi vysokıch pøesností. Z hlediska vıpoèetních nárokù je ovšem vhodnìjší volit pouze metodu DTW, jeliko klasifikátor SVM i neuronová sí vyuívají pøedpoèítané DTW vzdálenosti. 

% ZÁVÌR
\newpage
\section{Závìr}
Cílem této práce bylo porovnat rùzné typy pøíznakù pro úlohu klasifikace izolovanıch slov. V první èástí práce byly pøedstaveny metody zpracování akustického signálu vèetnì jednotlivıch typù pøíznakù a byly odvozeny klasifikaèní algoritmy. Ve druhé èásti pak byly pøedstaveny testované parametrizace pøíznakù a navrené algoritmy vyuité ke klasifikaci.

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{|l|r|r|r||r|r|r||r|r|r|}
\cline{2-10}
\multicolumn{1}{c}{} & \multicolumn{9}{|c|}{Prùmìrná pøesnost klasifikace [\%]} \\
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Typ pøíznakù}} & \multicolumn{3}{|c||}{DTW} & \multicolumn{3}{|c||}{SVM} & \multicolumn{3}{|c|}{NN} \\
\cline{2-10}
 & \multicolumn{1}{c|}{1to1} & \multicolumn{1}{c|}{AlltoAll} & \multicolumn{1}{c||}{Rozdíl} & \multicolumn{1}{c|}{1to1} & \multicolumn{1}{c|}{AlltoAll} & \multicolumn{1}{c||}{Rozdíl} & \multicolumn{1}{c|}{1to1} & \multicolumn{1}{c|}{AlltoAll} & \multicolumn{1}{c|}{Rozdíl} \\
\hline
bn\_8 & 95.0 & 98.9 & 3.9 & 89.4 & 94.4 & 5 & 91.7 & 98.9 & 7.2 \\
\hline
bn\_16 & 92.8 & 96.7 & 3.9 & 88.3 & 92.2 & 3.9 & 87.2 & 95.6 & 8.4 \\
\hline
bn\_SA\_8 & 86.7 & 90.6 & 3.9 & 80.0 & 76.7 & -3.3 & 87.8 & 91.1 & 3.3 \\
\hline
bn\_SA\_16 & 91.1 & 98.9 & 7.8 & 80.6 & 73.9 & -6.7 & 90.6 & 92.8 & 2.2 \\
\hline
bn\_SA\_32 & 91.1 & 94.4 & 3.3 & 77.8 & 75.0 & -2.8 & 88.9 & 91.1 & 2.2 \\
\hline
log\_fb\_en\_25\_10\_ham & 88.9 & 90.6 & 1.7 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_norm & 87.8 & 92.8 & 5.0 & 82.8 & 74.4 & -8.4 & 92.8 & 91.1 & -1.7 \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas & 85.0 & 86.1 & 1.1 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
log\_fb\_en\_25\_10\_ham\_deltas\_norm & 69.4 & 73.3 & 3.9 & 33.9 & 30.0 & -3.9 & 75.0 & 10.0 & -65.0 \\
\hline
mfcc\_25\_10\_ham & 90.6 & 91.7 & 1.1 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_norm & 90.0 & 90.6 & 0.6 & 70.6 & 52.8 & -17.8 & 92.8 & 65.0 & -27.8 \\
\hline
mfcc\_25\_10\_ham\_deltas & 87.8 & 89.4 & 1.6 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
mfcc\_25\_10\_ham\_deltas\_norm & 70.0 & 74.4 & 4.4 & 38.3 & 33.3 & -5 & 83.3 & 10.0 & -73.3 \\
\hline
ste\_10\_10 & 38.3 & 32.8 & -5.5 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_10\_10\_norm & 48.9 & 36.1 & -12.8 & 47.8 & 29.4 & -18.4 & 41.7 & 52.2 & 10.5 \\
\hline
ste\_sti\_stzcr\_10\_10 & 38.3 & 32.8 & -5.5 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
ste\_sti\_stzcr\_10\_10\_norm & 81.1 & 82.2 & 1.1 & 73.3 & 76.1 & 2.8 & 77.8 & 77.2 & -0.6 \\
\hline
sti\_10\_10 & 57.2 & 60.6 & 3.4 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
sti\_10\_10\_norm & 62.2 & 62.2 & 0.0 & 56.1 & 46.1 & -10.0 & 52.8 & 57.2 & 4.4 \\
\hline
stzcr\_10\_10 & 61.7 & 63.9 & 2.2 & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c||}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} \\
\hline
stzcr\_10\_10\_norm & 56.1 & 56.7 & 0.6 & 52.8 & 53.3 & 0.5 & 47.8 & 58.9 & 11.1 \\ 
\hline
\end{tabular}}
\caption{Porovnání prùmìrné pøesnosti klasifikace  v rámci jednoho øeèníka (\textit{1to1}) a~mezi všemi øeèníky (\textit{AlltoAll}).}
\label{tab:mean}
\end{table}

Porovnáním prùmìrnıch pøesností klasifikace (tabulka \ref{tab:mean}) se ukázalo, e nejpøesnìjším a nejrobustnìjším pøíznakem (nezávislı na øeèníkovi) je pøíznak vygenerovanı neuronovou sítí s bottleneck vrstvou. Velice dobrıch vısledkù také dosahovaly pøíznaky zaloené na logaritmované energii banky filtrù klasifikovanıch jak pomocí metody DTW, tak pomocí neuronové sítì.

Mezi nejménì pøesné typy pøíznakù pak patøily pøíznaky v èasové oblasti. Ukázalo se ovšem, e zkombinováním jednotlivıch pøíznakù v èasové oblasti lze vytvoøit pøíznak s~pomìrnì vysokou informaèní hodnotou. Aèkoliv se èekalo, e pøíznaky v èasové oblasti budou silnì závislé na øeèníkovi a pøi klasifikaci mezi všemi øeèníky dojde k poklesu pøesnosti, došlo k této situaci pouze pro krátkodobou energii u metod DTW a SVM a pro krátkodobou intenzitu u SVM. U ostatních pøíznakù došlo naopak k navıšení pøesnosti.

Pro další zlepšení dosaenıch vısledkù by bylo vhodné zamìøit se na pøíznaky generované neuronovou sítí a pokusit se optimalizovat její strukturu. Dalším krokem by pak bylo otestování rekurentních neuronovıch sítí (zejména typu LSTM), které v dnešní dobì pro podobné úlohy dosahují velice dobrıch vısledkù. Pro vyuití v praxi by pak bylo potøeba tento systém propojit se systémem pro detekci hlasové aktivity (voice activity detection).

% SEZNAM OBRÁZKÙ A TABULEK
\newpage
\listoffigures
\listoftables

% LITERATURA
\newpage
\bibliographystyle{unsrt}
\bibliography{literatura}


\end{document}